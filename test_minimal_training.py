"""
Minimal training test - éªŒè¯è®­ç»ƒpipelineèƒ½å¦æ­£å¸¸è¿è¡Œ
åªè®­ç»ƒ3ä¸ªstepsï¼Œç”¨äºå¿«é€ŸéªŒè¯ä»£ç 
"""

import os
import sys
import torch
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from transformers import AutoTokenizer, CLIPImageProcessor

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from llava.model.map_llava_model import build_map_detector
from llava.data.map_dataset import MapDetectionDataset

print("\n" + "="*70)
print("æœ€å°è®­ç»ƒæµ‹è¯• - éªŒè¯ä»£ç èƒ½å¦æ­£å¸¸è¿è¡Œ")
print("="*70)

# é…ç½®
DATAROOT = "/home/cly/auto/llava_test/LLaVA/data/nuscenes_mini"
VERSION = "v1.0-mini"
BATCH_SIZE = 1  # æœ€å°batch size
NUM_STEPS = 3   # åªè®­ç»ƒ3æ­¥
USE_FP16 = True  # Enable FP16 to reduce memory usage

print(f"\né…ç½®:")
print(f"  æ•°æ®è·¯å¾„: {DATAROOT}")
print(f"  ç‰ˆæœ¬: {VERSION}")
print(f"  Batch Size: {BATCH_SIZE}")
print(f"  æµ‹è¯•æ­¥æ•°: {NUM_STEPS}")
print(f"  æ··åˆç²¾åº¦: {USE_FP16}")

# Step 1: æ„å»ºæ¨¡å‹
print("\n" + "="*70)
print("Step 1: æ„å»ºæ¨¡å‹")
print("="*70)

try:
    # ä½¿ç”¨æœ¬åœ°æ¨¡å‹æƒé‡
    LOCAL_LLM_PATH = "/home/cly/auto/llava_test/LLaVA/vicuna-7b-v1.5"
    print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {LOCAL_LLM_PATH}")
    
    # Q-Formeré¢„è®­ç»ƒé…ç½®
    # ä¸ä½¿ç”¨BLIP-2ï¼ŒåŸå› ï¼š
    # 1. BLIP-2ä½¿ç”¨ViTï¼Œæˆ‘ä»¬ä½¿ç”¨ResNet50ï¼Œæ¶æ„ä¸åŒ¹é…ï¼ˆåªæœ‰39%æƒé‡èƒ½åŒ¹é…ï¼‰
    # 2. ResNet50å·²æœ‰ImageNeté¢„è®­ç»ƒï¼Œè¶³å¤Ÿå¼ºå¤§
    # 3. éšæœºåˆå§‹åŒ–æ›´ç¨³å®šï¼Œæ‰€æœ‰å‚æ•°ä»åŒä¸€èµ·ç‚¹å¼€å§‹
    qformer_pretrained = None
    print(f"Q-Former: éšæœºåˆå§‹åŒ– (ResNet50å·²æœ‰ImageNeté¢„è®­ç»ƒ)")
    
    model = build_map_detector(
        llm_path=LOCAL_LLM_PATH,
        freeze_llm=True,
        qformer_pretrained=qformer_pretrained,
    )
    # æ³¨æ„ï¼šä¸è¦è°ƒç”¨model.cuda()ï¼Œå› ä¸ºLLMå·²ç»é€šè¿‡device_map="auto"åˆ†é…å¥½è®¾å¤‡äº†
    model.train()
    print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
    print(f"   æ¨¡å‹å·²è‡ªåŠ¨åˆ†é…åˆ°GPUï¼ˆdevice_map='auto'ï¼‰")
except Exception as e:
    print(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Step 2: åˆ›å»ºä¼˜åŒ–å™¨
print("\n" + "="*70)
print("Step 2: åˆ›å»ºä¼˜åŒ–å™¨")
print("="*70)

try:
    # åˆ†ç»„å‚æ•°
    qformer_params = []
    queries_params = []
    decoder_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if 'qformer' in name:
            qformer_params.append(param)
        elif 'map_queries' in name:
            queries_params.append(param)
        elif 'decoder' in name:
            decoder_params.append(param)
    
    optimizer = torch.optim.AdamW([
        {'params': qformer_params, 'lr': 1e-5},
        {'params': queries_params, 'lr': 1e-4},
        {'params': decoder_params, 'lr': 1e-4},
    ], weight_decay=0.01)
    
    print(f"âœ… ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   Q-Formerå‚æ•°: {sum(p.numel() for p in qformer_params):,}")
    print(f"   Querieså‚æ•°: {sum(p.numel() for p in queries_params):,}")
    print(f"   Decoderå‚æ•°: {sum(p.numel() for p in decoder_params):,}")
except Exception as e:
    print(f"âŒ ä¼˜åŒ–å™¨åˆ›å»ºå¤±è´¥: {e}")
    sys.exit(1)

# Step 3: åŠ è½½æ•°æ®é›†
print("\n" + "="*70)
print("Step 3: åŠ è½½æ•°æ®é›†")
print("="*70)

try:
    # ä½¿ç”¨æœ¬åœ°tokenizerå’Œimage processor
    LOCAL_LLM_PATH = "/home/cly/auto/llava_test/LLaVA/vicuna-7b-v1.5"
    LOCAL_CLIP_PATH = "/home/cly/auto/llava_test/LLaVA/clip-vit-large-patch14-336"
    
    print(f"åŠ è½½tokenizer: {LOCAL_LLM_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_PATH, use_fast=False)
    
    print(f"åŠ è½½image processor: {LOCAL_CLIP_PATH}")
    image_processor = CLIPImageProcessor.from_pretrained(LOCAL_CLIP_PATH)
    
    # æ£€æŸ¥GT cache
    gt_cache_dir = os.path.join(DATAROOT, f'gt_cache_{VERSION}_train.pkl')
    if not os.path.exists(gt_cache_dir):
        print(f"âŒ GT cacheä¸å­˜åœ¨: {gt_cache_dir}")
        print("   è¯·å…ˆè¿è¡Œ: python tools/generate_gt_cache.py --split train")
        sys.exit(1)
    
    dataset = MapDetectionDataset(
        dataroot=DATAROOT,
        version=VERSION,
        split='train',
        gt_cache_path=gt_cache_dir,
        image_processor=image_processor,
        tokenizer=tokenizer,
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,  # ä¸shuffleï¼Œæ–¹ä¾¿è°ƒè¯•
        num_workers=0,  # å•è¿›ç¨‹ï¼Œæ–¹ä¾¿è°ƒè¯•
        collate_fn=dataset.collate_fn,
    )
    
    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
    print(f"   Batchæ•°é‡: {len(dataloader)}")
except Exception as e:
    print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Step 4: è®­ç»ƒæµ‹è¯•
print("\n" + "="*70)
print("Step 4: è®­ç»ƒæµ‹è¯• ({}æ­¥)".format(NUM_STEPS))
print("="*70)

# Note: Not using GradScaler because we have mixed FP16/FP32 trainable params
# (map_queries in LLM is FP16, Q-Former and Decoder are FP32)

try:
    for step, batch in enumerate(dataloader):
        if step >= NUM_STEPS:
            break
        
        print(f"\n--- Step {step+1}/{NUM_STEPS} ---")
        
        # ç§»åŠ¨åˆ°GPUï¼ˆä½¿ç”¨GPU 0ï¼Œå› ä¸ºQ-Formerå’ŒDecoderåœ¨GPU 0ï¼‰
        device = torch.device('cuda:0')
        images = batch['images'].to(device)
        text_ids = batch['text_ids'].to(device)
        gt_labels = batch['gt_labels'].to(device)
        gt_points = batch['gt_points'].to(device)
        gt_masks = batch['gt_masks'].to(device)
        
        # ç›¸æœºå‚æ•°ï¼ˆç”¨äº3Dä½ç½®ç¼–ç ï¼‰
        cam_intrinsics = batch.get('cam_intrinsics')
        cam_extrinsics = batch.get('cam_extrinsics')
        if cam_intrinsics is not None:
            cam_intrinsics = cam_intrinsics.to(device)
        if cam_extrinsics is not None:
            cam_extrinsics = cam_extrinsics.to(device)
        
        print(f"è¾“å…¥å½¢çŠ¶:")
        print(f"  images: {list(images.shape)}")
        print(f"  text_ids: {list(text_ids.shape)}")
        print(f"  gt_labels: {list(gt_labels.shape)}")
        print(f"  gt_points: {list(gt_points.shape)}")
        print(f"  gt_masks: {list(gt_masks.shape)}")
        print(f"  æœ‰æ•ˆGTæ•°é‡: {gt_masks.sum().item()}")
        
        # Forward with loss calculation
        with autocast(enabled=USE_FP16):
            output = model(
                images=images,
                text_ids=text_ids,
                return_loss=True,
                gt_labels=gt_labels,
                gt_points=gt_points,
                gt_masks=gt_masks,
                cam_intrinsics=cam_intrinsics,
                cam_extrinsics=cam_extrinsics,
            )
            
            # Check decoder outputs
            pred_logits = output['pred_logits']
            pred_points = output['pred_points']
            print(f"\nDecoder outputs:")
            print(f"  pred_logits: {list(pred_logits.shape)}, has_nan={torch.isnan(pred_logits).any()}")
            if not torch.isnan(pred_logits).any():
                print(f"    range: [{pred_logits.min():.4f}, {pred_logits.max():.4f}]")
            print(f"  pred_points: {list(pred_points.shape)}, has_nan={torch.isnan(pred_points).any()}")
            if not torch.isnan(pred_points).any():
                print(f"    range: [{pred_points.min():.4f}, {pred_points.max():.4f}]")
            
            loss = output['loss']
            loss_dict = output['loss_dict']
        
        print(f"è¾“å‡º:")
        print(f"  pred_logits: {list(output['pred_logits'].shape)}")
        print(f"  pred_points: {list(output['pred_points'].shape)}")
        print(f"  loss_total: {loss.item():.4f}")
        for key, value in loss_dict.items():
            if key != 'loss_total':
                print(f"  {key}: {value.item():.4f}")
        
        # Backward - don't use GradScaler since we have mixed FP16/FP32 trainable params
        optimizer.zero_grad()
        loss.backward()
        # Clip gradients
        trainable_params = [p for p in model.parameters() if p.requires_grad and p.grad is not None]
        if trainable_params:
            torch.nn.utils.clip_grad_norm_(trainable_params, 0.1)
        optimizer.step()
        
        print(f"âœ… Step {step+1} å®Œæˆ")
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒæµ‹è¯•æˆåŠŸï¼æ‰€æœ‰æ­¥éª¤æ­£å¸¸è¿è¡Œï¼")
    print("="*70)
    print("\næµ‹è¯•æ€»ç»“:")
    print(f"  âœ“ æ¨¡å‹æ„å»ºæˆåŠŸ")
    print(f"  âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"  âœ“ Forward passæˆåŠŸ")
    print(f"  âœ“ Lossè®¡ç®—æˆåŠŸ")
    print(f"  âœ“ Backward passæˆåŠŸ")
    print(f"  âœ“ ä¼˜åŒ–å™¨æ›´æ–°æˆåŠŸ")
    print("\nğŸ‰ ä»£ç å¯ä»¥æ­£å¸¸è®­ç»ƒï¼å¯ä»¥ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒäº†ï¼")
    print("="*70)

except Exception as e:
    print(f"\nâŒ è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

