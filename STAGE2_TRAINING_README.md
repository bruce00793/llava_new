# Stage 2 è®­ç»ƒ - å¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸ“‹ æ¦‚è§ˆ

**Stage 2: è”åˆè®­ç»ƒï¼ˆä½¿ç”¨BLIP-2é¢„è®­ç»ƒQ-Formerï¼‰**

- âœ… Q-Former: 1e-5 (å¾®è°ƒ)
- âœ… Map Queries: 1e-4 (ä»å¤´è®­ç»ƒ)  
- âœ… Map Decoder: 1e-4 (ä»å¤´è®­ç»ƒ)
- âŒ LLM: å†»ç»“

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

### Step 1: ç”ŸæˆGT Cache

```bash
cd /home/cly/auto/llava_test/LLaVA

# Train set
python tools/generate_gt_cache.py \
    --dataroot /path/to/nuscenes \
    --version v1.0-mini \
    --split train \
    --output gt_cache_train.pkl

# Val set
python tools/generate_gt_cache.py \
    --dataroot /path/to/nuscenes \
    --version v1.0-mini \
    --split val \
    --output gt_cache_val.pkl
```

### Step 2: ä¿®æ”¹é…ç½®

ç¼–è¾‘ `scripts/train_stage2.sh`:

```bash
# ä¿®æ”¹è¿™ä¸€è¡Œï¼š
DATAROOT="/path/to/nuscenes"  # â† æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
```

### Step 3: å¼€å§‹è®­ç»ƒ

```bash
# å•å¡è®­ç»ƒ
bash scripts/train_stage2.sh

# æˆ–å¤šå¡è®­ç»ƒï¼ˆæ¨èï¼‰
bash scripts/train_stage2_distributed.sh
```

---

## ğŸ”§ è®­ç»ƒå‰æµ‹è¯•

éªŒè¯ç¯å¢ƒå’Œæ¨¡å‹é…ç½®ï¼š

```bash
cd /home/cly/auto/llava_test/LLaVA
conda activate llava_new
python test_training_setup.py
```

é¢„æœŸè¾“å‡ºï¼š
```
âœ… All core tests passed!
```

---

## ğŸ“Š æ˜¾å­˜éœ€æ±‚

| é…ç½® | æ˜¾å­˜ | GPUæ¨è |
|-----|------|---------|
| Batch=4 | 23GB | A100 40GB |
| Batch=2 | 15GB | V100 32GB |
| Batch=1 | 10GB | RTX 3090 |

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

```
llava_test/LLaVA/
â”œâ”€â”€ train_map_detection.py              # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_stage2.sh                 # å•å¡å¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ train_stage2_distributed.sh     # å¤šå¡å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_training_setup.py              # è®­ç»ƒå‰æµ‹è¯•
â”œâ”€â”€ TRAINING_GUIDE.md                   # è¯¦ç»†è®­ç»ƒæŒ‡å—
â””â”€â”€ tools/
    â””â”€â”€ generate_gt_cache.py            # GTç”Ÿæˆå·¥å…·
```

---

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### æ—¥å¿—ç¤ºä¾‹

```
Epoch [1/24] Step [10/200] Loss: 3.5421 (Avg: 3.6234)
LR: q=1.00e-05 m=1.00e-04 d=1.00e-04

Detailed losses:
  loss_cls: 1.2345    # åˆ†ç±»æŸå¤±
  loss_pts: 1.8765    # ç‚¹å›å½’æŸå¤±
  loss_dir: 0.4311    # æ–¹å‘æŸå¤±
  loss_total: 3.5421  # æ€»æŸå¤±
```

### æ£€æŸ¥ç‚¹ä½ç½®

```
outputs/map_detection_stage2_YYYYMMDD_HHMMSS/
â”œâ”€â”€ best_model.pth           # â† éªŒè¯é›†æœ€ä¼˜æ¨¡å‹
â”œâ”€â”€ checkpoint_epoch_N.pth   # æ¯epochä¿å­˜
â””â”€â”€ final_model.pth          # è®­ç»ƒç»“æŸ
```

---

## â“ å¸¸è§é—®é¢˜

### Q: OOMé”™è¯¯

```bash
# é™ä½batch size
BATCH_SIZE=2  # æˆ– 1
```

### Q: GT Cacheç”Ÿæˆå¤±è´¥

```bash
# æ£€æŸ¥nuScenesè·¯å¾„
ls /path/to/nuscenes/v1.0-mini/
# åº”è¯¥çœ‹åˆ° scene.json, sample.json ç­‰æ–‡ä»¶
```

### Q: BLIP-2ä¸‹è½½æ…¢

```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

### Q: æƒ³è·³è¿‡BLIP-2é¢„è®­ç»ƒ

```bash
# ä¿®æ”¹ train_stage2.sh:
--qformer-pretrained none  # ä¸æ¨èï¼Œæ€§èƒ½ä¼šä¸‹é™
```

---

## ğŸ“– å®Œæ•´æ–‡æ¡£

è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ï¼š[TRAINING_GUIDE.md](./TRAINING_GUIDE.md)

---

## âœ… è®­ç»ƒæ£€æŸ¥æ¸…å•

- [ ] nuScenesæ•°æ®é›†å·²ä¸‹è½½
- [ ] GT Cacheå·²ç”Ÿæˆï¼ˆtrain + valï¼‰
- [ ] ä¿®æ”¹äº† `scripts/train_stage2.sh` ä¸­çš„ `DATAROOT`
- [ ] è¿è¡Œäº† `test_training_setup.py` å¹¶é€šè¿‡
- [ ] æ˜¾å­˜è¶³å¤Ÿï¼ˆå‚è€ƒä¸Šè¡¨ï¼‰
- [ ] ç¯å¢ƒæ¿€æ´»ï¼š`conda activate llava_new`

**å‡†å¤‡å¥½äº†ï¼Ÿå¼€å§‹è®­ç»ƒï¼ğŸš€**

```bash
bash scripts/train_stage2.sh
```

---

**é¢„è®¡è®­ç»ƒæ—¶é—´ï¼š**
- v1.0-mini: ~2å°æ—¶ (A100)
- v1.0-trainval: ~24å°æ—¶ (4Ã—A100)

