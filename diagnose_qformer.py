#!/usr/bin/env python3
"""
Q-Former è¯Šæ–­å·¥å…·ï¼šåˆ†æ 512 Scene Tokens èƒ½å¦æœ‰æ•ˆä»£è¡¨ 6 å¼ å›¾åƒ

å®éªŒå†…å®¹ï¼š
1. é‡å»ºæŸå¤±åˆ†æï¼šç”¨ 512 tokens é‡å»ºåŸå§‹ 2100 patch ç‰¹å¾
2. Attention å¯è§†åŒ–ï¼šæ¯ä¸ª Query å…³æ³¨å“ªäº›å›¾åƒåŒºåŸŸ
3. ä¿¡æ¯å‹ç¼©ç‡åˆ†æï¼šè®¡ç®—ä¿¡æ¯ä¿ç•™ç¨‹åº¦

ä½¿ç”¨æ–¹æ³•ï¼š
    python diagnose_qformer.py --checkpoint <checkpoint_path> --num-samples 50
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from llava.data.map_dataset import MapDetectionDataset
from llava.model.map_llava_model import LLaVAMapDetector


class FeatureReconstructor(nn.Module):
    """
    ç”¨ 512 Scene Tokens é‡å»º 2100 Patch ç‰¹å¾çš„ Decoder
    
    å¦‚æœé‡å»ºè¯¯å·®å¾ˆå¤§ï¼Œè¯´æ˜ 512 tokens ä¿¡æ¯ä¸è¶³
    """
    def __init__(self, token_dim: int = 4096, num_tokens: int = 512, 
                 num_patches: int = 2100, patch_dim: int = 1024):
        super().__init__()
        self.num_patches = num_patches
        
        # ç®€å•çš„ MLP Decoder
        self.decoder = nn.Sequential(
            nn.Linear(token_dim, 2048),
            nn.GELU(),
            nn.Linear(2048, 2048),
            nn.GELU(),
            nn.Linear(2048, num_patches * patch_dim // 4),  # å…ˆè¾“å‡ºå‹ç¼©è¡¨ç¤º
        )
        
        # æœ€ç»ˆæŠ•å½±åˆ° patch ç‰¹å¾
        self.final_proj = nn.Linear(patch_dim // 4, patch_dim)
        self.patch_dim = patch_dim
        
    def forward(self, scene_tokens: torch.Tensor) -> torch.Tensor:
        """
        Args:
            scene_tokens: [B, 512, 4096] Scene Tokens
        Returns:
            reconstructed: [B, 2100, 1024] é‡å»ºçš„ patch ç‰¹å¾
        """
        B = scene_tokens.shape[0]
        
        # å…¨å±€æ± åŒ– + è§£ç 
        pooled = scene_tokens.mean(dim=1)  # [B, 4096]
        decoded = self.decoder(pooled)  # [B, 2100 * 256]
        
        # Reshape
        decoded = decoded.view(B, self.num_patches, -1)  # [B, 2100, 256]
        reconstructed = self.final_proj(decoded)  # [B, 2100, 1024]
        
        return reconstructed


def extract_qformer_features(model, images, device):
    """
    æå– Q-Former çš„ä¸­é—´ç‰¹å¾ç”¨äºåˆ†æ
    
    Returns:
        image_features: [B, 2100, 1024] åŸå§‹å›¾åƒ patch ç‰¹å¾
        scene_tokens: [B, 512, D] Scene Tokens
        attention_weights: [B, 512, 2100] Attention weights (å¦‚æœå¯è·å–)
    """
    model.eval()
    with torch.no_grad():
        # è·å– Q-Former
        qformer = model.qformer
        
        # 1. æå–å›¾åƒç‰¹å¾
        B, num_cams, C, H, W = images.shape
        images_flat = images.view(B * num_cams, C, H, W)
        
        # é€šè¿‡ backbone
        backbone_features = qformer.backbone(images_flat)
        
        # å–æœ€åä¸€å±‚ç‰¹å¾
        if isinstance(backbone_features, dict):
            feat = backbone_features['layer4'] if 'layer4' in backbone_features else list(backbone_features.values())[-1]
        elif isinstance(backbone_features, (list, tuple)):
            feat = backbone_features[-1]
        else:
            feat = backbone_features
            
        # é€šè¿‡ neck
        if hasattr(qformer, 'neck') and qformer.neck is not None:
            feat = qformer.neck([feat])[0]
        
        # feat: [B*6, C, h, w]
        _, C_feat, h, w = feat.shape
        num_patches_per_cam = h * w
        total_patches = num_cams * num_patches_per_cam
        
        # Reshape ä¸º [B, 6*h*w, C]
        feat = feat.view(B, num_cams, C_feat, h, w)
        feat = feat.permute(0, 1, 3, 4, 2).reshape(B, total_patches, C_feat)
        
        image_features = feat  # [B, 2100, C]
        
        # 2. è·å– Scene Tokensï¼ˆå®Œæ•´å‰å‘ä¼ æ’­ï¼‰
        scene_tokens = qformer(images)  # [B, 512, D]
        
        # 3. å°è¯•è·å– attention weightsï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        attention_weights = None
        
        return image_features, scene_tokens, attention_weights, (h, w)


def compute_reconstruction_metrics(image_features, scene_tokens, device):
    """
    è®¡ç®—é‡å»ºæŒ‡æ ‡
    """
    B, num_patches, feat_dim = image_features.shape
    _, num_tokens, token_dim = scene_tokens.shape
    
    # æ–¹æ³•1ï¼šç›´æ¥ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆScene Tokens ä¸æœ€è¿‘ Patch çš„ç›¸ä¼¼åº¦ï¼‰
    # å…ˆæŠ•å½±åˆ°ç›¸åŒç»´åº¦
    if feat_dim != token_dim:
        proj = nn.Linear(feat_dim, token_dim).to(device)
        with torch.no_grad():
            image_features_proj = proj(image_features)
    else:
        image_features_proj = image_features
    
    # å½’ä¸€åŒ–
    img_norm = F.normalize(image_features_proj, dim=-1)  # [B, 2100, D]
    tok_norm = F.normalize(scene_tokens, dim=-1)  # [B, 512, D]
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = torch.bmm(tok_norm, img_norm.transpose(1, 2))  # [B, 512, 2100]
    
    # æ¯ä¸ª token çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆå®ƒæœ€å…³æ³¨çš„ patchï¼‰
    max_sim_per_token = sim_matrix.max(dim=2)[0]  # [B, 512]
    
    # æ¯ä¸ª patch è¢«å…³æ³¨çš„æœ€å¤§ç¨‹åº¦
    max_sim_per_patch = sim_matrix.max(dim=1)[0]  # [B, 2100]
    
    # ç»Ÿè®¡
    metrics = {
        'avg_token_max_sim': max_sim_per_token.mean().item(),
        'min_token_max_sim': max_sim_per_token.min().item(),
        'avg_patch_coverage': max_sim_per_patch.mean().item(),
        'min_patch_coverage': max_sim_per_patch.min().item(),
        'uncovered_patches_ratio': (max_sim_per_patch < 0.3).float().mean().item(),
    }
    
    return metrics, sim_matrix


def visualize_attention_coverage(sim_matrix, h, w, num_cams=6, save_path=None):
    """
    å¯è§†åŒ– 512 ä¸ª tokens å¯¹ 6 å¼ å›¾åƒçš„è¦†ç›–æƒ…å†µ
    """
    # sim_matrix: [B, 512, 2100]
    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
    sim = sim_matrix[0].cpu().numpy()  # [512, 2100]
    
    # æ¯ä¸ª patch è¢«å…³æ³¨çš„æœ€å¤§ç¨‹åº¦
    patch_coverage = sim.max(axis=0)  # [2100]
    
    # Reshape ä¸º [6, h, w]
    num_patches_per_cam = h * w
    coverage_maps = patch_coverage.reshape(num_cams, h, w)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    cam_names = ['FRONT', 'FRONT_LEFT', 'FRONT_RIGHT', 'BACK', 'BACK_LEFT', 'BACK_RIGHT']
    
    for i, (ax, name) in enumerate(zip(axes.flat, cam_names)):
        im = ax.imshow(coverage_maps[i], cmap='hot', vmin=0, vmax=1)
        ax.set_title(f'{name}\nAvg Coverage: {coverage_maps[i].mean():.3f}')
        ax.axis('off')
        plt.colorbar(im, ax=ax, fraction=0.046)
    
    plt.suptitle('Scene Token Coverage per Camera\n(Higher = Better represented by 512 tokens)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved coverage visualization to {save_path}")
    
    plt.close()
    
    return coverage_maps


def analyze_token_distribution(sim_matrix, h, w, num_cams=6, save_path=None):
    """
    åˆ†ææ¯ä¸ª token ä¸»è¦å…³æ³¨å“ªä¸ªç›¸æœº
    """
    sim = sim_matrix[0].cpu().numpy()  # [512, 2100]
    num_patches_per_cam = h * w
    
    # æ¯ä¸ª token æœ€å…³æ³¨çš„ patch ç´¢å¼•
    best_patch_idx = sim.argmax(axis=1)  # [512]
    
    # è½¬æ¢ä¸ºç›¸æœºç´¢å¼•
    best_cam_idx = best_patch_idx // num_patches_per_cam  # [512]
    
    # ç»Ÿè®¡æ¯ä¸ªç›¸æœºè¢«å¤šå°‘ tokens å…³æ³¨
    cam_counts = np.bincount(best_cam_idx, minlength=num_cams)
    
    # å¯è§†åŒ–
    fig, ax = plt.subplots(figsize=(10, 6))
    cam_names = ['FRONT', 'FRONT_LEFT', 'FRONT_RIGHT', 'BACK', 'BACK_LEFT', 'BACK_RIGHT']
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
    
    bars = ax.bar(cam_names, cam_counts, color=colors, edgecolor='black', linewidth=1.5)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, count in zip(bars, cam_counts):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, 
                f'{count}\n({count/512*100:.1f}%)', 
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax.set_ylabel('Number of Tokens', fontsize=12)
    ax.set_title('Distribution of 512 Scene Tokens Across 6 Cameras\n(Each token is assigned to its most-attended camera)', 
                 fontsize=13, fontweight='bold')
    ax.set_ylim(0, max(cam_counts) * 1.2)
    
    # æ·»åŠ ç†æƒ³å‡åŒ€åˆ†å¸ƒçº¿
    ax.axhline(y=512/6, color='red', linestyle='--', linewidth=2, label=f'Ideal uniform: {512/6:.0f}')
    ax.legend()
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved token distribution to {save_path}")
    
    plt.close()
    
    return cam_counts


def compute_information_retention(image_features, scene_tokens):
    """
    è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡ï¼šé€šè¿‡ PCA åˆ†æ
    """
    # åŸå§‹å›¾åƒç‰¹å¾çš„æœ‰æ•ˆç»´åº¦ï¼ˆé€šè¿‡ PCA è§£é‡Šæ–¹å·®æ¯”ï¼‰
    img_feat = image_features[0].cpu().numpy()  # [2100, C]
    tok_feat = scene_tokens[0].cpu().numpy()  # [512, D]
    
    # è®¡ç®—åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼
    def compute_effective_dim(features, threshold=0.95):
        """è®¡ç®—ä¿ç•™ threshold æ–¹å·®æ‰€éœ€çš„ç»´åº¦æ•°"""
        # ä¸­å¿ƒåŒ–
        features = features - features.mean(axis=0)
        # SVD
        try:
            _, s, _ = np.linalg.svd(features, full_matrices=False)
            explained_variance = (s ** 2) / (s ** 2).sum()
            cumsum = np.cumsum(explained_variance)
            effective_dim = np.searchsorted(cumsum, threshold) + 1
            return effective_dim, explained_variance
        except:
            return features.shape[1], np.ones(features.shape[1]) / features.shape[1]
    
    img_eff_dim, img_var = compute_effective_dim(img_feat)
    tok_eff_dim, tok_var = compute_effective_dim(tok_feat)
    
    return {
        'image_effective_dim': img_eff_dim,
        'token_effective_dim': tok_eff_dim,
        'image_total_patches': img_feat.shape[0],
        'token_count': tok_feat.shape[0],
        'compression_ratio': img_feat.shape[0] / tok_feat.shape[0],
        'dim_retention_ratio': tok_eff_dim / img_eff_dim if img_eff_dim > 0 else 0,
    }


def main():
    parser = argparse.ArgumentParser(description='Q-Former Diagnostic Tool')
    parser.add_argument('--checkpoint', type=str, 
                        default='outputs/6x4090_fresh_20260125_143156/best_model_ema.pth',
                        help='Path to checkpoint')
    parser.add_argument('--num-samples', type=int, default=50,
                        help='Number of samples to analyze')
    parser.add_argument('--output-dir', type=str, default='qformer_diagnosis',
                        help='Output directory for visualizations')
    parser.add_argument('--dataroot', type=str, 
                        default='/home/cly/auto/llava_test/LLaVA/data/nuscenes',
                        help='nuScenes data root')
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åŠ è½½æ¨¡å‹
    print("\n" + "="*60)
    print("Loading model...")
    print("="*60)
    
    # æœ¬åœ°è·¯å¾„
    llm_path = "/home/cly/auto/llava_test/LLaVA/vicuna-7b-v1.5"
    
    model = LLaVAMapDetector(
        llm_path=llm_path,
        qformer_config={},
    )
    
    # åŠ è½½ checkpoint
    if os.path.exists(args.checkpoint):
        print(f"Loading checkpoint: {args.checkpoint}")
        checkpoint = torch.load(args.checkpoint, map_location='cpu')
        state_dict = checkpoint.get('model_state_dict', checkpoint.get('ema_state_dict', checkpoint))
        model.load_state_dict(state_dict, strict=False)
    else:
        print(f"Warning: Checkpoint not found at {args.checkpoint}, using random weights")
    
    model = model.to(device)
    model.eval()
    
    # åŠ è½½æ•°æ®
    print("\n" + "="*60)
    print("Loading dataset...")
    print("="*60)
    
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(llm_path, local_files_only=True)
    
    val_dataset = MapDetectionDataset(
        dataroot=args.dataroot,
        version='v1.0-trainval',
        split='val',
        gt_cache_path=os.path.join(args.dataroot, 'gt_cache'),
        tokenizer=tokenizer,
        use_augmentation=False,
    )
    
    # åˆ†æ
    print("\n" + "="*60)
    print(f"Analyzing Q-Former with {args.num_samples} samples...")
    print("="*60)
    
    all_metrics = []
    all_sim_matrices = []
    
    for i in tqdm(range(min(args.num_samples, len(val_dataset))), desc="Processing"):
        sample = val_dataset[i]
        images = sample['images'].unsqueeze(0).to(device)  # [1, 6, 3, H, W]
        
        try:
            image_features, scene_tokens, attn_weights, (h, w) = extract_qformer_features(
                model, images, device
            )
            
            metrics, sim_matrix = compute_reconstruction_metrics(
                image_features, scene_tokens, device
            )
            
            all_metrics.append(metrics)
            
            if i < 5:  # ä¿å­˜å‰ 5 ä¸ªæ ·æœ¬çš„ç›¸ä¼¼åº¦çŸ©é˜µç”¨äºå¯è§†åŒ–
                all_sim_matrices.append((sim_matrix, h, w))
                
        except Exception as e:
            print(f"Error processing sample {i}: {e}")
            continue
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("DIAGNOSIS RESULTS")
    print("="*60)
    
    avg_metrics = {}
    for key in all_metrics[0].keys():
        values = [m[key] for m in all_metrics]
        avg_metrics[key] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values),
        }
    
    print("\nğŸ“Š Coverage Metrics (higher is better):")
    print("-" * 50)
    
    print(f"\n1. Token-Patch Similarity:")
    print(f"   Average max similarity per token: {avg_metrics['avg_token_max_sim']['mean']:.4f} Â± {avg_metrics['avg_token_max_sim']['std']:.4f}")
    print(f"   Min max similarity (worst token): {avg_metrics['min_token_max_sim']['mean']:.4f}")
    
    print(f"\n2. Patch Coverage:")
    print(f"   Average patch coverage: {avg_metrics['avg_patch_coverage']['mean']:.4f} Â± {avg_metrics['avg_patch_coverage']['std']:.4f}")
    print(f"   Min patch coverage (worst patch): {avg_metrics['min_patch_coverage']['mean']:.4f}")
    print(f"   Uncovered patches ratio (<0.3): {avg_metrics['uncovered_patches_ratio']['mean']*100:.2f}%")
    
    # ä¿¡æ¯ä¿ç•™åˆ†æ
    if len(all_sim_matrices) > 0:
        sim_matrix, h, w = all_sim_matrices[0]
        
        # å¯è§†åŒ–è¦†ç›–æƒ…å†µ
        coverage_maps = visualize_attention_coverage(
            sim_matrix, h, w, 
            save_path=output_dir / 'coverage_heatmap.png'
        )
        
        # å¯è§†åŒ– token åˆ†å¸ƒ
        cam_counts = analyze_token_distribution(
            sim_matrix, h, w,
            save_path=output_dir / 'token_distribution.png'
        )
        
        # ä¿¡æ¯ä¿ç•™åˆ†æ
        info_metrics = compute_information_retention(
            image_features.cpu(), scene_tokens.cpu()
        )
        
        print(f"\n3. Information Retention:")
        print(f"   Original patches: {info_metrics['image_total_patches']}")
        print(f"   Scene tokens: {info_metrics['token_count']}")
        print(f"   Compression ratio: {info_metrics['compression_ratio']:.2f}x")
        print(f"   Image effective dim (95% var): {info_metrics['image_effective_dim']}")
        print(f"   Token effective dim (95% var): {info_metrics['token_effective_dim']}")
    
    # è¯Šæ–­ç»“è®º
    print("\n" + "="*60)
    print("ğŸ” DIAGNOSIS CONCLUSION")
    print("="*60)
    
    avg_coverage = avg_metrics['avg_patch_coverage']['mean']
    uncovered_ratio = avg_metrics['uncovered_patches_ratio']['mean']
    
    if avg_coverage > 0.7 and uncovered_ratio < 0.1:
        status = "âœ… GOOD"
        conclusion = "512 Scene Tokens èƒ½è¾ƒå¥½åœ°ä»£è¡¨ 6 å¼ å›¾åƒ"
    elif avg_coverage > 0.5 and uncovered_ratio < 0.3:
        status = "âš ï¸ MODERATE"
        conclusion = "512 Scene Tokens æœ‰ä¸€å®šä¿¡æ¯æŸå¤±ï¼Œå»ºè®®å¢åŠ  token æ•°é‡æˆ–æ”¹è¿› Q-Former"
    else:
        status = "âŒ POOR"
        conclusion = "512 Scene Tokens ä¿¡æ¯ä¸¥é‡ä¸è¶³ï¼Œè¿™æ˜¯æ€§èƒ½ç“¶é¢ˆçš„ä¸»è¦åŸå› "
    
    print(f"\nStatus: {status}")
    print(f"Conclusion: {conclusion}")
    
    print(f"\nğŸ“ Visualizations saved to: {output_dir}/")
    print("   - coverage_heatmap.png: æ¯ä¸ªç›¸æœºåŒºåŸŸè¢« tokens è¦†ç›–çš„ç¨‹åº¦")
    print("   - token_distribution.png: 512 tokens åœ¨ 6 ä¸ªç›¸æœºé—´çš„åˆ†å¸ƒ")
    
    # å»ºè®®
    print("\n" + "="*60)
    print("ğŸ’¡ RECOMMENDATIONS")
    print("="*60)
    
    if uncovered_ratio > 0.2:
        print("\n1. âš ï¸ æœ‰ {:.1f}% çš„å›¾åƒåŒºåŸŸæœªè¢«æœ‰æ•ˆè¦†ç›–".format(uncovered_ratio * 100))
        print("   å»ºè®®ï¼šå¢åŠ  Scene Tokens æ•°é‡ï¼ˆå¦‚ 1024ï¼‰æˆ–ä½¿ç”¨ Deformable Attention")
    
    if avg_coverage < 0.6:
        print("\n2. âš ï¸ å¹³å‡è¦†ç›–ç‡è¾ƒä½ ({:.2f})".format(avg_coverage))
        print("   å»ºè®®ï¼šæ£€æŸ¥ Q-Former çš„è®­ç»ƒæ˜¯å¦å……åˆ†ï¼Œæˆ–æ·»åŠ é‡å»ºæŸå¤±")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = output_dir / 'diagnosis_report.txt'
    with open(report_path, 'w') as f:
        f.write("Q-Former Diagnosis Report\n")
        f.write("="*50 + "\n\n")
        f.write(f"Samples analyzed: {len(all_metrics)}\n")
        f.write(f"Average patch coverage: {avg_coverage:.4f}\n")
        f.write(f"Uncovered patches ratio: {uncovered_ratio*100:.2f}%\n")
        f.write(f"Status: {status}\n")
        f.write(f"Conclusion: {conclusion}\n")
    
    print(f"\nğŸ“„ Report saved to: {report_path}")


if __name__ == '__main__':
    main()
