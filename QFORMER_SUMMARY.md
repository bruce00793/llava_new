# Q-Former å®ç°æ€»ç»“

## âœ… å·²å®Œæˆ

### 1. æ ¸å¿ƒæ¨¡å—ï¼š`llava/model/qformer.py`

**ç»„ä»¶**ï¼š
- âœ… `PositionEmbeddingSine`: 2Dä½ç½®ç¼–ç 
- âœ… `QFormer`: ä¸»æ¨¡å—
  - Backboneæ¥å£ï¼ˆé»˜è®¤ResNet-50ï¼‰
  - Position Encodingï¼ˆ2D + Camera IDï¼‰
  - Learnable Scene Queriesï¼ˆ512ä¸ªï¼‰
  - Transformer Decoderï¼ˆ6å±‚ï¼‰
  - Projectorï¼ˆ256 â†’ 4096ï¼‰
- âœ… `build_qformer()`: æ„å»ºå‡½æ•°

**ç‰¹ç‚¹**ï¼š
- ä»£ç é‡ï¼š~250è¡Œ
- å‚æ•°é‡ï¼š~40Mï¼ˆè½»é‡ï¼‰
- è®¾è®¡ï¼šå‚è€ƒORIONï¼Œç®€åŒ–å†—ä½™

---

## ğŸ“‹ æ¶æ„è®¾è®¡ï¼ˆå‚è€ƒORIONï¼‰

### æ ¸å¿ƒæµç¨‹

```
è¾“å…¥ï¼šimages (B, 6, 3, 336, 336)
    â†“
Backbone (ResNet50)
    â†“
img_feats (B*6, 256, 21, 21)
    â†“
Position Encoding (2D + Camera ID)
    â†“
memory (B, 2646, 256)  # 2646 = 6Ã—21Ã—21
    â†“
Transformer Decoder (512 queries, 6 layers)
    â†“
scene_features (B, 512, 256)
    â†“
Projector (MLP)
    â†“
è¾“å‡ºï¼šscene_tokens (B, 512, 4096)
```

### ä¸ORIONå¯¹æ¯”

| ç»„ä»¶ | ORION | Q-Formerï¼ˆä½ çš„ï¼‰ |
|------|-------|-----------------|
| Backbone | EVA-ViT (1B+) | ResNet50 (25M) |
| ä½ç½®ç¼–ç  | 3Dæ·±åº¦ç¼–ç  | 2D + Camera ID |
| Query | det(256) + map(257) | scene(512) |
| è¾“å‡º | (513, 4096) | (512, 4096) |
| å‚æ•°é‡ | ~1B | ~40M |

**ä¼˜åŠ¿**ï¼šç®€æ´ã€è½»é‡ã€æ˜“è°ƒè¯•

---

## ğŸ¯ å…³é”®è®¾è®¡ç‚¹

### 1. ç®€åŒ–çš„ä½ç½®ç¼–ç 

**ORIONæ–¹å¼**ï¼ˆå¤æ‚ï¼‰ï¼š
```python
# 3Dä½ç½®ç¼–ç 
coords3d = img2lidar @ pixel_coords  # éœ€è¦ç›¸æœºå‚æ•°
pos_embed = mlp(coords3d)  # å¤æ‚è®¡ç®—
```

**ä½ çš„æ–¹å¼**ï¼ˆç®€æ´ï¼‰ï¼š
```python
# 2Dä½ç½®ç¼–ç  + Camera ID
pos_embed = PositionEmbeddingSine(h, w)  # æ ‡å‡†2Dç¼–ç 
cam_embed = CameraEmbedding(cam_id)  # ç›¸æœºèº«ä»½
img_feats = img_feats + pos_embed + cam_embed
```

### 2. ç»Ÿä¸€çš„Scene Queries

**ORIONæ–¹å¼**ï¼š
```python
det_query = nn.Embedding(256, 256)  # ç‰©ä½“æ£€æµ‹
map_query = nn.Embedding(257, 256)  # åœ°å›¾æ£€æµ‹
# ä¸¤ä¸ªç‹¬ç«‹çš„headï¼Œåˆ†åˆ«å¤„ç†
```

**ä½ çš„æ–¹å¼**ï¼š
```python
scene_query = nn.Embedding(512, 256)  # ç»Ÿä¸€çš„åœºæ™¯query
# ä¸€ä¸ªDecoderå¤„ç†æ‰€æœ‰ä¿¡æ¯ï¼Œè‡ªåŠ¨åˆ†å·¥
```

### 3. Query-based Fusion

**æ ¸å¿ƒæ€æƒ³**ï¼ˆæ¥è‡ªDETR/ORIONï¼‰ï¼š
- 512ä¸ªå¯å­¦ä¹ queryå‘é‡
- é€šè¿‡Cross-Attentionä»å›¾åƒæå–ä¿¡æ¯
- è®­ç»ƒæ—¶è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªqueryçš„èŒè´£
- çµæ´»ã€é«˜æ•ˆã€å¯è§£é‡Šæ€§å¼º

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
llava/model/
â”œâ”€â”€ qformer.py                 # Q-Formerå®ç°ï¼ˆæ ¸å¿ƒï¼‰
â”œâ”€â”€ QFORMER_DESIGN.md          # è®¾è®¡æ–‡æ¡£
â””â”€â”€ map_config.py              # é…ç½®ï¼ˆå·²æœ‰ï¼‰
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```python
from llava.model.qformer import build_qformer

# é…ç½®
config = {
    'embed_dims': 256,
    'num_queries': 512,
    'num_decoder_layers': 6,
    'llm_hidden_size': 4096,
}

# æ„å»º
qformer = build_qformer(config)

# Forward
imgs = batch['images']  # (B, 6, 3, 336, 336)
scene_tokens = qformer(imgs)  # (B, 512, 4096)
```

### ä¸Dataseté›†æˆ

```python
from llava.data.map_dataset import create_dataloader

# åˆ›å»ºdataloader
dataloader = create_dataloader(...)

for batch in dataloader:
    imgs = batch['images']  # (B, 6, 3, 336, 336)
    
    # Q-Formeræå–åœºæ™¯ç‰¹å¾
    scene_tokens = qformer(imgs)  # (B, 512, 4096)
    
    # scene_tokensä¼šæ›¿æ¢text_idsä¸­çš„IMAGE_TOKEN_INDEX (-200)
```

---

## ğŸ”§ é…ç½®å‚æ•°

### æ¨èé…ç½®ï¼ˆé»˜è®¤ï¼‰

```python
config = {
    'embed_dims': 256,              # ç‰¹å¾ç»´åº¦
    'num_queries': 512,             # Scene queryæ•°é‡
    'num_decoder_layers': 6,        # Decoderå±‚æ•°
    'num_heads': 8,                 # Attentionå¤´æ•°
    'ffn_dims': 2048,               # FFNç»´åº¦
    'dropout': 0.1,                 # Dropoutç‡
    'llm_hidden_size': 4096,        # LLMç»´åº¦ï¼ˆå›ºå®šï¼‰
}
```

### å¯è°ƒå‚æ•°

| å‚æ•° | èŒƒå›´ | å½±å“ |
|------|------|------|
| `embed_dims` | 128-512 | ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ï¼Œè¶Šå¤§è¶Šå¼ºä½†è®¡ç®—é‡å¤§ |
| `num_queries` | 256-1024 | åœºæ™¯tokenæ•°é‡ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡ |
| `num_decoder_layers` | 3-12 | æ¨¡å‹æ·±åº¦ï¼Œè¶Šæ·±è¡¨è¾¾èƒ½åŠ›è¶Šå¼º |

---

## ğŸ“Š è®¡ç®—å¤æ‚åº¦

### å‚æ•°é‡åˆ†è§£

```
Backbone (ResNet50):      25.6M
Position Encoding:        å¯å¿½ç•¥
Query Embedding:          0.13M  (512Ã—256)
Decoder (6å±‚):            ~12M
Projector:                ~2M    (256â†’512â†’4096)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡:                     ~40M
```

### å†…å­˜å ç”¨ï¼ˆBatch=4ï¼‰

```
è¾“å…¥images:               4Ã—6Ã—3Ã—336Ã—336Ã—4 bytes = ~26MB
Backbone features:        4Ã—6Ã—256Ã—21Ã—21Ã—4 bytes = ~5MB
Memory (å±•å¹³):            4Ã—2646Ã—256Ã—4 bytes = ~11MB
Scene tokens:             4Ã—512Ã—4096Ã—4 bytes = ~34MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡:                     ~76MB (forward)
```

---

## ğŸ“ è®­ç»ƒç­–ç•¥

### é˜¶æ®µ1ï¼šå†»ç»“Backboneï¼ˆæ¨èå…ˆåšï¼‰

```python
# åªè®­ç»ƒDecoder + Projector
for name, param in qformer.named_parameters():
    if 'img_backbone' in name:
        param.requires_grad = False
    else:
        param.requires_grad = True
```

**ä¼˜åŠ¿**ï¼š
- è®­ç»ƒå¿«
- ç¨³å®š
- é€‚åˆåˆæœŸè°ƒè¯•

### é˜¶æ®µ2ï¼šç«¯åˆ°ç«¯å¾®è°ƒ

```python
# è§£å†»æ‰€æœ‰å‚æ•°
for param in qformer.parameters():
    param.requires_grad = True
```

**å­¦ä¹ ç‡å»ºè®®**ï¼š
- Backbone: 1e-5ï¼ˆå°å­¦ä¹ ç‡ï¼‰
- Decoder: 1e-4
- Projector: 1e-4

---

## ğŸ” è°ƒè¯•å»ºè®®

### 1. æµ‹è¯•Forward

```bash
cd /home/cly/auto/llava_test/LLaVA
python llava/model/qformer.py
```

é¢„æœŸè¾“å‡ºï¼š
```
âœ“ Input shape: (2, 6, 3, 336, 336)
âœ“ Output shape: (2, 512, 4096)
âœ“ Q-Former test passed!
```

### 2. æ£€æŸ¥æ¢¯åº¦æµ

```python
scene_tokens = qformer(imgs)
loss = scene_tokens.sum()
loss.backward()

# æ£€æŸ¥æ¢¯åº¦
for name, param in qformer.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.4f}")
```

### 3. å¯è§†åŒ–Attention

```python
# åœ¨Decoderä¸­æ·»åŠ return_attention=True
# å¯è§†åŒ–å“ªäº›queryå…³æ³¨å“ªäº›å›¾åƒåŒºåŸŸ
```

---

## ğŸš§ åç»­ä¼˜åŒ–æ–¹å‘

### 1. å‡çº§Backbone

```python
# ä»ResNet50å‡çº§åˆ°æ›´å¼ºçš„backbone
from timm import create_model
backbone = create_model('eva_giant_patch14_336', pretrained=True)
```

### 2. åŠ å…¥3Dä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼‰

```python
# å‚è€ƒORIONçš„3Dç¼–ç 
ray_dirs = compute_ray_direction(cam_intrinsics)
pos_embed_3d = mlp(ray_dirs)
```

### 3. Deformable Attentionï¼ˆé«˜æ•ˆï¼‰

```python
# æ›¿æ¢æ ‡å‡†Attention
from mmcv.ops import MultiScaleDeformableAttention
# è®¡ç®—é‡æ›´å°ï¼Œæ•ˆæœå¯èƒ½æ›´å¥½
```

---

## âœ… æ€»ç»“

### å·²å®ç°
1. âœ… æ ¸å¿ƒQ-Formeræ¨¡å—ï¼ˆ~250è¡Œä»£ç ï¼‰
2. âœ… å‚è€ƒORIONæ¶æ„ï¼Œç®€åŒ–å†—ä½™
3. âœ… å®Œæ•´çš„æ•°æ®æµï¼šimages â†’ scene_tokens
4. âœ… æ˜“äºç†è§£ã€è°ƒè¯•ã€æ‰©å±•

### ç‰¹ç‚¹
- **ç®€æ´**ï¼šåªä¿ç•™æ ¸å¿ƒç»„ä»¶
- **è½»é‡**ï¼š40Må‚æ•°ï¼ˆvs ORIONçš„1B+ï¼‰
- **é«˜æ•ˆ**ï¼šè®­ç»ƒå¿«ï¼Œæ˜“è°ƒè¯•
- **çµæ´»**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“æ‰©å±•

### ä¸‹ä¸€æ­¥
- é›†æˆåˆ°å®Œæ•´çš„è®­ç»ƒpipeline
- è¿æ¥LLMï¼ˆæ›¿æ¢IMAGE_TOKEN_INDEXï¼‰
- æ·»åŠ Output Querieså’ŒDetection Head

**Q-Formerå®ç°å®Œæˆï¼ä»£ç ç®€æ´ã€æ¶æ„æ¸…æ™°ã€å‚è€ƒORIONæ ¸å¿ƒæ€æƒ³ã€‚** ğŸ‰

