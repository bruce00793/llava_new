"""
Q-Former Verification - éªŒè¯ Q-Former 768 queries çš„åœºæ™¯è¡¨ç¤ºèƒ½åŠ›

============================================
éªŒè¯æ–¹æ³•ï¼šLinear Probingï¼ˆä¸šç•Œæ ‡å‡†ï¼‰
============================================
æ ¸å¿ƒæ€æƒ³ï¼šç”¨æœ€ç®€å•çš„çº¿æ€§å±‚éªŒè¯ç‰¹å¾è´¨é‡
- å¦‚æœç®€å•çš„çº¿æ€§å±‚å°±èƒ½å®Œæˆä»»åŠ¡ï¼Œè¯´æ˜ Q-Former æå–äº†è¶³å¤Ÿçš„åœºæ™¯ä¿¡æ¯
- ä¸å¼•å…¥å¤æ‚æ¨¡å—ï¼Œç»“æœ 100% åæ˜  Q-Former çš„èƒ½åŠ›

============================================
ä»»åŠ¡è®¾è®¡ï¼šåœºæ™¯çº§åˆ«ç›®æ ‡æ•°é‡é¢„æµ‹
============================================
è¾“å…¥ï¼š6 å¼ å›¾åƒ
è¾“å‡ºï¼šåœºæ™¯ä¸­å„ç±»ç›®æ ‡çš„æ•°é‡ [B, 13]

æ¶æ„ï¼š
6 å¼ å›¾ â†’ Q-Former â†’ 768 tokens [B, 768, 4096]
              â†“
      Global Average Pooling
              â†“
      scene_feature [B, 4096]
              â†“
      Linear Layerï¼ˆå”¯ä¸€å¯å­¦ä¹ å‚æ•°ï¼‰
              â†“
      å„ç±»ç›®æ ‡æ•°é‡ [B, 13]
      (car: 5, pedestrian: 2, divider: 1, ...)

============================================
éªŒè¯çš„é—®é¢˜
============================================
Q1: 768 tokens æ˜¯å¦åŒ…å«"åœºæ™¯é‡Œæœ‰ä»€ä¹ˆ"çš„ä¿¡æ¯ï¼Ÿ
    â†’ å¦‚æœæ•°é‡é¢„æµ‹å‡†ç¡®ï¼Œç­”æ¡ˆæ˜¯ YES

Q2: Q-Former èƒ½å¦åŒºåˆ†ä¸åŒç±»åˆ«ï¼Ÿ
    â†’ å¦‚æœå„ç±»æ•°é‡éƒ½å‡†ç¡®ï¼Œç­”æ¡ˆæ˜¯ YES

============================================
æˆåŠŸæ ‡å‡†
============================================
- æ•°é‡ MAE < 2: å¹³å‡æ¯ç±»çš„æ•°é‡è¯¯å·®å°äº 2 ä¸ª
- å­˜åœ¨æ€§å‡†ç¡®ç‡ > 80%: æ˜¯å¦å­˜åœ¨æŸç±»ç›®æ ‡çš„åˆ¤æ–­å‡†ç¡®ç‡

Author: Auto-generated
Date: 2025-02
"""

import os
import sys
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pickle
from typing import Dict, List, Optional, Tuple

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from llava.model.qformer import build_qformer


# ============================================
# é…ç½®
# ============================================
NUM_SCENE_QUERIES = 768  # ä¸ä¸»è®­ç»ƒæ¶æ„ä¸€è‡´
MAX_INSTANCES = 50       # æ¯ä¸ªåœºæ™¯æœ€å¤šé¢„æµ‹çš„å®ä¾‹æ•°ï¼ˆåœºæ™¯ä¸­ç›®æ ‡æ›´å¤šï¼‰

# æ‰€æœ‰åœºæ™¯ç±»åˆ«ï¼ˆ10 ç±» 3D ç›®æ ‡ + 3 ç±»åœ°å›¾å…ƒç´  = 13 ç±»ï¼‰
OBJECT_CATEGORIES = [
    'car', 'truck', 'bus', 'trailer', 'construction_vehicle',  # è½¦è¾†
    'pedestrian', 'motorcycle', 'bicycle',                      # è¡Œäººå’Œéª‘è¡Œè€…
    'barrier', 'traffic_cone',                                  # éšœç¢ç‰©
]
MAP_CATEGORIES = ['divider', 'ped_crossing', 'boundary']        # åœ°å›¾å…ƒç´ 

ALL_CATEGORIES = OBJECT_CATEGORIES + MAP_CATEGORIES  # å…± 13 ç±»
NUM_CLASSES = len(ALL_CATEGORIES)  # 13

# nuScenes ç±»åˆ«æ˜ å°„
NUSCENES_CATEGORY_MAP = {
    'vehicle.car': 'car',
    'vehicle.truck': 'truck',
    'vehicle.bus.bendy': 'bus',
    'vehicle.bus.rigid': 'bus',
    'vehicle.trailer': 'trailer',
    'vehicle.construction': 'construction_vehicle',
    'human.pedestrian.adult': 'pedestrian',
    'human.pedestrian.child': 'pedestrian',
    'human.pedestrian.construction_worker': 'pedestrian',
    'human.pedestrian.police_officer': 'pedestrian',
    'vehicle.motorcycle': 'motorcycle',
    'vehicle.bicycle': 'bicycle',
    'movable_object.barrier': 'barrier',
    'movable_object.trafficcone': 'traffic_cone',
}


class SceneCountingHead(nn.Module):
    """
    åœºæ™¯çº§åˆ«é¢„æµ‹å¤´ - Linear Probing éªŒè¯æ–¹æ³•ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    è®¾è®¡ç†å¿µï¼ˆå‚è€ƒä¸šç•Œæ ‡å‡† Linear Probingï¼‰ï¼š
    - ç”¨æœ€ç®€å•çš„ç»“æ„éªŒè¯ç‰¹å¾è´¨é‡
    - åªç”¨çº¿æ€§å±‚ï¼Œç»“æœ 100% åæ˜  Q-Former çš„èƒ½åŠ›
    
    ä»»åŠ¡ï¼š
    1. é¢„æµ‹åœºæ™¯ä¸­å„ç±»ç›®æ ‡çš„æ•°é‡ [B, 13]ï¼ˆåŸæœ‰ï¼‰
    2. é¢„æµ‹å„ç±»ç›®æ ‡çš„ä¸­å¿ƒä½ç½®å‡å€¼ [B, 13, 2]ï¼ˆæ–°å¢ï¼‰
    3. é¢„æµ‹å„ç±»ç›®æ ‡çš„ä½ç½®åˆ†æ•£åº¦ [B, 13, 2]ï¼ˆæ–°å¢ï¼‰
    
    è¾“å…¥ï¼š768 scene tokens [B, 768, 4096]
    """
    
    def __init__(
        self,
        input_dim: int = 4096,      # Q-Former è¾“å‡ºç»´åº¦
        num_classes: int = 13,      # åœºæ™¯ç±»åˆ«æ•°ï¼ˆ10 ç±» 3D + 3 ç±»åœ°å›¾ï¼‰
    ):
        super().__init__()
        
        self.num_classes = num_classes
        
        # Linear Probing: åªç”¨çº¿æ€§å±‚ï¼
        # ä»»åŠ¡ 1: æ•°é‡é¢„æµ‹ [B, 13]
        self.count_head = nn.Linear(input_dim, num_classes)
        
        # ä»»åŠ¡ 2: ä½ç½®å‡å€¼é¢„æµ‹ [B, 13, 2] (æ–°å¢)
        # é¢„æµ‹å„ç±»ç›®æ ‡çš„å¹³å‡ä¸­å¿ƒä½ç½® (x, y)
        self.center_head = nn.Linear(input_dim, num_classes * 2)
        
        # ä»»åŠ¡ 3: ä½ç½®æ–¹å·®é¢„æµ‹ [B, 13, 2] (æ–°å¢)
        # é¢„æµ‹å„ç±»ç›®æ ‡ä½ç½®çš„åˆ†æ•£ç¨‹åº¦
        self.variance_head = nn.Linear(input_dim, num_classes * 2)
        
        self._init_weights()
    
    def _init_weights(self):
        for module in [self.count_head, self.center_head, self.variance_head]:
            nn.init.xavier_uniform_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, scene_tokens: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        é¢„æµ‹åœºæ™¯ä¸­å„ç±»ç›®æ ‡çš„æ•°é‡å’Œä½ç½®ç»Ÿè®¡ä¿¡æ¯ã€‚
        
        Args:
            scene_tokens: [B, 768, 4096] - Q-Former è¾“å‡ºçš„ scene tokens
        
        Returns:
            pred_counts: [B, 13] - å„ç±»ç›®æ ‡çš„é¢„æµ‹æ•°é‡
            pred_centers: [B, 13, 2] - å„ç±»ç›®æ ‡çš„é¢„æµ‹ä¸­å¿ƒå‡å€¼
            pred_variances: [B, 13, 2] - å„ç±»ç›®æ ‡çš„é¢„æµ‹ä½ç½®æ–¹å·®
        """
        B = scene_tokens.shape[0]
        
        # 1. Global Average Pooling: 768 tokens â†’ 1 ä¸ªåœºæ™¯å‘é‡
        scene_feature = scene_tokens.mean(dim=1)  # [B, 4096]
        
        # 2. æ•°é‡é¢„æµ‹
        pred_counts = self.count_head(scene_feature)  # [B, 13]
        pred_counts = F.relu(pred_counts)  # ç¡®ä¿æ•°é‡éè´Ÿ
        
        # 3. ä¸­å¿ƒä½ç½®é¢„æµ‹ï¼ˆæ–°å¢ï¼‰
        pred_centers = self.center_head(scene_feature)  # [B, 13*2]
        pred_centers = pred_centers.view(B, self.num_classes, 2)  # [B, 13, 2]
        pred_centers = torch.sigmoid(pred_centers)  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        # 4. ä½ç½®æ–¹å·®é¢„æµ‹ï¼ˆæ–°å¢ï¼‰
        pred_variances = self.variance_head(scene_feature)  # [B, 13*2]
        pred_variances = pred_variances.view(B, self.num_classes, 2)  # [B, 13, 2]
        pred_variances = F.relu(pred_variances)  # æ–¹å·®éè´Ÿ
        
        return {
            'pred_counts': pred_counts,      # [B, 13]
            'pred_centers': pred_centers,    # [B, 13, 2] æ–°å¢
            'pred_variances': pred_variances,  # [B, 13, 2] æ–°å¢
        }


class QFormerVerificationModel(nn.Module):
    """
    Q-Former éªŒè¯æ¨¡å‹ã€‚
    
    æ¶æ„ï¼šQ-Former â†’ ç®€å•æ£€æµ‹å¤´
    ä¸ä½¿ç”¨ LLMï¼
    """
    
    def __init__(self):
        super().__init__()
        
        print("\n" + "="*60)
        print("Q-Former Verification Model")
        print(f"  éªŒè¯ç›®æ ‡: {NUM_SCENE_QUERIES} scene queries çš„åœºæ™¯è¡¨ç¤ºèƒ½åŠ›")
        print("  ä¸ä½¿ç”¨ LLMï¼Œç›´æ¥éªŒè¯ Q-Former")
        print("="*60)
        
        # 1. Q-Formerï¼ˆé…ç½®ä¸ä¸»è®­ç»ƒå®Œå…¨ä¸€è‡´ï¼‰
        qformer_config = {
            'img_backbone': 'resnet50',
            'embed_dims': 256,
            'num_queries': NUM_SCENE_QUERIES,  # 768
            'num_decoder_layers': 6,
            'llm_hidden_size': 4096,
            'num_heads': 8,
            'ffn_dims': 2048,
            'dropout': 0.1,
            'num_cams': 6,  # ç›¸æœºæ•°é‡
            # 3D Position Encoding Configï¼ˆä¸ qformer.py é»˜è®¤å€¼ä¸€è‡´ï¼‰
            'depth_num': 32,          # 32 ä¸ªæ·±åº¦å‡è®¾
            'depth_start': 1.0,
            'depth_max': 60.0,
            'use_lid': True,          # LID æ·±åº¦åˆ†å¸ƒ
            'pc_range': [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0],  # BEV èŒƒå›´
        }
        self.qformer = build_qformer(qformer_config)
        print(f"âœ… Q-Former initialized ({NUM_SCENE_QUERIES} queries, ä¸ä¸»è®­ç»ƒé…ç½®ä¸€è‡´)")
        
        # 2. åœºæ™¯æ•°é‡é¢„æµ‹å¤´ï¼ˆLinear Probing - åªç”¨ 1 ä¸ªçº¿æ€§å±‚ï¼‰
        self.counting_head = SceneCountingHead(
            input_dim=4096,
            num_classes=NUM_CLASSES,  # 13 ç±»ï¼ˆ10 ç±» 3D + 3 ç±»åœ°å›¾ï¼‰
        )
        print(f"âœ… Counting head initialized (Linear Probing: åªç”¨ 1 ä¸ªçº¿æ€§å±‚é¢„æµ‹ {NUM_CLASSES} ç±»æ•°é‡)")
        
        print("="*60 + "\n")
    
    def forward(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: [B, 6, 3, H, W] - 6 ä¸ªç›¸æœºå›¾åƒ
        
        Returns:
            pred_counts: [B, 13] - å„ç±»ç›®æ ‡çš„é¢„æµ‹æ•°é‡
        """
        # Q-Former: 6 images â†’ 768 scene tokens
        scene_tokens = self.qformer(images)  # [B, 768, 4096]
        
        # Counting head: 768 tokens â†’ åœºæ™¯æ•°é‡é¢„æµ‹
        outputs = self.counting_head(scene_tokens)
        
        return outputs


class QFormerVerificationDataset(Dataset):
    """
    Q-Former éªŒè¯æ•°æ®é›†ã€‚
    
    åŠ è½½æ‰€æœ‰åœºæ™¯ç›®æ ‡ï¼š
    - 10 ç±» 3D ç›®æ ‡ï¼ˆä» nuScenes annotationsï¼‰
    - 3 ç±»åœ°å›¾å…ƒç´ ï¼ˆä» GT cacheï¼‰
    
    éªŒè¯ 768 scene tokens èƒ½å¦ä»£è¡¨æ•´ä¸ªåœºæ™¯çš„ä¿¡æ¯ã€‚
    """
    
    def __init__(
        self,
        dataroot: str,
        version: str,
        split: str,
        gt_cache_path: str,
        sample_ratio: float = 1.0,
    ):
        self.dataroot = dataroot
        self.version = version
        self.split = split
        
        # Load nuScenes
        from nuscenes import NuScenes
        print(f"Loading nuScenes {version} from {dataroot}...")
        self.nusc = NuScenes(version=version, dataroot=dataroot, verbose=True)
        
        # Get sample tokens
        self.sample_tokens = self._get_split_tokens(split)
        
        # Apply sample ratio
        if sample_ratio < 1.0:
            num_samples = int(len(self.sample_tokens) * sample_ratio)
            random.shuffle(self.sample_tokens)
            self.sample_tokens = self.sample_tokens[:num_samples]
        
        # GT cache for map elements
        self.gt_ann_dir = os.path.join(gt_cache_path, 'annotations')
        
        # Camera order (ä¸ä¸»è®­ç»ƒä¸€è‡´)
        self.cam_names = [
            'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT',
            'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT'
        ]
        
        # Image preprocessing (ä¸ä¸»è®­ç»ƒä¸€è‡´)
        self.target_img_size = (800, 448)
        self.img_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
        self.img_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
        
        print(f"Loaded {len(self.sample_tokens)} samples for {split}")
        print(f"  é¢„æµ‹ç±»åˆ«: {NUM_CLASSES} ç±» ({len(OBJECT_CATEGORIES)} ç±» 3D ç›®æ ‡ + {len(MAP_CATEGORIES)} ç±»åœ°å›¾å…ƒç´ )")
    
    def _get_split_tokens(self, split: str) -> List[str]:
        from nuscenes.utils.splits import create_splits_scenes
        
        split_scenes = create_splits_scenes()
        if self.version == 'v1.0-mini':
            scene_names = split_scenes['mini_train'] if split == 'train' else split_scenes['mini_val']
        else:
            scene_names = split_scenes['train'] if split == 'train' else split_scenes['val']
        
        sample_tokens = []
        for scene in self.nusc.scene:
            if scene['name'] in scene_names:
                sample_token = scene['first_sample_token']
                while sample_token:
                    sample_tokens.append(sample_token)
                    sample = self.nusc.get('sample', sample_token)
                    sample_token = sample['next']
        
        return sample_tokens
    
    def __len__(self):
        return len(self.sample_tokens)
    
    def _load_images(self, sample_token: str) -> torch.Tensor:
        """åŠ è½½å¹¶é¢„å¤„ç† 6 å¼ å›¾åƒ"""
        from PIL import Image
        
        sample = self.nusc.get('sample', sample_token)
        images = []
        
        for cam_name in self.cam_names:
            cam_data = self.nusc.get('sample_data', sample['data'][cam_name])
            img_path = os.path.join(self.dataroot, cam_data['filename'])
            
            img = Image.open(img_path).convert('RGB')
            img = img.resize(self.target_img_size, Image.BILINEAR)
            
            # Normalize
            img_array = np.array(img, dtype=np.float32) / 255.0
            img_array = (img_array - self.img_mean) / self.img_std
            img_tensor = torch.from_numpy(img_array.transpose(2, 0, 1)).float()
            
            images.append(img_tensor)
        
        return torch.stack(images, dim=0)  # [6, 3, H, W]
    
    def _load_3d_objects(self, sample_token: str) -> List[Tuple[int, float, float]]:
        """
        åŠ è½½ 3D ç›®æ ‡ï¼ˆ10 ç±»ï¼‰ä» nuScenes annotationsã€‚
        
        Returns:
            List of (class_id, x_norm, y_norm)
        """
        from pyquaternion import Quaternion
        
        sample = self.nusc.get('sample', sample_token)
        
        # Get ego pose
        lidar_data = self.nusc.get('sample_data', sample['data']['LIDAR_TOP'])
        ego_pose = self.nusc.get('ego_pose', lidar_data['ego_pose_token'])
        ego_translation = np.array(ego_pose['translation'])
        ego_rotation = Quaternion(ego_pose['rotation'])
        
        instances = []
        for ann_token in sample['anns']:
            ann = self.nusc.get('sample_annotation', ann_token)
            
            # Get category
            category_name = ann['category_name']
            if category_name not in NUSCENES_CATEGORY_MAP:
                continue
            category = NUSCENES_CATEGORY_MAP[category_name]
            
            if category not in OBJECT_CATEGORIES:
                continue
            
            class_id = ALL_CATEGORIES.index(category)
            
            # Transform to ego frame
            global_pos = np.array(ann['translation'][:2])
            pos_ego = ego_rotation.inverse.rotate(np.append(global_pos - ego_translation[:2], 0))[:2]
            x_ego, y_ego = pos_ego[0], pos_ego[1]
            
            # èŒƒå›´æ£€æŸ¥: x in [-15, 15], y in [-30, 30]
            if not (-15 <= x_ego <= 15 and -30 <= y_ego <= 30):
                continue
            
            # å½’ä¸€åŒ–åˆ° [0, 1]
            x_norm = (x_ego + 15) / 30
            y_norm = (y_ego + 30) / 60
            
            instances.append((class_id, x_norm, y_norm))
        
        return instances
    
    def _load_map_elements(self, sample_token: str) -> List[Tuple[int, float, float]]:
        """
        åŠ è½½åœ°å›¾å…ƒç´ ï¼ˆ3 ç±»ï¼‰ä» GT cacheã€‚
        
        Returns:
            List of (class_id, x_norm, y_norm)
        """
        gt_file = os.path.join(self.gt_ann_dir, f'{sample_token}.pkl')
        
        if not os.path.exists(gt_file):
            return []
        
        with open(gt_file, 'rb') as f:
            gt_data = pickle.load(f)
        
        instances = []
        gt_classes = gt_data['gt_classes']
        gt_points = gt_data['gt_points']  # [N, 20, 2]
        
        for i, (cls_id, points) in enumerate(zip(gt_classes, gt_points)):
            # åœ°å›¾å…ƒç´ ç±»åˆ« ID = 10 + cls_id (å› ä¸ºå‰ 10 ä¸ªæ˜¯ 3D ç›®æ ‡)
            class_id = len(OBJECT_CATEGORIES) + cls_id
            
            # è®¡ç®—ä¸­å¿ƒç‚¹
            center_x = (points[:, 0].mean() + 15) / 30
            center_y = (points[:, 1].mean() + 30) / 60
            
            if 0 <= center_x <= 1 and 0 <= center_y <= 1:
                instances.append((class_id, center_x, center_y))
        
        return instances
    
    def _load_gt_stats(self, sample_token: str) -> Dict[str, torch.Tensor]:
        """
        åŠ è½½åœºæ™¯ä¸­å„ç±»ç›®æ ‡çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆ Linear Probingï¼‰
        
        Returns:
            counts: [13] - æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡
            centers: [13, 2] - æ¯ä¸ªç±»åˆ«çš„ä¸­å¿ƒä½ç½®å‡å€¼
            variances: [13, 2] - æ¯ä¸ªç±»åˆ«çš„ä½ç½®æ–¹å·®
            exist_mask: [13] - æ¯ä¸ªç±»åˆ«æ˜¯å¦å­˜åœ¨ï¼ˆç”¨äº Loss è®¡ç®—ï¼‰
        """
        counts = torch.zeros(NUM_CLASSES, dtype=torch.float32)
        centers = torch.zeros(NUM_CLASSES, 2, dtype=torch.float32)
        variances = torch.zeros(NUM_CLASSES, 2, dtype=torch.float32)
        exist_mask = torch.zeros(NUM_CLASSES, dtype=torch.float32)
        
        # åŠ è½½ 3D ç›®æ ‡å’Œåœ°å›¾å…ƒç´ 
        obj_instances = self._load_3d_objects(sample_token)
        map_instances = self._load_map_elements(sample_token)
        all_instances = obj_instances + map_instances
        
        # æŒ‰ç±»åˆ«åˆ†ç»„å®ä¾‹
        class_positions = {i: [] for i in range(NUM_CLASSES)}
        for class_id, x, y in all_instances:
            if 0 <= class_id < NUM_CLASSES:
                class_positions[class_id].append([x, y])
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
        for class_id in range(NUM_CLASSES):
            positions = class_positions[class_id]
            counts[class_id] = len(positions)
            
            if len(positions) > 0:
                exist_mask[class_id] = 1.0
                pos_array = np.array(positions)  # [N, 2]
                
                # ä¸­å¿ƒå‡å€¼
                centers[class_id, 0] = pos_array[:, 0].mean()
                centers[class_id, 1] = pos_array[:, 1].mean()
                
                # ä½ç½®æ–¹å·®ï¼ˆå¦‚æœåªæœ‰ 1 ä¸ªå®ä¾‹ï¼Œæ–¹å·®ä¸º 0ï¼‰
                if len(positions) > 1:
                    variances[class_id, 0] = pos_array[:, 0].var()
                    variances[class_id, 1] = pos_array[:, 1].var()
        
        return {
            'counts': counts,        # [13]
            'centers': centers,      # [13, 2]
            'variances': variances,  # [13, 2]
            'exist_mask': exist_mask,  # [13]
        }
    
    def __getitem__(self, idx: int) -> Dict:
        sample_token = self.sample_tokens[idx]
        
        images = self._load_images(sample_token)
        gt_stats = self._load_gt_stats(sample_token)
        
        return {
            'images': images,
            'counts': gt_stats['counts'],        # [13] å„ç±»ç›®æ ‡æ•°é‡
            'centers': gt_stats['centers'],      # [13, 2] å„ç±»ä¸­å¿ƒå‡å€¼
            'variances': gt_stats['variances'],  # [13, 2] å„ç±»ä½ç½®æ–¹å·®
            'exist_mask': gt_stats['exist_mask'],  # [13] å­˜åœ¨æ€§æ©ç 
            'sample_token': sample_token,
        }


def collate_fn(batch):
    images = torch.stack([item['images'] for item in batch])
    counts = torch.stack([item['counts'] for item in batch])        # [B, 13]
    centers = torch.stack([item['centers'] for item in batch])      # [B, 13, 2]
    variances = torch.stack([item['variances'] for item in batch])  # [B, 13, 2]
    exist_mask = torch.stack([item['exist_mask'] for item in batch])  # [B, 13]
    tokens = [item['sample_token'] for item in batch]
    
    return {
        'images': images,
        'counts': counts,        # GT: å„ç±»ç›®æ ‡æ•°é‡
        'centers': centers,      # GT: å„ç±»ä¸­å¿ƒå‡å€¼
        'variances': variances,  # GT: å„ç±»ä½ç½®æ–¹å·®
        'exist_mask': exist_mask,  # GT: å­˜åœ¨æ€§æ©ç 
        'sample_tokens': tokens,
    }


class CountingLoss(nn.Module):
    """
    å¢å¼ºç‰ˆåœºæ™¯é¢„æµ‹æŸå¤±å‡½æ•°
    
    ä»»åŠ¡ 1: æ•°é‡é¢„æµ‹ - MSE Loss
    ä»»åŠ¡ 2: ä¸­å¿ƒä½ç½®é¢„æµ‹ - Masked L1 Lossï¼ˆä»…å¯¹å­˜åœ¨çš„ç±»åˆ«è®¡ç®—ï¼‰
    ä»»åŠ¡ 3: ä½ç½®æ–¹å·®é¢„æµ‹ - Masked L1 Lossï¼ˆä»…å¯¹å­˜åœ¨çš„ç±»åˆ«è®¡ç®—ï¼‰
    
    Loss æƒé‡è®¾è®¡ï¼š
    - æ•°é‡é¢„æµ‹æ˜¯ä¸»ä»»åŠ¡ï¼Œæƒé‡ 1.0
    - ä½ç½®é¢„æµ‹æ˜¯è¾…åŠ©éªŒè¯ï¼Œæƒé‡è¾ƒä½ 0.5
    """
    
    def __init__(
        self, 
        num_classes: int = 13,
        weight_count: float = 1.0,
        weight_center: float = 0.5,
        weight_variance: float = 0.2,
    ):
        super().__init__()
        self.num_classes = num_classes
        self.weight_count = weight_count
        self.weight_center = weight_center
        self.weight_variance = weight_variance
    
    def forward(
        self,
        pred_counts: torch.Tensor,     # [B, 13]
        pred_centers: torch.Tensor,    # [B, 13, 2]
        pred_variances: torch.Tensor,  # [B, 13, 2]
        gt_counts: torch.Tensor,       # [B, 13]
        gt_centers: torch.Tensor,      # [B, 13, 2]
        gt_variances: torch.Tensor,    # [B, 13, 2]
        exist_mask: torch.Tensor,      # [B, 13]
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—ç»¼åˆæŸå¤±
        
        Returns:
            loss: æ€»æŸå¤±
            loss_count: æ•°é‡é¢„æµ‹æŸå¤±
            loss_center: ä¸­å¿ƒé¢„æµ‹æŸå¤±
            loss_variance: æ–¹å·®é¢„æµ‹æŸå¤±
            mae: æ•°é‡ MAEï¼ˆç›‘æ§ï¼‰
            center_mae: ä¸­å¿ƒ MAEï¼ˆç›‘æ§ï¼‰
        """
        # 1. æ•°é‡é¢„æµ‹æŸå¤± (MSE)
        loss_count = F.mse_loss(pred_counts, gt_counts)
        
        # 2. ä¸­å¿ƒä½ç½®é¢„æµ‹æŸå¤± (Masked L1)
        # åªå¯¹å­˜åœ¨çš„ç±»åˆ«è®¡ç®—
        if exist_mask.sum() > 0:
            mask_expanded = exist_mask.unsqueeze(-1).expand_as(pred_centers)  # [B, 13, 2]
            center_diff = (pred_centers - gt_centers).abs() * mask_expanded
            loss_center = center_diff.sum() / (exist_mask.sum() * 2 + 1e-6)
        else:
            loss_center = torch.tensor(0.0, device=pred_counts.device)
        
        # 3. æ–¹å·®é¢„æµ‹æŸå¤± (Masked L1)
        if exist_mask.sum() > 0:
            var_diff = (pred_variances - gt_variances).abs() * mask_expanded
            loss_variance = var_diff.sum() / (exist_mask.sum() * 2 + 1e-6)
        else:
            loss_variance = torch.tensor(0.0, device=pred_counts.device)
        
        # æ€»æŸå¤±
        loss = (
            self.weight_count * loss_count +
            self.weight_center * loss_center +
            self.weight_variance * loss_variance
        )
        
        # ç›‘æ§æŒ‡æ ‡
        with torch.no_grad():
            mae = F.l1_loss(pred_counts, gt_counts)
            
            if exist_mask.sum() > 0:
                center_mae = center_diff.sum() / (exist_mask.sum() * 2 + 1e-6)
            else:
                center_mae = torch.tensor(0.0, device=pred_counts.device)
        
        return {
            'loss': loss,
            'loss_count': loss_count,
            'loss_center': loss_center,
            'loss_variance': loss_variance,
            'mae': mae,
            'center_mae': center_mae,
        }


def train_epoch(model, dataloader, criterion, optimizer, scaler, epoch, args, scheduler=None):
    """
    è®­ç»ƒä¸€ä¸ª epoch - å¢å¼ºç‰ˆåœºæ™¯é¢„æµ‹ä»»åŠ¡
    """
    model.train()
    total_loss = 0
    total_loss_count = 0
    total_loss_center = 0
    total_mae = 0
    total_center_mae = 0
    num_batches = 0
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
    optimizer.zero_grad()
    
    for step, batch in enumerate(pbar):
        images = batch['images'].cuda()           # [B, 6, 3, H, W]
        gt_counts = batch['counts'].cuda()        # [B, 13]
        gt_centers = batch['centers'].cuda()      # [B, 13, 2]
        gt_variances = batch['variances'].cuda()  # [B, 13, 2]
        exist_mask = batch['exist_mask'].cuda()   # [B, 13]
        
        with torch.cuda.amp.autocast(enabled=args.fp16):
            outputs = model(images)
            losses = criterion(
                outputs['pred_counts'],      # [B, 13]
                outputs['pred_centers'],     # [B, 13, 2]
                outputs['pred_variances'],   # [B, 13, 2]
                gt_counts,
                gt_centers,
                gt_variances,
                exist_mask,
            )
            loss = losses['loss'] / args.accumulation_steps
        
        if scaler is not None:
            scaler.scale(loss).backward()
        else:
            loss.backward()
        
        if (step + 1) % args.accumulation_steps == 0:
            if scaler is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
                optimizer.step()
            optimizer.zero_grad()
            
            # Scheduler step after optimizer step
            if scheduler is not None:
                scheduler.step()
        
        total_loss += losses['loss'].item()
        total_loss_count += losses['loss_count'].item()
        total_loss_center += losses['loss_center'].item()
        total_mae += losses['mae'].item()
        total_center_mae += losses['center_mae'].item()
        num_batches += 1
        
        pbar.set_postfix({
            'loss': f"{losses['loss'].item():.4f}",
            'MAE': f"{losses['mae'].item():.2f}",
            'ctr': f"{losses['center_mae'].item():.3f}",
        })
    
    return {
        'loss': total_loss / num_batches,
        'loss_count': total_loss_count / num_batches,
        'loss_center': total_loss_center / num_batches,
        'mae': total_mae / num_batches,
        'center_mae': total_center_mae / num_batches,
    }


@torch.no_grad()
def validate(model, dataloader, criterion, epoch, args):
    """
    éªŒè¯ - å¢å¼ºç‰ˆåœºæ™¯é¢„æµ‹ä»»åŠ¡
    
    è¯„ä¼°æŒ‡æ ‡ï¼š
    1. æ•°é‡ MAE: å„ç±»ç›®æ ‡æ•°é‡çš„å¹³å‡ç»å¯¹è¯¯å·®
    2. å­˜åœ¨æ€§å‡†ç¡®ç‡: åˆ¤æ–­æŸç±»æ˜¯å¦å­˜åœ¨çš„å‡†ç¡®ç‡
    3. ä¸­å¿ƒä½ç½® MAE: å­˜åœ¨ç±»åˆ«çš„ä¸­å¿ƒé¢„æµ‹è¯¯å·®ï¼ˆæ–°å¢ï¼‰
    4. åˆ†ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    """
    model.eval()
    total_loss = 0
    total_mae = 0
    total_center_mae = 0
    num_batches = 0
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’Œ GT
    all_pred_counts = []
    all_gt_counts = []
    all_pred_centers = []
    all_gt_centers = []
    all_exist_masks = []
    
    for batch in tqdm(dataloader, desc="Validating"):
        images = batch['images'].cuda()
        gt_counts = batch['counts'].cuda()
        gt_centers = batch['centers'].cuda()
        gt_variances = batch['variances'].cuda()
        exist_mask = batch['exist_mask'].cuda()
        
        with torch.cuda.amp.autocast(enabled=args.fp16):
            outputs = model(images)
            losses = criterion(
                outputs['pred_counts'],
                outputs['pred_centers'],
                outputs['pred_variances'],
                gt_counts,
                gt_centers,
                gt_variances,
                exist_mask,
            )
        
        total_loss += losses['loss'].item()
        total_mae += losses['mae'].item()
        total_center_mae += losses['center_mae'].item()
        num_batches += 1
        
        all_pred_counts.append(outputs['pred_counts'].cpu())
        all_gt_counts.append(gt_counts.cpu())
        all_pred_centers.append(outputs['pred_centers'].cpu())
        all_gt_centers.append(gt_centers.cpu())
        all_exist_masks.append(exist_mask.cpu())
    
    # æ±‡æ€»
    all_pred_counts = torch.cat(all_pred_counts, dim=0)    # [N, 13]
    all_gt_counts = torch.cat(all_gt_counts, dim=0)        # [N, 13]
    all_pred_centers = torch.cat(all_pred_centers, dim=0)  # [N, 13, 2]
    all_gt_centers = torch.cat(all_gt_centers, dim=0)      # [N, 13, 2]
    all_exist_masks = torch.cat(all_exist_masks, dim=0)    # [N, 13]
    
    avg_loss = total_loss / num_batches
    avg_mae = total_mae / num_batches
    avg_center_mae = total_center_mae / num_batches
    
    # ===== æ•°é‡é¢„æµ‹æŒ‡æ ‡ =====
    # åˆ†ç±»åˆ«æ•°é‡ MAE
    per_class_count_mae = (all_pred_counts - all_gt_counts).abs().mean(dim=0)  # [13]
    
    # å­˜åœ¨æ€§å‡†ç¡®ç‡
    pred_exist = (all_pred_counts > 0.5).float()
    gt_exist = (all_gt_counts > 0).float()
    exist_acc = (pred_exist == gt_exist).float().mean()
    
    # ===== ä½ç½®é¢„æµ‹æŒ‡æ ‡ï¼ˆæ–°å¢ï¼‰=====
    # åˆ†ç±»åˆ«ä¸­å¿ƒ MAEï¼ˆåªå¯¹å­˜åœ¨çš„æ ·æœ¬è®¡ç®—ï¼‰
    per_class_center_mae = torch.zeros(NUM_CLASSES, 2)
    for c in range(NUM_CLASSES):
        mask = all_exist_masks[:, c] > 0  # [N]
        if mask.sum() > 0:
            center_diff = (all_pred_centers[mask, c] - all_gt_centers[mask, c]).abs()  # [num_exist, 2]
            per_class_center_mae[c] = center_diff.mean(dim=0)  # [2]
    
    # ä¸­å¿ƒé¢„æµ‹çš„æ•´ä½“ MAEï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼Œè½¬æ¢åˆ°ç±³ï¼‰
    # åæ ‡èŒƒå›´ x: [-15, 15]m, y: [-30, 30]m
    # center_mae æ˜¯ [0,1] å½’ä¸€åŒ–ç©ºé—´çš„è¯¯å·®
    center_mae_meters_x = avg_center_mae * 30  # 30m èŒƒå›´
    center_mae_meters_y = avg_center_mae * 60  # 60m èŒƒå›´
    
    # ===== è¯¦ç»†è¾“å‡º =====
    print(f"\n{'='*80}")
    print(f"Epoch {epoch+1} Validation Results (å¢å¼ºç‰ˆ)")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š æ•°é‡é¢„æµ‹æŒ‡æ ‡:")
    print(f"  Overall Count MAE: {avg_mae:.2f} (ç›®æ ‡: < 2.0)")
    print(f"  Existence Accuracy: {exist_acc*100:.1f}% (ç›®æ ‡: > 80%)")
    
    print(f"\nğŸ“ ä½ç½®é¢„æµ‹æŒ‡æ ‡ (æ–°å¢):")
    print(f"  Overall Center MAE: {avg_center_mae:.4f} (å½’ä¸€åŒ–ç©ºé—´)")
    print(f"  Approx Center Error: X ~{center_mae_meters_x:.1f}m, Y ~{center_mae_meters_y:.1f}m")
    
    print(f"\n  Loss: {avg_loss:.4f}")
    
    print(f"\n  Per-class æ•°é‡ MAE / ä¸­å¿ƒ MAE:")
    
    # 3D ç›®æ ‡
    print(f"    3D Objects:")
    for i, name in enumerate(OBJECT_CATEGORIES):
        cx, cy = per_class_center_mae[i]
        print(f"      {name:20s}: count={per_class_count_mae[i]:.2f}, center=({cx:.3f}, {cy:.3f})")
    
    # åœ°å›¾å…ƒç´ 
    print(f"    Map Elements:")
    for i, name in enumerate(MAP_CATEGORIES):
        idx = len(OBJECT_CATEGORIES) + i
        cx, cy = per_class_center_mae[idx]
        print(f"      {name:20s}: count={per_class_count_mae[idx]:.2f}, center=({cx:.3f}, {cy:.3f})")
    
    print(f"{'='*80}")
    
    # ===== ç»¼åˆåˆ¤æ–­ =====
    count_ok = avg_mae < 2.0 and exist_acc > 0.8
    center_ok = avg_center_mae < 0.15  # å½’ä¸€åŒ–ç©ºé—´è¯¯å·® < 0.15 (çº¦ 4.5m x, 9m y)
    
    if count_ok and center_ok:
        print(f"  âœ… éªŒè¯æˆåŠŸ! Q-Former 768 tokens èƒ½å¤Ÿæœ‰æ•ˆè¡¨ç¤ºåœºæ™¯çš„ã€è¯­ä¹‰ã€‘å’Œã€ä½ç½®ã€‘ä¿¡æ¯")
    elif count_ok and not center_ok:
        print(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸ: æ•°é‡é¢„æµ‹ OKï¼Œä½†ä½ç½®ä¿¡æ¯å¯èƒ½ä¸è¶³")
        print(f"     â†’ å»ºè®®æ£€æŸ¥ä½ç½®ç¼–ç æˆ–ç‰¹å¾æå–èƒ½åŠ›")
    elif not count_ok and center_ok:
        print(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸ: ä½ç½®é¢„æµ‹ OKï¼Œä½†æ•°é‡é¢„æµ‹éœ€è¦æ”¹è¿›")
    else:
        print(f"  âŒ éªŒè¯å¤±è´¥ï¼ŒQ-Former è®¾è®¡å¯èƒ½éœ€è¦æ”¹è¿›")
    print(f"{'='*80}\n")
    
    return {
        'loss': avg_loss,
        'mae': avg_mae,
        'center_mae': avg_center_mae,
        'exist_acc': exist_acc.item(),
    }


def main():
    parser = argparse.ArgumentParser()
    
    parser.add_argument('--dataroot', type=str, required=True)
    parser.add_argument('--version', type=str, default='v1.0-trainval')
    parser.add_argument('--gt-cache', type=str, required=True)
    parser.add_argument('--sample-ratio', type=float, default=0.15)
    
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch-size', type=int, default=2)
    parser.add_argument('--accumulation-steps', type=int, default=4)
    parser.add_argument('--num-workers', type=int, default=4)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--weight-decay', type=float, default=0.01)
    parser.add_argument('--grad-clip', type=float, default=5.0)
    parser.add_argument('--fp16', action='store_true')
    
    parser.add_argument('--output-dir', type=str, required=True)
    
    args = parser.parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("\n" + "="*80)
    print("Q-Former Verification - Linear Probing (å¢å¼ºç‰ˆ)")
    print("="*80)
    print(f"éªŒè¯æ–¹æ³•: åœºæ™¯çº§åˆ«ç›®æ ‡ç»Ÿè®¡é¢„æµ‹")
    print(f"éªŒè¯ç›®æ ‡: {NUM_SCENE_QUERIES} scene queries èƒ½å¦ä»£è¡¨åœºæ™¯çš„è¯­ä¹‰å’Œä½ç½®ä¿¡æ¯")
    print(f"é¢„æµ‹å†…å®¹:")
    print(f"  1. å„ç±»ç›®æ ‡çš„æ•°é‡ [13] - éªŒè¯ã€è¯­ä¹‰ã€‘ä¿¡æ¯")
    print(f"  2. å„ç±»ç›®æ ‡çš„ä¸­å¿ƒä½ç½®å‡å€¼ [13, 2] - éªŒè¯ã€ä½ç½®ã€‘ä¿¡æ¯ (æ–°å¢)")
    print(f"  3. å„ç±»ç›®æ ‡çš„ä½ç½®åˆ†æ•£åº¦ [13, 2] - éªŒè¯ã€ç©ºé—´åˆ†å¸ƒã€‘ä¿¡æ¯ (æ–°å¢)")
    print(f"ç±»åˆ«:")
    print(f"  - 10 ç±» 3D ç›®æ ‡: {', '.join(OBJECT_CATEGORIES)}")
    print(f"  - 3 ç±»åœ°å›¾å…ƒç´ : {', '.join(MAP_CATEGORIES)}")
    print(f"æ£€æµ‹å¤´: åªç”¨çº¿æ€§å±‚ (Linear Probing)")
    print(f"æˆåŠŸæ ‡å‡†:")
    print(f"  - æ•°é‡ MAE < 2.0, å­˜åœ¨æ€§å‡†ç¡®ç‡ > 80%")
    print(f"  - ä¸­å¿ƒ MAE < 0.15 (å½’ä¸€åŒ–ç©ºé—´)")
    print("="*80)
    
    # Model
    model = QFormerVerificationModel()
    model = model.cuda()
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nTotal parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    
    # Dataset
    train_dataset = QFormerVerificationDataset(
        dataroot=args.dataroot,
        version=args.version,
        split='train',
        gt_cache_path=args.gt_cache,
        sample_ratio=args.sample_ratio,
    )
    
    val_dataset = QFormerVerificationDataset(
        dataroot=args.dataroot,
        version=args.version,
        split='val',
        gt_cache_path=args.gt_cache,
        sample_ratio=0.1,
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    # Loss & Optimizer
    criterion = CountingLoss(num_classes=NUM_CLASSES)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    # Scheduler
    from transformers import get_cosine_schedule_with_warmup
    total_steps = len(train_loader) * args.epochs // args.accumulation_steps
    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)
    
    scaler = torch.cuda.amp.GradScaler() if args.fp16 else None
    
    # Training
    best_mae = float('inf')
    best_exist_acc = 0
    best_center_mae = float('inf')
    
    for epoch in range(args.epochs):
        train_metrics = train_epoch(model, train_loader, criterion, optimizer, scaler, epoch, args, scheduler)
        
        # æ‰“å°è®­ç»ƒæŒ‡æ ‡
        print(f"\nğŸ“ˆ Train Epoch {epoch+1}: "
              f"loss={train_metrics['loss']:.4f}, "
              f"count_MAE={train_metrics['mae']:.2f}, "
              f"center_MAE={train_metrics['center_mae']:.4f}")
        
        val_metrics = validate(model, val_loader, criterion, epoch, args)
        
        # Save best (ç»¼åˆ MAE å’Œ center_mae)
        # ä½¿ç”¨ç»¼åˆå¾—åˆ†: mae * 0.7 + center_mae * 30 * 0.3 (å½’ä¸€åŒ–åçš„æƒé‡)
        current_score = val_metrics['mae'] * 0.7 + val_metrics['center_mae'] * 30 * 0.3
        best_score = best_mae * 0.7 + best_center_mae * 30 * 0.3
        
        if current_score < best_score:
            best_mae = val_metrics['mae']
            best_exist_acc = val_metrics['exist_acc']
            best_center_mae = val_metrics['center_mae']
            save_path = os.path.join(args.output_dir, 'best_model.pt')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'metrics': val_metrics,
            }, save_path)
            print(f"âœ… Saved best model (count_MAE={best_mae:.2f}, "
                  f"center_MAE={best_center_mae:.4f}, "
                  f"Exist Acc={best_exist_acc*100:.1f}%)")
    
    print("\n" + "="*80)
    print(f"âœ… Training completed!")
    print(f"   Best Count MAE: {best_mae:.2f} (ç›®æ ‡: < 2.0)")
    print(f"   Best Center MAE: {best_center_mae:.4f} (ç›®æ ‡: < 0.15)")
    print(f"   Best Existence Accuracy: {best_exist_acc*100:.1f}% (ç›®æ ‡: > 80%)")
    print("="*80)
    
    # ç»“è®º
    count_ok = best_mae < 2.0 and best_exist_acc > 0.8
    center_ok = best_center_mae < 0.15
    
    print("\n" + "="*80)
    print("éªŒè¯ç»“è®º:")
    if count_ok and center_ok:
        print("  âœ… Q-Former 768 queries èƒ½å¤Ÿæœ‰æ•ˆæå–åœºæ™¯çš„ã€è¯­ä¹‰ã€‘å’Œã€ä½ç½®ã€‘ä¿¡æ¯ï¼")
        print("  âœ… åœºæ™¯ä¸­æœ‰ä»€ä¹ˆã€æœ‰å¤šå°‘ã€åœ¨å“ªé‡Œ â†’ Q-Former éƒ½çŸ¥é“")
        print("  âœ… å¦‚æœä¸»è®­ç»ƒæ•ˆæœä¸å¥½ï¼Œé—®é¢˜åœ¨ LLM æˆ– MapDecoderï¼Œä¸æ˜¯ Q-Former")
    elif count_ok and not center_ok:
        print("  âš ï¸ Q-Former èƒ½æå–ã€è¯­ä¹‰ã€‘ä¿¡æ¯ï¼ˆæœ‰ä»€ä¹ˆã€æœ‰å¤šå°‘ï¼‰")
        print("  âš ï¸ ä½†ã€ä½ç½®ã€‘ä¿¡æ¯æå–èƒ½åŠ›æœ‰é™")
        print("  â†’ å»ºè®®: æ£€æŸ¥ä½ç½®ç¼–ç æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–å¢åŠ ä½ç½®æ•æ„Ÿçš„æŸå¤±")
    elif not count_ok and center_ok:
        print("  âš ï¸ Q-Former èƒ½æå–ã€ä½ç½®ã€‘ä¿¡æ¯")
        print("  âš ï¸ ä½†ã€è¯­ä¹‰ã€‘ä¿¡æ¯æå–èƒ½åŠ›æœ‰é™")
        print("  â†’ å»ºè®®: æ£€æŸ¥ç‰¹å¾æå–æˆ– scene queries è®¾è®¡")
    else:
        print("  âŒ Q-Former è®¾è®¡å¯èƒ½éœ€è¦æ”¹è¿›")
        print("  âŒ 768 tokens å¯èƒ½ä¸è¶³ä»¥è¡¨ç¤ºå®Œæ•´çš„åœºæ™¯ä¿¡æ¯")
        print("  â†’ å»ºè®®: å¢åŠ  scene queries æ•°é‡æˆ–è°ƒæ•´ decoder å±‚æ•°")
    print("="*80)


if __name__ == '__main__':
    main()
