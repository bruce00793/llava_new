"""
Complete End-to-End LLaVA Map Detection Model

Flow:
    Images (6 views) ‚Üí Q-Former ‚Üí Scene Tokens (768)
                                        ‚Üì
    Text Prompt ‚Üí Embed ‚Üí Text Embeds
                                        ‚Üì
    Learnable Queries (1050) ‚Üí [Text + Scene + Queries]
                                        ‚Üì
                                    LLM Forward
                                        ‚Üì
                Extract Instance/Point Features + Scene Tokens
                  - instance_features: (B, 50, 4096)
                  - point_features: (B, 50, 20, 4096)
                  - scene_tokens: (B, 768, 4096)
                                        ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Map-Scene Interaction Layer (Êñ∞Â¢ûÔºÅ)     ‚îÇ
                    ‚îÇ                                           ‚îÇ
                    ‚îÇ   Map Features ‚Üê‚îÄCross-Attention‚îÄ‚Üí Scene  ‚îÇ
                    ‚îÇ   (ËÆ© Map Queries Áõ¥Êé•‰ªéÂõæÂÉèÊèêÂèñ‰ø°ÊÅØ)        ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚Üì
                    Map Decoder (Instance-Conditioned Point Prediction)
                      - inst_reduced + pt_reduced ‚Üí concat ‚Üí PointHead
                                        ‚Üì
                            Predictions (logits, points, bbox)

Author: Auto-generated for Map Detection
Date: 2025-01
"""

import torch
import torch.nn as nn
from typing import Dict, Optional, Tuple
from transformers import AutoTokenizer

from .qformer import QFormer, build_qformer
from .qformer_v2 import QFormerV2, build_qformer_v2
from .language_model.llava_map import LlavaMapDetectionModel
from .map_decoder import MapDecoder
from .map_config import MapDetectionConfig, DEFAULT_MAP_CONFIG
from .map_scene_interaction import MapSceneInteractionLayer, build_map_scene_interaction

# LoRA support
try:
    from peft import get_peft_model, LoraConfig, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("Warning: peft not installed. LoRA fine-tuning will not be available.")
    print("Install with: pip install peft")


class LLaVAMapDetector(nn.Module):
    """
    Complete end-to-end model for map detection using LLaVA architecture.
    
    Components:
    1. Q-Former: 6 camera images ‚Üí 768 scene tokens
    2. LLM: text + scene + 1050 queries ‚Üí hidden states
    3. Decoder: hidden states ‚Üí predictions
    """
    
    def __init__(
        self,
        qformer_config: dict,
        llm_path: str = "lmsys/vicuna-7b-v1.5",
        map_config: MapDetectionConfig = None,
        freeze_llm: bool = True,
        qformer_pretrained_path: Optional[str] = None,
        use_lora: bool = True,           # ÈªòËÆ§ÂêØÁî® LoRA ÂæÆË∞É
        lora_r: int = 32,                 # Â¢ûÂä† rank ‰ª•Êèê‰æõË∂≥Â§üÂ≠¶‰π†ËÉΩÂäõ
        lora_alpha: int = 64,             # ‰øùÊåÅ alpha/r = 2
        lora_dropout: float = 0.1,
        lora_target_modules: Optional[list] = None,
    ):
        """
        Args:
            qformer_config: Config dict for Q-Former
            llm_path: Path to pretrained LLM
            map_config: Map detection config
            freeze_llm: Whether to freeze LLM parameters (ignored if use_lora=True)
            qformer_pretrained_path: Path to BLIP-2 pretrained Q-Former weights (optional)
            use_lora: Whether to use LoRA fine-tuning for LLM (default: True)
            lora_r: LoRA rank (default: 32, Â¢ûÂä†‰ª•ÈÄÇÂ∫îÁ©∫Èó¥ÁêÜËß£‰ªªÂä°)
            lora_alpha: LoRA alpha scaling factor (default: 64, ‰øùÊåÅ alpha/r=2)
            lora_dropout: LoRA dropout (default: 0.1)
            lora_target_modules: Which modules to apply LoRA 
                (default: ["q_proj", "k_proj", "v_proj", "o_proj"] - Âè™ÂæÆË∞É Attention Â±Ç)
        """
        self.use_lora = use_lora
        super().__init__()
        
        self.config = map_config or DEFAULT_MAP_CONFIG
        
        # 1. Q-Former for multi-view encoding
        print(f"\n{'='*60}")
        qformer_ver = qformer_config.get('_version', 'v1')
        print(f"Initializing Q-Former ({qformer_ver})...")
        print(f"{'='*60}")
        if qformer_ver == 'v2':
            self.qformer = build_qformer_v2(qformer_config)
        else:
            self.qformer = build_qformer(qformer_config)
        # Move Q-Former to GPU
        # Note: Keep FP32 for numerical stability, will be cast to FP16 via autocast if needed
        self.qformer = self.qformer.cuda()
        
        # Load pretrained Q-Former weights if provided
        if qformer_pretrained_path is not None:
            self._load_qformer_pretrained(qformer_pretrained_path)
        else:
            print(f"‚ö†Ô∏è  Q-Former initialized from scratch (random weights)")
            print(f"   Tip: Use qformer_pretrained_path='blip2' for better performance")
        
        # 2. LLM with map queries
        print(f"\n{'='*60}")
        print(f"Loading LLM: {llm_path}")
        print(f"{'='*60}")
        
        # Ê£ÄÊµãÊòØÂê¶Âú®ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁéØÂ¢É‰∏≠
        # Â¶ÇÊûúÊòØÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÔºå‰∏ç‰ΩøÁî® device_map="auto"Ôºà‰ºöÂàÜÂ∏ÉÂà∞Â§öGPUÂØºËá¥ÈóÆÈ¢òÔºâ
        # ËÄåÊòØÂÖàÂä†ËΩΩÂà∞ CPUÔºåÂêéÁª≠Áî± .cuda() Âíå DDP Â§ÑÁêÜ
        import os
        is_distributed = 'WORLD_SIZE' in os.environ and int(os.environ.get('WORLD_SIZE', 1)) > 1
        is_single_gpu = torch.cuda.device_count() == 1
        
        # „ÄêÂÖ≥ÈîÆ„Äë‰ΩøÁî® BF16 ËÄåÈùû FP16 Âä†ËΩΩ LLM
        # BF16 ÊåáÊï∞ËåÉÂõ¥‰∏é FP32 Áõ∏ÂêåÔºàmax ~3.4e38ÔºâÔºåÂèçÂêë‰º†Êí≠Ê¢ØÂ∫¶‰∏ç‰ºöÊ∫¢Âá∫
        # FP16 max ‰ªÖ 65504Ôºå7B Ê®°Âûã 32 Â±ÇÂèçÂêë‰º†Êí≠Ê¢ØÂ∫¶ÂøÖÁÑ∂Ê∫¢Âá∫
        # RTX 4090 (Ada Lovelace, compute capability 8.9) ÂÆåÂÖ®ÊîØÊåÅ BF16
        llm_dtype = torch.bfloat16
        self.llm_dtype = llm_dtype
        print(f"  LLM dtype: {llm_dtype} (BF16 prevents gradient overflow in backward pass)")
        
        if is_distributed or is_single_gpu:
            print(f"  Mode: {'Distributed' if is_distributed else 'Single GPU'} - loading to CPU first")
            self.llm = LlavaMapDetectionModel.from_pretrained(
                llm_path,
                torch_dtype=llm_dtype,
                device_map=None,
                low_cpu_mem_usage=True,
            )
        else:
            print(f"  Mode: Multi-GPU Auto - using device_map='auto'")
            self.llm = LlavaMapDetectionModel.from_pretrained(
                llm_path,
                torch_dtype=llm_dtype,
                device_map="auto",
            )
        
        # Fix: Convert map_queries to FP32 for stable training
        # (FP16 parameters can overflow during optimizer updates)
        print(f"Converting Map Queries to FP32 for training stability...")
        self.llm.map_queries = self.llm.map_queries.float()
        
        # Re-initialize with proper values in FP32
        with torch.no_grad():
            device = self.llm.map_queries.instance_content.device
            
            # Re-init instance content
            self.llm.map_queries.instance_content.data = torch.randn(
                self.llm.map_queries.instance_content.shape,
                device=device, dtype=torch.float32
            ) * 0.02
            
            # Re-init point content
            self.llm.map_queries.point_content.data = torch.randn(
                self.llm.map_queries.point_content.shape,
                device=device, dtype=torch.float32
            ) * 0.02
        
        # Verify
        print(f"‚úÖ Map Queries: dtype={self.llm.map_queries.instance_content.dtype}, "
              f"no_nan={not torch.isnan(self.llm.map_queries.instance_content).any()}")
        
        print(f"‚úÖ LLM loaded successfully!")
        
        # 3. Tokenizer for text (from local path)
        print(f"\nLoading tokenizer from local path...")
        self.tokenizer = AutoTokenizer.from_pretrained(llm_path, use_fast=False, local_files_only=True)
        print(f"‚úÖ Tokenizer loaded from local: {llm_path}")
        
        # 4. Map-Scene Interaction Layer (Êñ∞Â¢ûÔºÅ)
        # Âú® LLM ËæìÂá∫Âêé„ÄÅDecoder ‰πãÂâçÔºåËÆ© Map Features Áõ¥Êé•Âíå Scene Tokens ‰∫§‰∫í
        print(f"\n{'='*60}")
        print(f"Initializing Map-Scene Interaction Layer...")
        print(f"{'='*60}")
        self.map_scene_interaction = build_map_scene_interaction(
            input_dim=4096,      # LLM hidden size
            embed_dim=256,       # ‰∫§‰∫íÂ±ÇÁª¥Â∫¶
            num_heads=8,         # Ê≥®ÊÑèÂäõÂ§¥Êï∞
            num_layers=6,        # 6 Â±Ç‰∫§‰∫íÔºà‰∏é MapTR Decoder ÂØπÈΩêÔºâ
            ffn_dim=1024,        # FFN Áª¥Â∫¶
            dropout=0.1,
        )
        self.map_scene_interaction = self.map_scene_interaction.cuda()
        print(f"‚úÖ Map-Scene Interaction Layer initialized (6 layers)")
        
        # 5. Decoder for predictions
        print(f"\n{'='*60}")
        print(f"Initializing Map Decoder...")
        print(f"{'='*60}")
        self.decoder = MapDecoder(self.config)
        # Move Decoder to GPU
        self.decoder = self.decoder.cuda()
        print(f"‚úÖ Map Decoder initialized (random weights)")
        
        # 6. Loss function (Âú® __init__ ‰∏≠ÂàõÂª∫ÔºåËÄå‰∏çÊòØ forward ‰∏≠Âä®ÊÄÅÂàõÂª∫)
        # dir_loss ÊåâÂÆû‰æãÊï∞ÂΩí‰∏ÄÂåñÔºåÈáèÁ∫ßÁ∫¶ 9.5
        # weight_dir=0.25 ÊòØÊäò‰∏≠ÊñπÊ°àÔºöÊñπÂêëÊçüÂ§±Ë¥°ÁåÆÁ∫¶ 5-8%ÔºåÊúâÊÑè‰πâ‰ΩÜ‰∏ç‰∏ªÂØºËÆ≠ÁªÉ
        from .map_loss import MapDetectionLoss, HungarianMatcher
        self.criterion = MapDetectionLoss(
            num_classes=3,
            weight_cls=2.0,
            weight_pts=5.0,
            weight_dir=0.25,  # Êäò‰∏≠ÊñπÊ°àÔºàMapTRÁî®0.005Âá†‰πéÊó†‰ΩúÁî®Ôºå2.0‰ºö‰∏ªÂØºËÆ≠ÁªÉÔºâ
        )
        self._aux_matcher = HungarianMatcher(cost_class=2.0, cost_points=5.0)
        print(f"‚úÖ Loss function initialized")
        
        # 7. LoRA or Freeze LLM
        if use_lora:
            if not PEFT_AVAILABLE:
                raise ImportError("peft is required for LoRA. Install with: pip install peft")
            
            print(f"\n{'='*60}")
            print(f"Applying LoRA to LLM...")
            print(f"{'='*60}")
            
            # Default target modules for LLaMA-based models
            # ÈíàÂØπÂú∞ÂõæÊ£ÄÊµã‰ªªÂä°‰ºòÂåñÁöÑ LoRA ÈÖçÁΩÆÔºö
            # - q_proj: Map Queries Â¶Ç‰ΩïÊü•ËØ¢ Scene TokensÔºàÊ†∏ÂøÉÔºâ
            # - k_proj: Scene Tokens Â¶Ç‰ΩïË¢´Á¥¢ÂºïÔºàÈáçË¶ÅÔºâ
            # - v_proj: Scene Tokens Êèê‰æõ‰ªÄ‰πà‰ø°ÊÅØÔºàÊ†∏ÂøÉÔºâ
            # - o_proj: Attention ËæìÂá∫ÊäïÂΩ±ÔºàÈáçË¶ÅÔºâ
            # Ê≥®Ôºö‰∏çÂåÖÂê´ MLP Â±ÇÔºåÂõ†‰∏∫Ê£ÄÊµã‰ªªÂä°‰∏ªË¶Å‰æùËµñ Attention Êú∫Âà∂
            if lora_target_modules is None:
                lora_target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
            
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=lora_r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                target_modules=lora_target_modules,
                bias="none",
            )
            
            # Apply LoRA to LLM
            self.llm = get_peft_model(self.llm, lora_config)
            
            # Make sure map_queries are trainable
            for param in self.llm.base_model.model.map_queries.parameters():
                param.requires_grad = True
            
            # „ÄêÂÖ≥ÈîÆ‰øÆÂ§ç„ÄëÂ∞Ü LoRA ÂèÇÊï∞ËΩ¨Êç¢‰∏∫ FP32
            # ÂéüÂõ†ÔºöLLM ‰ª• FP16 Âä†ËΩΩÔºåLoRA ÁªßÊâø FP16„ÄÇ
            # Âú® FP16 ‰∏ãÔºåGradScaler Áº©ÊîæÂêéÁöÑÊ¢ØÂ∫¶ÊûÅÊòìÊ∫¢Âá∫ÔºàFP16 max=65504Ôºâ
            # ÂØºËá¥ÊØè‰∏™Ê¢ØÂ∫¶Ê≠•ÈÉΩÂá∫Áé∞ NaN/Inf„ÄÇ
            # ËΩ¨‰∏∫ FP32 ÂêéÔºåÊ¢ØÂ∫¶ËåÉÂõ¥Êâ©Â§ßÂà∞ 3.4e38ÔºåÂΩªÂ∫ïËß£ÂÜ≥Ê∫¢Âá∫ÈóÆÈ¢ò„ÄÇ
            lora_param_count = 0
            for name, param in self.llm.named_parameters():
                if 'lora_' in name and param.requires_grad:
                    param.data = param.data.float()
                    lora_param_count += 1
            print(f"‚úÖ Converted {lora_param_count} LoRA parameters to FP32")
            
            print(f"‚úÖ LoRA applied to LLM!")
            print(f"   - LoRA rank (r): {lora_r}")
            print(f"   - LoRA alpha: {lora_alpha}")
            print(f"   - LoRA dropout: {lora_dropout}")
            print(f"   - Target modules: {lora_target_modules}")
            self.llm.print_trainable_parameters()
            
        elif freeze_llm:
            print(f"\n{'='*60}")
            print(f"Freezing LLM backbone parameters...")
            print(f"{'='*60}")
            for param in self.llm.model.parameters():
                param.requires_grad = False
            # Only train map_queries
            for param in self.llm.map_queries.parameters():
                param.requires_grad = True
            print(f"‚úÖ LLM frozen, only training:")
            print(f"   - Q-Former")
            print(f"   - Map Queries (1050 learnable queries)")
            print(f"   - Map-Scene Interaction Layer")
            print(f"   - Map Decoder")
        else:
            print(f"\n{'='*60}")
            print(f"Full LLM fine-tuning enabled (not recommended)")
            print(f"{'='*60}")
        
        print(f"\n{'='*60}")
        print(f"‚úÖ LLaVAMapDetector initialized successfully!")
        print(f"{'='*60}")
        self._print_trainable_params()
    
    def _load_qformer_pretrained(self, pretrained_path: str):
        """
        Load pretrained Q-Former weights from BLIP-2 or custom checkpoint.
        
        Args:
            pretrained_path: 
                - 'blip2': Load from Salesforce BLIP-2
                - Local path: Load from local checkpoint
        """
        import os
        
        if pretrained_path == 'blip2':
            print(f"üì• Loading BLIP-2 pretrained Q-Former...")
            
            # Use local BLIP-2 path if available
            local_blip2_path = "/home/cly/auto/llava_test/LLaVA/blip2-opt-2.7b"
            
            try:
                from transformers import Blip2Model
                
                # Check if local BLIP-2 exists
                if os.path.exists(local_blip2_path):
                    print(f"   Loading from local: {local_blip2_path}")
                    blip2 = Blip2Model.from_pretrained(
                        local_blip2_path,
                        torch_dtype=torch.float32,
                        local_files_only=True,
                    )
                else:
                    print(f"   ‚ö†Ô∏è Local BLIP-2 not found, trying remote: Salesforce/blip2-opt-2.7b")
                    blip2 = Blip2Model.from_pretrained(
                        "Salesforce/blip2-opt-2.7b",
                        torch_dtype=torch.float32,
                    )
                
                # Extract Q-Former components
                qformer_state = {}
                for name, param in blip2.named_parameters():
                    if 'qformer' in name or 'query_tokens' in name:
                        # Remove prefix
                        new_name = name.replace('qformer.', '')
                        new_name = new_name.replace('language_model.', '')
                        qformer_state[new_name] = param.data
                
                # Load into our Q-Former (partial loading, ignore size mismatch)
                missing, unexpected = self.qformer.load_state_dict(qformer_state, strict=False)
                
                print(f"‚úÖ BLIP-2 Q-Former loaded!")
                print(f"   Loaded parameters: {len(qformer_state)}")
                if len(missing) > 0:
                    print(f"   Missing keys (will use random init): {len(missing)}")
                if len(unexpected) > 0:
                    print(f"   Unexpected keys (ignored): {len(unexpected)}")
                
                del blip2  # Free memory
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to load BLIP-2 weights: {e}")
                print(f"   Falling back to random initialization")
        
        elif os.path.exists(pretrained_path):
            print(f"üì• Loading Q-Former from local checkpoint...")
            print(f"   Path: {pretrained_path}")
            
            try:
                # Check if it's a directory (HuggingFace model format)
                if os.path.isdir(pretrained_path):
                    print(f"   Detected HuggingFace model directory, using from_pretrained...")
                    from transformers import Blip2Model
                    
                    # Load BLIP-2 model from local directory
                    blip2 = Blip2Model.from_pretrained(
                        pretrained_path,
                        torch_dtype=torch.float32,
                    )
                    
                    # Extract Q-Former components
                    qformer_state = {}
                    for name, param in blip2.named_parameters():
                        if 'qformer' in name or 'query_tokens' in name:
                            new_name = name.replace('qformer.', '')
                            new_name = new_name.replace('language_model.', '')
                            qformer_state[new_name] = param.data
                    
                    # Load into our Q-Former (partial loading)
                    missing, unexpected = self.qformer.load_state_dict(qformer_state, strict=False)
                    
                    print(f"‚úÖ BLIP-2 Q-Former loaded from local directory!")
                    print(f"   Loaded parameters: {len(qformer_state)}")
                    if len(missing) > 0:
                        print(f"   Missing keys (will use random init): {len(missing)}")
                    if len(unexpected) > 0:
                        print(f"   Unexpected keys (ignored): {len(unexpected)}")
                    
                    del blip2  # Free memory
                else:
                    # It's a single file checkpoint
                    state_dict = torch.load(pretrained_path, map_location='cpu')
                    
                    # Handle different checkpoint formats
                    if 'qformer' in state_dict:
                        state_dict = state_dict['qformer']
                    elif 'model' in state_dict:
                        state_dict = state_dict['model']
                    
                    missing, unexpected = self.qformer.load_state_dict(state_dict, strict=False)
                    
                    print(f"‚úÖ Q-Former checkpoint loaded!")
                    if len(missing) > 0:
                        print(f"   Missing keys: {len(missing)}")
                    if len(unexpected) > 0:
                        print(f"   Unexpected keys: {len(unexpected)}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to load checkpoint: {e}")
                import traceback
                traceback.print_exc()
                print(f"   Falling back to random initialization")
        
        else:
            print(f"‚ö†Ô∏è  Pretrained path not found: {pretrained_path}")
            print(f"   Using random initialization")
    
    def _print_trainable_params(self):
        """Print trainable parameter statistics."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"\nParameter Statistics:")
        print(f"  Total: {total_params:,}")
        print(f"  Trainable: {trainable_params:,}")
        print(f"  Frozen: {total_params - trainable_params:,}")
        print(f"  Trainable %: {100 * trainable_params / total_params:.2f}%")
    
    def forward(
        self,
        images: torch.Tensor,
        text_ids: torch.Tensor,
        return_loss: bool = False,
        gt_labels: Optional[torch.Tensor] = None,
        gt_points: Optional[torch.Tensor] = None,
        gt_masks: Optional[torch.Tensor] = None,
        cam_intrinsics: Optional[torch.Tensor] = None,
        cam_extrinsics: Optional[torch.Tensor] = None,
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass of complete model.
        
        Args:
            images: (B, 6, 3, 448, 800) - 6 camera views (H=448, W=800)
            text_ids: (B, L) - Tokenized text with IMAGE_TOKEN_INDEX=-200
            return_loss: Whether to compute loss (requires GT)
            cam_intrinsics: (B, 6, 3, 3) - Camera intrinsic matrices (optional, for 3D pos encoding)
            cam_extrinsics: (B, 6, 4, 4) - Camera extrinsic matrices (optional, for 3D pos encoding)
            gt_labels: (B, M) - Ground truth class labels
            gt_points: (B, M, 20, 2) - Ground truth points
            gt_masks: (B, M) - Valid GT mask
        
        Returns:
            dict with keys:
                - pred_logits: (B, 50, 3) classification logits
                - pred_points: (B, 50, 20, 2) point coordinates
                - instance_features: (B, 50, 4096) (optional)
                - point_features: (B, 50, 20, 4096) (optional)
                - loss_dict: dict of losses (if return_loss=True)
        """
        batch_size = images.shape[0]
        
        # ===== Step 1: Q-Former - Images to Scene Tokens =====
        # Pass camera parameters for 3D position encoding if available
        scene_tokens = self.qformer(
            images, 
            cam_intrinsics=cam_intrinsics,
            cam_extrinsics=cam_extrinsics
        )  # (B, 768, 4096)
        
        # „ÄêÂÆâÂÖ®Ê£ÄÊü•„ÄëÁ°ÆËÆ§ Q-Former ËæìÂá∫Ê≠£Â∏∏Ôºà‰øÆÂ§ç autocast Âêé‰∏çÂ∫îÂÜçÂá∫Áé∞ NaNÔºâ
        if torch.isnan(scene_tokens).any() or torch.isinf(scene_tokens).any():
            print(f"‚ùå [Forward] Q-Former output still contains NaN/Inf after autocast fix! "
                  f"This indicates a deeper issue.", flush=True)
        
        # Convert to match LLM precision (BF16)
        scene_tokens = scene_tokens.to(self.llm_dtype)
        
        # ===== Step 2: Embed Text and Replace IMAGE_TOKEN =====
        # Handle IMAGE_TOKEN_INDEX (-200) which is a placeholder for scene tokens
        from llava.constants import IMAGE_TOKEN_INDEX
        
        # Replace IMAGE_TOKEN_INDEX with a valid token ID (0 = pad token) temporarily
        text_ids_safe = text_ids.clone()
        image_token_mask = (text_ids == IMAGE_TOKEN_INDEX)
        text_ids_safe[image_token_mask] = 0  # Use pad token temporarily
        
        # Get text embeddings
        # Note: ÈúÄË¶ÅÂ§ÑÁêÜ LoRA ÂåÖË£ÖÂêéÁöÑËÆøÈóÆË∑ØÂæÑ
        if self.use_lora:
            # LoRA ÂåÖË£ÖÂêéË∑ØÂæÑ: base_model.model.model.embed_tokens
            embed_tokens = self.llm.base_model.model.model.embed_tokens
        else:
            # ÂéüÂßãË∑ØÂæÑ: model.embed_tokens
            embed_tokens = self.llm.model.embed_tokens
        text_embeds_temp = embed_tokens(text_ids_safe)  # (B, L, 4096)
        
        # Replace the embeddings at IMAGE_TOKEN positions with scene_tokens
        # Create new list to hold embeddings with varying lengths
        text_embeds_list = []
        expected_length = None
        
        for b in range(batch_size):
            image_positions = torch.where(image_token_mask[b])[0]
            if len(image_positions) > 0:
                # Replace IMAGE_TOKEN embedding with scene_tokens
                pos = image_positions[0].item()
                # Concatenate: text[:pos] + scene_tokens + text[pos+1:]
                new_embeds = torch.cat([
                    text_embeds_temp[b, :pos],
                    scene_tokens[b],  # Insert 768 scene tokens
                    text_embeds_temp[b, pos+1:]
                ], dim=0)
            else:
                # Ê≤°Êúâ IMAGE_TOKENÔºåÁõ¥Êé•‰ΩøÁî®ÂéüÂßã embeddings
                # ËøôÁßçÊÉÖÂÜµ‰∏çÂ∫îËØ•ÂèëÁîüÔºåÊ∑ªÂä†Ë≠¶Âëä
                import warnings
                warnings.warn(f"Batch {b} has no IMAGE_TOKEN! This may cause issues.")
                new_embeds = text_embeds_temp[b]
            
            # Ê£ÄÊü•ÈïøÂ∫¶‰∏ÄËá¥ÊÄß
            if expected_length is None:
                expected_length = new_embeds.shape[0]
            else:
                if new_embeds.shape[0] != expected_length:
                    raise ValueError(
                        f"Batch {b} has different embedding length {new_embeds.shape[0]} "
                        f"vs expected {expected_length}. "
                        f"Ensure all samples have IMAGE_TOKEN at the same position."
                    )
            
            text_embeds_list.append(new_embeds)
        
        # Stack back into tensor (all should have same length now)
        # Ê≥®Ôºö768 scene tokens ÊõøÊç¢ 1 ‰∏™ IMAGE_TOKEN = ÂáÄÂ¢û 767 ‰∏™ tokens
        text_embeds = torch.stack(text_embeds_list, dim=0)  # (B, L+767, 4096)
        
        # ===== Step 3: LLM Forward with Map Queries =====
        # This will:
        # - Add 1050 learnable queries
        # - Concatenate [text_with_scene, queries]
        # - Forward through LLM
        # - Extract instance and point features from query positions
        # Note: text_embeds now includes scene_tokens, so we pass it directly
        # and set scene_tokens=None to avoid double-adding
        
        # Â§ÑÁêÜ LoRA Ê®°ÂºèÔºöPEFT ÂåÖË£ÖÂêéÈúÄË¶ÅÈÄöËøá base_model ËÆøÈóÆËá™ÂÆö‰πâÊñπÊ≥ï
        if self.use_lora:
            # LoRA Ê®°ÂºèÔºöÈÄöËøá base_model Ë∞ÉÁî® forward_with_map
            llm_output = self.llm.base_model.forward_with_map(
                text_embeds=text_embeds,  # Already includes scene tokens
                scene_tokens=None,  # Don't add scene tokens again
                return_map_features=True,
            )
        else:
            # Èùû LoRA Ê®°ÂºèÔºöÁõ¥Êé•Ë∞ÉÁî®
            llm_output = self.llm.forward_with_map(
                text_embeds=text_embeds,  # Already includes scene tokens
                scene_tokens=None,  # Don't add scene tokens again
                return_map_features=True,
            )
        
        # „Äê‰ºòÂåñ„Äë‰ΩøÁî® query_outputs ‰øùÊåÅ LLM ÂéüÂßãËæìÂá∫È°∫Â∫è
        # ÂéüÂßãÈ°∫Â∫è: [Inst0, P0_1..P0_20, Inst1, P1_1..P1_20, ..., Inst49, P49_1..P49_20]
        query_outputs = llm_output['query_outputs']  # (B, 1050, 4096) - ‰øùÊåÅÂéüÂßãÈ°∫Â∫è
        
        # „ÄêÂÆâÂÖ®Ê£ÄÊü•„ÄëÁ°ÆËÆ§ LLM ËæìÂá∫Ê≠£Â∏∏
        if torch.isnan(query_outputs).any() or torch.isinf(query_outputs).any():
            print(f"‚ùå [Forward] LLM query_outputs contains NaN/Inf!", flush=True)
        
        # Ëé∑ÂèñÁª¥Â∫¶‰ø°ÊÅØ
        B = query_outputs.shape[0]
        N_inst = 50   # ÂÆû‰æãÊï∞Èáè
        N_pts = 20    # ÊØè‰∏™ÂÆû‰æãÁöÑÁÇπÊï∞Èáè
        queries_per_inst = 1 + N_pts  # 21 (1 instance query + 20 point queries)
        H = query_outputs.shape[2]  # hidden_size (4096)
        
        # ===== Step 4: Map-Scene Interaction (Êñ∞Â¢ûÔºÅ) =====
        # ËÆ© Map Features Áõ¥Êé•Âíå Scene Tokens ÂÅö Cross-Attention
        # 
        # „ÄêÈáçË¶ÅËÆæËÆ°ÂÜ≥Á≠ñ„Äë
        # 1. ‰ΩøÁî®**ÂéüÂßã scene tokens**ÔºàQ-Former ËæìÂá∫ÔºâÔºåËÄåÈùû LLM Â§ÑÁêÜÂêéÁöÑ
        # 2. „Äê‰ºòÂåñ„Äë‰øùÊåÅ LLM ËæìÂá∫ÁöÑÂéüÂßãÈ°∫Â∫èÈÄÅÂÖ• Map-Scene Interaction
        #    - ÂéüÂßãÈ°∫Â∫è: [Inst0, P0_1..P0_20, Inst1, P1_1..P1_20, ...]
        #    - Â•ΩÂ§Ñ: Âêå‰∏ÄÂÆû‰æãÁöÑ instance Âíå points Âú®Â∫èÂàó‰∏≠Áõ∏ÈÇªÔºå
        #           Self-Attention Êó∂Êõ¥ÂÆπÊòìÂª∫Á´ãÂ±ÄÈÉ®ÂÖ≥ËÅî
        #
        # Map-Scene Interaction: Cross-Attention
        # „ÄêÂÖ≥ÈîÆ„Äë‰ΩøÁî®ÂéüÂßã scene_tokensÔºàQ-Former Áõ¥Êé•ËæìÂá∫ÔºâÔºå‰∏ç‰ΩøÁî® LLM Â§ÑÁêÜÂêéÁöÑ
        scene_tokens_for_interaction = scene_tokens  # ‰ΩøÁî® Q-Former ÂéüÂßãËæìÂá∫
        
        # Á°Æ‰øù dtype ‰∏ÄËá¥ÔºàËΩ¨‰∏∫ FP32 ‰ª•‰øùËØÅÊï∞ÂÄºÁ®≥ÂÆöÊÄßÔºâ
        map_features_combined = query_outputs.to(dtype=torch.float32)
        scene_tokens_for_interaction = scene_tokens_for_interaction.to(dtype=torch.float32)
        
        # Cross-Attention: Map Features ‰ªé Scene Tokens ÊèêÂèñËßÜËßâ‰ø°ÊÅØ
        enhanced_map_features = self.map_scene_interaction(
            map_features=map_features_combined,
            scene_tokens=scene_tokens_for_interaction,
        )  # (B, 1050, 4096)
        
        # „ÄêÂÆâÂÖ®Ê£ÄÊü•„ÄëÁ°ÆËÆ§ Map-Scene Interaction ËæìÂá∫Ê≠£Â∏∏
        if torch.isnan(enhanced_map_features).any() or torch.isinf(enhanced_map_features).any():
            print(f"‚ùå [Forward] Map-Scene Interaction output contains NaN/Inf!", flush=True)
        
        # „Äê‰ºòÂåñ„Äë‰ªéÂ¢ûÂº∫ÂêéÁöÑÁâπÂæÅ‰∏≠ÊåâÂéüÂßãÈ°∫Â∫èÈáçÊñ∞ÊèêÂèñ instance Âíå point features
        # ÂéüÂßãÈ°∫Â∫è: [Inst0, P0_1..P0_20, Inst1, P1_1..P1_20, ...]
        instance_features_list = []
        point_features_list = []
        
        for i in range(N_inst):
            start_idx = i * queries_per_inst
            # Instance query ‰Ωç‰∫éÊØèÁªÑÁöÑÁ¨¨‰∏Ä‰∏™‰ΩçÁΩÆ
            inst_feat = enhanced_map_features[:, start_idx:start_idx+1, :]  # (B, 1, H)
            instance_features_list.append(inst_feat)
            # Point queries ‰Ωç‰∫é instance ‰πãÂêéÁöÑ 20 ‰∏™‰ΩçÁΩÆ
            point_feat = enhanced_map_features[:, start_idx+1:start_idx+queries_per_inst, :]  # (B, 20, H)
            point_features_list.append(point_feat)
        
        # ÊãºÊé•ÊàêÊúÄÁªàÂΩ¢Áä∂
        instance_features = torch.cat(instance_features_list, dim=1)  # (B, 50, H)
        point_features = torch.stack(point_features_list, dim=1)      # (B, 50, 20, H)
        
        # ===== Step 5: Decode to Predictions =====
        # Move features to decoder's device and dtype
        decoder_device = next(self.decoder.parameters()).device
        decoder_dtype = next(self.decoder.parameters()).dtype
        instance_features = instance_features.to(device=decoder_device, dtype=decoder_dtype)
        point_features = point_features.to(device=decoder_device, dtype=decoder_dtype)
        
        # Ê≥®Ôºö‰∏çÂÜç clamp Decoder ËæìÂÖ•ÔºåÈÅøÂÖçÈòªÊñ≠Ê¢ØÂ∫¶ÊµÅ
        
        # Instance-Conditioned Point Prediction
        # Uses both instance_features and point_features
        decoder_output = self.decoder(instance_features, point_features)
        
        pred_logits = decoder_output['class_logits']  # (B, 50, 3)
        pred_points = decoder_output['points']        # (B, 50, 20, 2)
        pred_bbox = decoder_output['bbox']            # (B, 50, 4)
        
        # „ÄêÂÆâÂÖ®Ê£ÄÊü•„ÄëÁ°ÆËÆ§ Decoder ËæìÂá∫Ê≠£Â∏∏
        if torch.isnan(pred_logits).any() or torch.isinf(pred_logits).any():
            print(f"‚ùå [Forward] Decoder pred_logits contains NaN/Inf!", flush=True)
        if torch.isnan(pred_points).any() or torch.isinf(pred_points).any():
            print(f"‚ùå [Forward] Decoder pred_points contains NaN/Inf!", flush=True)
        
        # Build output dict
        output = {
            'pred_logits': pred_logits,
            'pred_points': pred_points,
            'pred_bbox': pred_bbox,
            'instance_features': instance_features,
            'point_features': point_features,
        }
        
        # ===== Step 6: Compute Loss (if requested) =====
        if return_loss:
            if gt_labels is None or gt_points is None or gt_masks is None:
                raise ValueError("GT data required for loss computation")
            
            # „ÄêÂÖ≥ÈîÆ‰øÆÂ§ç„ÄëÊçüÂ§±ËÆ°ÁÆóÂøÖÈ°ªÂú® FP32 ‰∏ãÊâßË°åÔºÅ
            # ÂéüÂõ†Ôºöautocast ‰ºöÊääÊüê‰∫õÊìç‰ΩúÈôç‰∏∫ FP16ÔºàÂ¶Ç F.binary_cross_entropy_with_logitsÔºâÔºå
            # ÂØºËá¥Â§ß loss ÂÄºÂú®ÂèçÂêë‰º†Êí≠Êó∂‰∫ßÁîü FP16 Ê¢ØÂ∫¶Ê∫¢Âá∫„ÄÇ
            # ‰ΩøÁî® autocast(enabled=False) Á°Æ‰øùÊâÄÊúâÊçüÂ§±ËÆ°ÁÆóÈÉΩÂú® FP32 ‰∏ã„ÄÇ
            with torch.cuda.amp.autocast(enabled=False):
                # Á°Æ‰øùÊâÄÊúâËæìÂÖ•ÈÉΩÊòØ FP32
                pred_logits_f32 = pred_logits.float()
                pred_points_f32 = pred_points.float()
                
                # Prepare GT lists (Loss expects lists)
                gt_labels_list = []
                gt_points_list = []
                gt_masks_list = []
                
                for b in range(batch_size):
                    mask = gt_masks[b]  # (M,)
                    num_valid = int(mask.sum().item())  # Á°Æ‰øùÊòØÊï¥Êï∞ÔºåÈÅøÂÖçÊµÆÁÇπÂàáÁâáÈîôËØØ
                    
                    if num_valid > 0:
                        gt_labels_list.append(gt_labels[b, :num_valid])
                        gt_points_list.append(gt_points[b, :num_valid].float())
                        gt_masks_list.append(torch.ones(num_valid, 20, dtype=torch.bool, device=mask.device))
                    else:
                        # Empty GT
                        gt_labels_list.append(torch.empty(0, dtype=torch.long, device=mask.device))
                        gt_points_list.append(torch.empty(0, 20, 2, dtype=torch.float32, device=mask.device))
                        gt_masks_list.append(torch.empty(0, 20, dtype=torch.bool, device=mask.device))
                
                # Compute main loss (final points)
                total_loss, loss_dict = self.criterion(
                    pred_logits=pred_logits_f32,
                    pred_lines=pred_points_f32,
                    gt_labels=gt_labels_list,
                    gt_lines=gt_points_list,
                    gt_masks=gt_masks_list,
                )
                
                # ========== ËæÖÂä©ÊçüÂ§±ÔºöÁõëÁù£‰∏≠Èó¥Â±ÇÔºàP0‰øÆÂ§çÁâàÔºâ==========
                # 
                # „ÄêP0-2 ‰øÆÂ§ç„Äë‰∏âÂ§ÑÂÖ≥ÈîÆÊîπÂä®Ôºö
                # 1. Ë∑≥Ëøá layer 0Ôºàinit_pointsÔºâÔºöÁé∞Âú®Áî®ÈîöÁÇπ+ÂÅèÁßªÔºå‰∏çÈúÄË¶ÅÁõ¥Êé•ÁõëÁù£
                #    init_points ÁöÑÁõëÁù£‰ø°Âè∑‰ºöÈÄöËøá refinement layers Èó¥Êé•‰º†Âõû
                # 2. Èôç‰ΩéËæÖÂä©ÊùÉÈáçÔºö0.5√ó6=3.0ÔºàÊóßÔºâ‚Üí 0.2√ó5=1.0ÔºàÊñ∞Ôºâ
                #    ÊóßÊñπÊ°àËæÖÂä©ÊçüÂ§±Âç†ÊÄªÊçüÂ§± 82%Ôºå‰∏ªÊçüÂ§±‰ªÖ 18%
                #    Êñ∞ÊñπÊ°àËæÖÂä©:‰∏ª = 1:1ÔºåÊ¢ØÂ∫¶‰ø°Âè∑Êõ¥Âπ≥Ë°°
                # 3. „ÄêP0-3 ‰øÆÂ§ç„Äëpred_logits ‰ΩøÁî® detach()
                #    ÊóßÊñπÊ°àÔºöÂàÜÁ±ªÂ§¥Âú® 7 ‰∏™ÊçüÂ§±È°π‰∏≠ÈÉΩÂèçÂêë‰º†Êí≠Ôºà1‰∏ª+6ËæÖÔºâÔºåÊ¢ØÂ∫¶Ë¢´ 7√ó ÊîæÂ§ß
                #    Êñ∞ÊñπÊ°àÔºöËæÖÂä©ÊçüÂ§±‰∏ç‰º†Ê¢ØÂ∫¶Âà∞ÂàÜÁ±ªÂ§¥ÔºåÂàÜÁ±ªÂ§¥Âè™Ë¢´‰∏ªÊçüÂ§±Êõ¥Êñ∞
                #
                if 'init_points' in decoder_output and 'intermediate_points' in decoder_output:
                    intermediate_points = decoder_output['intermediate_points']
                    
                    # intermediate_points: [init, layer1, layer2, layer3, layer4, layer5, layer6(=final)]
                    # intermediate_points[:-1] = [init, layer1, ..., layer5] (ÂÖ± 6 ‰∏™)
                    # Ë∑≥Ëøá index 0 (init_points)ÔºåÂè™ÁõëÁù£ index 1-5
                    num_aux = len(intermediate_points) - 1  # 6Ôºà‰∏çÂê´ÊúÄÁªàÂ±ÇÔºâ
                    aux_weight_per_layer = 0.2  # 5 Â±Ç √ó 0.2 = ÊÄªËæÖÂä©ÊùÉÈáç 1.0
                    
                    aux_loss_total = 0.0
                    for i in range(1, num_aux):  # i = 1, 2, 3, 4, 5ÔºàË∑≥Ëøá i=0 Âç≥ init_pointsÔºâ
                        aux_pts = intermediate_points[i]
                        
                        # „ÄêP0-3 ‰øÆÂ§ç„Äëpred_logits.detach() ‚Äî ËæÖÂä©ÊçüÂ§±‰∏çÊõ¥Êñ∞ÂàÜÁ±ªÂ§¥
                        aux_loss_dict = self._compute_aux_full_loss(
                            pred_logits=pred_logits_f32.detach(),
                            pred_points=aux_pts.float(),
                            gt_labels_list=gt_labels_list,
                            gt_points_list=gt_points_list,
                            gt_masks_list=gt_masks_list,
                        )
                        
                        # Âä†ÊùÉÊ±ÇÂíåÔºö‰∏é‰∏ªÊçüÂ§±‰ΩøÁî®Áõ∏ÂêåÁöÑÊùÉÈáçÊØî‰æã
                        aux_layer_loss = (
                            self.criterion.weight_cls * aux_loss_dict['cls'] +
                            self.criterion.weight_pts * aux_loss_dict['pts'] +
                            self.criterion.weight_dir * aux_loss_dict['dir']
                        )
                        aux_loss_total = aux_loss_total + aux_weight_per_layer * aux_layer_loss
                        
                        # ËÆ∞ÂΩïÂêÑÈ°πÊçüÂ§±ÔºàÁî®‰∫éÁõëÊéßÔºâ
                        loss_dict[f'loss_aux_{i}_cls'] = aux_loss_dict['cls'].detach()
                        loss_dict[f'loss_aux_{i}_pts'] = aux_loss_dict['pts'].detach()
                        loss_dict[f'loss_aux_{i}_dir'] = aux_loss_dict['dir'].detach()
                    
                    total_loss = total_loss + aux_loss_total
                    loss_dict['loss_aux_total'] = aux_loss_total.detach() if isinstance(aux_loss_total, torch.Tensor) else torch.tensor(aux_loss_total)
            
            output['loss_dict'] = loss_dict
            output['loss'] = total_loss
        
        return output
    
    def _compute_aux_full_loss(
        self,
        pred_logits: torch.Tensor,
        pred_points: torch.Tensor,
        gt_labels_list: list,
        gt_points_list: list,
        gt_masks_list: list,
    ) -> Dict[str, torch.Tensor]:
        """
        ËÆ°ÁÆóÂÆåÊï¥ÁöÑËæÖÂä©ÊçüÂ§±Ôºàcls + pts + dirÔºâÔºå‰∏é MapTR ËÆæËÆ°‰∏ÄËá¥„ÄÇ
        
        ÂØπÊØè‰∏™‰∏≠Èó¥Â±ÇÈÉΩËÆ°ÁÆóÂÆåÊï¥ÁöÑ‰∏âÈ°πÊçüÂ§±ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÁõëÁù£‰ø°Âè∑„ÄÇ
        
        Args:
            pred_logits: [B, N, 3] ÂàÜÁ±ªÈ¢ÑÊµãÔºà‰ΩøÁî®ÊúÄÁªàÂ±ÇÁöÑ logitsÔºâ
            pred_points: [B, N, P, 2] ‰∏≠Èó¥Â±ÇÁöÑÁÇπÈ¢ÑÊµã
            gt_labels_list: List[Tensor] ÁúüÂÆûÊ†áÁ≠æ
            gt_points_list: List[Tensor] ÁúüÂÆûÁÇπÂùêÊ†á
            gt_masks_list: List[Tensor] ÁÇπÊúâÊïàÊé©Á†Å
            
        Returns:
            Dict with 'cls', 'pts', 'dir' losses
        """
        B = pred_points.shape[0]
        N = pred_points.shape[1]
        device = pred_points.device
        dtype = pred_points.dtype
        
        # ‰ΩøÁî® matcher Ëé∑ÂèñÂåπÈÖçÂÖ≥Á≥ª
        indices = self._aux_matcher(
            pred_logits, pred_points, 
            gt_labels_list, gt_points_list, gt_masks_list
        )
        
        # ========== 1. ÂàÜÁ±ªÊçüÂ§± ==========
        target_classes = torch.full((B, N), 3, dtype=torch.long, device=device)  # 3 = background
        num_total_pos = 0
        
        for b, (pred_idx, gt_idx, _) in enumerate(indices):
            if len(pred_idx) > 0:
                target_classes[b, pred_idx] = gt_labels_list[b][gt_idx].to(device)
                num_total_pos += len(pred_idx)
        
        avg_factor = max(num_total_pos, 1.0)
        pred_logits_flat = pred_logits.reshape(-1, 3)
        target_classes_flat = target_classes.reshape(-1)
        loss_cls = self.criterion.focal_loss(pred_logits_flat, target_classes_flat, avg_factor=avg_factor)
        
        # ========== 2. ÁÇπË∑ùÁ¶ªÊçüÂ§± ==========
        all_pts_loss = []
        for b, (pred_idx, gt_idx, best_gt) in enumerate(indices):
            if len(pred_idx) == 0:
                continue
            matched_pred = pred_points[b, pred_idx]
            matched_gt = best_gt.to(device=device, dtype=dtype)
            matched_mask = gt_masks_list[b][gt_idx].to(device=device)
            
            diff = torch.abs(matched_pred - matched_gt)
            diff_masked = diff * matched_mask.unsqueeze(-1)
            all_pts_loss.append(diff_masked.sum())
        
        if num_total_pos == 0:
            loss_pts = pred_points.sum() * 0.0
        else:
            # „Äê‰øÆÂ§ç„ÄëÈô§‰ª•ÁÇπÊï∞ PÔºå‰∏é‰∏ªÊçüÂ§± _points_loss ‰øùÊåÅ‰∏ÄËá¥
            num_points = pred_points.shape[2]  # P = 20
            loss_pts = sum(all_pts_loss) / (avg_factor * num_points)
        
        # ========== 3. ÊñπÂêëÊçüÂ§± ==========
        all_dir_loss = []
        for b, (pred_idx, gt_idx, best_gt) in enumerate(indices):
            if len(pred_idx) == 0:
                continue
            matched_pred = pred_points[b, pred_idx]
            matched_gt = best_gt.to(device=device, dtype=dtype)
            matched_mask = gt_masks_list[b][gt_idx].to(device=device)
            
            # ÂèçÂΩí‰∏ÄÂåñÂà∞Áâ©ÁêÜÂùêÊ†á
            pred_denorm = matched_pred.clone()
            pred_denorm[..., 0] = matched_pred[..., 0] * 15.0
            pred_denorm[..., 1] = matched_pred[..., 1] * 30.0
            gt_denorm = matched_gt.clone()
            gt_denorm[..., 0] = matched_gt[..., 0] * 15.0
            gt_denorm[..., 1] = matched_gt[..., 1] * 30.0
            
            # ËÆ°ÁÆóÊñπÂêëÂêëÈáè
            pred_dirs = pred_denorm[:, 1:] - pred_denorm[:, :-1]
            gt_dirs = gt_denorm[:, 1:] - gt_denorm[:, :-1]
            
            # „ÄêÊ†πÊú¨‰øÆÂ§ç„Äë‰ΩøÁî® sqrt(x^2 + eps) ‰ª£Êõø .norm()
            # .norm() Âú®Èõ∂ÁÇπÊ¢ØÂ∫¶‰∏∫ NaN (0/0)Ôºåtorch.where Êó†Ê≥ïÂ±èËîΩÔºàNaN*0=NaNÔºâ
            eps_sq = 1e-6
            pred_len = torch.sqrt((pred_dirs ** 2).sum(dim=-1, keepdim=True) + eps_sq)
            gt_len = torch.sqrt((gt_dirs ** 2).sum(dim=-1, keepdim=True) + eps_sq)
            
            # ÂÆâÂÖ®ÂΩí‰∏ÄÂåñ
            pred_dirs_norm = pred_dirs / pred_len
            gt_dirs_norm = gt_dirs / gt_len
            
            # ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÊçüÂ§±
            cosine_sim = (pred_dirs_norm * gt_dirs_norm).sum(dim=-1).clamp(-1, 1)
            dir_loss = 1.0 - cosine_sim
            
            # ËæπÊé©Á†ÅÔºàÁî® raw squared length Âà§Êñ≠ÔºåÈÅøÂÖç .norm()Ôºâ
            edge_mask = matched_mask[:, :-1] & matched_mask[:, 1:]
            raw_pred_len_sq = (pred_dirs ** 2).sum(dim=-1)
            raw_gt_len_sq = (gt_dirs ** 2).sum(dim=-1)
            valid_edge = (raw_pred_len_sq > 1e-4) & (raw_gt_len_sq > 1e-4)
            final_mask = edge_mask & valid_edge
            
            dir_loss_masked = dir_loss * final_mask.to(dir_loss.device)
            all_dir_loss.append(dir_loss_masked.sum())
        
        if num_total_pos == 0:
            loss_dir = pred_points.sum() * 0.0
        else:
            # „Äê‰øÆÂ§ç„ÄëÈô§‰ª•ËæπÊï∞ (P-1)Ôºå‰∏é‰∏ªÊçüÂ§± _direction_loss ‰øùÊåÅ‰∏ÄËá¥
            num_edges = pred_points.shape[2] - 1  # P-1 = 19
            loss_dir = sum(all_dir_loss) / (avg_factor * num_edges)
        
        return {
            'cls': loss_cls,
            'pts': loss_pts,
            'dir': loss_dir,
        }
    
    @torch.no_grad()
    def predict(
        self,
        images: torch.Tensor,
        text_ids: torch.Tensor,
        score_threshold: float = 0.3,
    ) -> Dict[str, torch.Tensor]:
        """
        Inference mode prediction.
        
        Args:
            images: (B, 6, 3, 448, 800) - 6 camera views (H=448, W=800)
            text_ids: (B, L)
            score_threshold: Confidence threshold
        
        Returns:
            dict with predictions
        """
        self.eval()
        
        output = self.forward(images, text_ids, return_loss=False)
        
        # Post-process: filter by score
        pred_logits = output['pred_logits']  # (B, 50, 3)
        pred_points = output['pred_points']  # (B, 50, 20, 2)
        
        # Get scores and labels
        # „Äê‰øÆÂ§ç„ÄëËÆ≠ÁªÉÁî® sigmoid focal lossÔºåÊé®ÁêÜ‰πüË¶ÅÁî® sigmoid ‰øùÊåÅ‰∏ÄËá¥
        pred_probs = pred_logits.sigmoid()  # (B, 50, 3)
        pred_scores, pred_labels = pred_probs.max(dim=-1)  # (B, 50)
        
        # Filter by threshold
        batch_predictions = []
        for b in range(pred_logits.shape[0]):
            valid_mask = pred_scores[b] >= score_threshold
            
            batch_predictions.append({
                'labels': pred_labels[b][valid_mask],
                'scores': pred_scores[b][valid_mask],
                'points': pred_points[b][valid_mask],
            })
        
        return batch_predictions


def build_map_detector(
    qformer_config_path: str = None,
    llm_path: str = "lmsys/vicuna-7b-v1.5",
    freeze_llm: bool = True,
    qformer_pretrained: str = None,
    qformer_version: str = 'v1',      # 'v1' = ÂéüÁâà, 'v2' = ‰∏âÈò∂ÊÆµÂèåÊµÅ
    use_lora: bool = True,            # ÈªòËÆ§ÂêØÁî® LoRA ÂæÆË∞É
    lora_r: int = 32,                  # Â¢ûÂä† rank ‰ª•Êèê‰æõË∂≥Â§üÂ≠¶‰π†ËÉΩÂäõ
    lora_alpha: int = 64,              # ‰øùÊåÅ alpha/r = 2
    lora_dropout: float = 0.1,
    lora_target_modules: Optional[list] = None,
) -> LLaVAMapDetector:
    """
    Build complete map detector model.
    
    Args:
        qformer_config_path: Path to Q-Former config (None = use default)
        llm_path: Path to LLM (default: Vicuna-7B)
        freeze_llm: Whether to freeze LLM (default: True, ignored if use_lora=True)
        qformer_pretrained: Q-Former pretrained weights
            - None: Random initialization (not recommended)
            - 'blip2': Load from BLIP-2 (recommended)
            - '/path/to/checkpoint.pth': Load from local file
        use_lora: Whether to use LoRA fine-tuning (default: True, Êé®ËçêÁî®‰∫éÂú∞ÂõæÊ£ÄÊµã)
        lora_r: LoRA rank (default: 32, Â¢ûÂä†‰ª•ÈÄÇÂ∫îÁ©∫Èó¥ÁêÜËß£‰ªªÂä°)
        lora_alpha: LoRA alpha (default: 64, ‰øùÊåÅ alpha/r=2)
        lora_dropout: LoRA dropout (default: 0.1)
        lora_target_modules: Target modules for LoRA 
            (default: ["q_proj", "k_proj", "v_proj", "o_proj"] - Âè™ÂæÆË∞É Attention Â±Ç)
    
    Returns:
        LLaVAMapDetector model
    
    Example:
        >>> # Freeze LLM (default, fast training)
        >>> model = build_map_detector(freeze_llm=True)
        
        >>> # LoRA fine-tuning (recommended for better performance)
        >>> model = build_map_detector(
        ...     use_lora=True,
        ...     lora_r=16,
        ...     lora_alpha=32,
        ... )
        
        >>> # Full fine-tuning (not recommended, requires lots of memory)
        >>> model = build_map_detector(freeze_llm=False)
    """
    # Default Q-Former config
    if qformer_config_path is None:
        if qformer_version == 'v1':
            qformer_config = {
                'img_backbone': 'resnet50',
                'embed_dims': 256,
                'num_queries': 768,
                'num_decoder_layers': 6,
                'llm_hidden_size': 4096,
                # Enhanced 3D Position Encoding (ABCÊñπÊ°à)
                'depth_num': 32,
                'depth_start': 1.0,
                'depth_max': 60.0,
                'use_lid': True,
                'pc_range': [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0],
            }
        elif qformer_version == 'v2':
            qformer_config = {
                'img_backbone': 'resnet50',
                'embed_dims': 256,
                'num_output_tokens': 768,
                'llm_hidden_size': 4096,
                # ‰∏âÈò∂ÊÆµÂ±ÇÊï∞ÈÖçÁΩÆ
                'num_image_encoder_layers': 5,
                'num_position_encoder_layers': 5,
                'num_cross_attn_layers': 3,
                'num_fusion_self_attn_layers': 3,
                'num_compression_layers': 2,
                # 3D Position Encoding
                'depth_num': 32,
                'depth_start': 1.0,
                'depth_max': 60.0,
                'use_lid': True,
                'pc_range': [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0],
            }
        else:
            raise ValueError(f"Unknown qformer_version: {qformer_version}. Use 'v1' or 'v2'.")
    else:
        import json
        with open(qformer_config_path, 'r') as f:
            qformer_config = json.load(f)
    
    # ËÆ∞ÂΩï Q-Former ÁâàÊú¨Âà∞ configÔºå‰æõ LLaVAMapDetector ‰ΩøÁî®
    qformer_config['_version'] = qformer_version
    
    model = LLaVAMapDetector(
        qformer_config=qformer_config,
        llm_path=llm_path,
        freeze_llm=freeze_llm,
        qformer_pretrained_path=qformer_pretrained,
        use_lora=use_lora,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        lora_target_modules=lora_target_modules,
    )
    
    return model


if __name__ == "__main__":
    print("Testing LLaVAMapDetector...")
    
    # Build model
    model = build_map_detector(freeze_llm=True)
    
    # Test input (H=448 divisible by 32, W=800)
    batch_size = 2
    images = torch.randn(batch_size, 6, 3, 448, 800)
    text_ids = torch.randint(0, 32000, (batch_size, 100))
    
    # GT data
    gt_labels = torch.randint(0, 3, (batch_size, 10))
    gt_points = torch.randn(batch_size, 10, 20, 2)
    gt_masks = torch.ones(batch_size, 10, dtype=torch.bool)
    
    print(f"\nForward pass (with loss)...")
    output = model(
        images=images,
        text_ids=text_ids,
        return_loss=True,
        gt_labels=gt_labels,
        gt_points=gt_points,
        gt_masks=gt_masks,
    )
    
    print(f"\nOutput keys: {output.keys()}")
    print(f"  pred_logits: {output['pred_logits'].shape}")
    print(f"  pred_points: {output['pred_points'].shape}")
    print(f"  loss: {output['loss'].item():.4f}")
    
    print(f"\n‚úÖ Test passed!")

