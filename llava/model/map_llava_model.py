"""
Complete End-to-End LLaVA Map Detection Model

Flow:
    Images (6 views) â†’ Q-Former â†’ Scene Tokens (768)
                                        â†“
    Text Prompt â†’ Embed â†’ Text Embeds
                                        â†“
    Learnable Queries (1050) â†’ [Text + Scene + Queries]
                                        â†“
                                    LLM Forward
                                        â†“
                Extract Instance/Point Features + Scene Tokens
                  - instance_features: (B, 50, 4096)
                  - point_features: (B, 50, 20, 4096)
                  - scene_tokens: (B, 768, 4096)
                                        â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Map-Scene Interaction Layer (æ–°å¢ï¼)     â”‚
                    â”‚                                           â”‚
                    â”‚   Map Features â†â”€Cross-Attentionâ”€â†’ Scene  â”‚
                    â”‚   (è®© Map Queries ç›´æ¥ä»å›¾åƒæå–ä¿¡æ¯)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â†“
                    Map Decoder (Instance-Conditioned Point Prediction)
                      - inst_reduced + pt_reduced â†’ concat â†’ PointHead
                                        â†“
                            Predictions (logits, points, bbox)

Author: Auto-generated for Map Detection
Date: 2025-01
"""

import torch
import torch.nn as nn
from typing import Dict, Optional, Tuple
from transformers import AutoTokenizer

from .qformer import QFormer, build_qformer
from .language_model.llava_map import LlavaMapDetectionModel
from .map_decoder import MapDecoder
from .map_config import MapDetectionConfig, DEFAULT_MAP_CONFIG
from .map_scene_interaction import MapSceneInteractionLayer, build_map_scene_interaction

# LoRA support
try:
    from peft import get_peft_model, LoraConfig, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("Warning: peft not installed. LoRA fine-tuning will not be available.")
    print("Install with: pip install peft")


class LLaVAMapDetector(nn.Module):
    """
    Complete end-to-end model for map detection using LLaVA architecture.
    
    Components:
    1. Q-Former: 6 camera images â†’ 768 scene tokens
    2. LLM: text + scene + 1050 queries â†’ hidden states
    3. Decoder: hidden states â†’ predictions
    """
    
    def __init__(
        self,
        qformer_config: dict,
        llm_path: str = "lmsys/vicuna-7b-v1.5",
        map_config: MapDetectionConfig = None,
        freeze_llm: bool = True,
        qformer_pretrained_path: Optional[str] = None,
        use_lora: bool = True,           # é»˜è®¤å¯ç”¨ LoRA å¾®è°ƒ
        lora_r: int = 32,                 # å¢åŠ  rank ä»¥æä¾›è¶³å¤Ÿå­¦ä¹ èƒ½åŠ›
        lora_alpha: int = 64,             # ä¿æŒ alpha/r = 2
        lora_dropout: float = 0.1,
        lora_target_modules: Optional[list] = None,
    ):
        """
        Args:
            qformer_config: Config dict for Q-Former
            llm_path: Path to pretrained LLM
            map_config: Map detection config
            freeze_llm: Whether to freeze LLM parameters (ignored if use_lora=True)
            qformer_pretrained_path: Path to BLIP-2 pretrained Q-Former weights (optional)
            use_lora: Whether to use LoRA fine-tuning for LLM (default: True)
            lora_r: LoRA rank (default: 32, å¢åŠ ä»¥é€‚åº”ç©ºé—´ç†è§£ä»»åŠ¡)
            lora_alpha: LoRA alpha scaling factor (default: 64, ä¿æŒ alpha/r=2)
            lora_dropout: LoRA dropout (default: 0.1)
            lora_target_modules: Which modules to apply LoRA 
                (default: ["q_proj", "k_proj", "v_proj", "o_proj"] - åªå¾®è°ƒ Attention å±‚)
        """
        self.use_lora = use_lora
        super().__init__()
        
        self.config = map_config or DEFAULT_MAP_CONFIG
        
        # 1. Q-Former for multi-view encoding
        print(f"\n{'='*60}")
        print(f"Initializing Q-Former...")
        print(f"{'='*60}")
        self.qformer = build_qformer(qformer_config)
        # Move Q-Former to GPU
        # Note: Keep FP32 for numerical stability, will be cast to FP16 via autocast if needed
        self.qformer = self.qformer.cuda()
        
        # Load pretrained Q-Former weights if provided
        if qformer_pretrained_path is not None:
            self._load_qformer_pretrained(qformer_pretrained_path)
        else:
            print(f"âš ï¸  Q-Former initialized from scratch (random weights)")
            print(f"   Tip: Use qformer_pretrained_path='blip2' for better performance")
        
        # 2. LLM with map queries
        print(f"\n{'='*60}")
        print(f"Loading LLM: {llm_path}")
        print(f"{'='*60}")
        
        # æ£€æµ‹æ˜¯å¦åœ¨åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­
        # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œä¸ä½¿ç”¨ device_map="auto"ï¼ˆä¼šåˆ†å¸ƒåˆ°å¤šGPUå¯¼è‡´é—®é¢˜ï¼‰
        # è€Œæ˜¯å…ˆåŠ è½½åˆ° CPUï¼Œåç»­ç”± .cuda() å’Œ DDP å¤„ç†
        import os
        is_distributed = 'WORLD_SIZE' in os.environ and int(os.environ.get('WORLD_SIZE', 1)) > 1
        is_single_gpu = torch.cuda.device_count() == 1
        
        # ã€å…³é”®ã€‘ä½¿ç”¨ BF16 è€Œé FP16 åŠ è½½ LLM
        # BF16 æŒ‡æ•°èŒƒå›´ä¸ FP32 ç›¸åŒï¼ˆmax ~3.4e38ï¼‰ï¼Œåå‘ä¼ æ’­æ¢¯åº¦ä¸ä¼šæº¢å‡º
        # FP16 max ä»… 65504ï¼Œ7B æ¨¡å‹ 32 å±‚åå‘ä¼ æ’­æ¢¯åº¦å¿…ç„¶æº¢å‡º
        # RTX 4090 (Ada Lovelace, compute capability 8.9) å®Œå…¨æ”¯æŒ BF16
        llm_dtype = torch.bfloat16
        self.llm_dtype = llm_dtype
        print(f"  LLM dtype: {llm_dtype} (BF16 prevents gradient overflow in backward pass)")
        
        if is_distributed or is_single_gpu:
            print(f"  Mode: {'Distributed' if is_distributed else 'Single GPU'} - loading to CPU first")
            self.llm = LlavaMapDetectionModel.from_pretrained(
                llm_path,
                torch_dtype=llm_dtype,
                device_map=None,
                low_cpu_mem_usage=True,
            )
        else:
            print(f"  Mode: Multi-GPU Auto - using device_map='auto'")
            self.llm = LlavaMapDetectionModel.from_pretrained(
                llm_path,
                torch_dtype=llm_dtype,
                device_map="auto",
            )
        
        # Fix: Convert map_queries to FP32 for stable training
        # (FP16 parameters can overflow during optimizer updates)
        print(f"Converting Map Queries to FP32 for training stability...")
        self.llm.map_queries = self.llm.map_queries.float()
        
        # Re-initialize with proper values in FP32
        with torch.no_grad():
            device = self.llm.map_queries.instance_content.device
            
            # Re-init instance content
            self.llm.map_queries.instance_content.data = torch.randn(
                self.llm.map_queries.instance_content.shape,
                device=device, dtype=torch.float32
            ) * 0.02
            
            # Re-init point content
            self.llm.map_queries.point_content.data = torch.randn(
                self.llm.map_queries.point_content.shape,
                device=device, dtype=torch.float32
            ) * 0.02
        
        # Verify
        print(f"âœ… Map Queries: dtype={self.llm.map_queries.instance_content.dtype}, "
              f"no_nan={not torch.isnan(self.llm.map_queries.instance_content).any()}")
        
        print(f"âœ… LLM loaded successfully!")
        
        # 3. Tokenizer for text (from local path)
        print(f"\nLoading tokenizer from local path...")
        self.tokenizer = AutoTokenizer.from_pretrained(llm_path, use_fast=False, local_files_only=True)
        print(f"âœ… Tokenizer loaded from local: {llm_path}")
        
        # 4. Map-Scene Interaction Layer (æ–°å¢ï¼)
        # åœ¨ LLM è¾“å‡ºåã€Decoder ä¹‹å‰ï¼Œè®© Map Features ç›´æ¥å’Œ Scene Tokens äº¤äº’
        print(f"\n{'='*60}")
        print(f"Initializing Map-Scene Interaction Layer...")
        print(f"{'='*60}")
        self.map_scene_interaction = build_map_scene_interaction(
            input_dim=4096,      # LLM hidden size
            embed_dim=256,       # äº¤äº’å±‚ç»´åº¦
            num_heads=8,         # æ³¨æ„åŠ›å¤´æ•°
            num_layers=6,        # 6 å±‚äº¤äº’ï¼ˆä¸ MapTR Decoder å¯¹é½ï¼‰
            ffn_dim=1024,        # FFN ç»´åº¦
            dropout=0.1,
        )
        self.map_scene_interaction = self.map_scene_interaction.cuda()
        print(f"âœ… Map-Scene Interaction Layer initialized (6 layers)")
        
        # 5. Decoder for predictions
        print(f"\n{'='*60}")
        print(f"Initializing Map Decoder...")
        print(f"{'='*60}")
        self.decoder = MapDecoder(self.config)
        # Move Decoder to GPU
        self.decoder = self.decoder.cuda()
        print(f"âœ… Map Decoder initialized (random weights)")
        
        # 6. Loss function (åœ¨ __init__ ä¸­åˆ›å»ºï¼Œè€Œä¸æ˜¯ forward ä¸­åŠ¨æ€åˆ›å»º)
        # dir_loss æŒ‰å®ä¾‹æ•°å½’ä¸€åŒ–ï¼Œé‡çº§çº¦ 9.5
        # weight_dir=0.25 æ˜¯æŠ˜ä¸­æ–¹æ¡ˆï¼šæ–¹å‘æŸå¤±è´¡çŒ®çº¦ 5-8%ï¼Œæœ‰æ„ä¹‰ä½†ä¸ä¸»å¯¼è®­ç»ƒ
        from .map_loss import MapDetectionLoss, HungarianMatcher
        self.criterion = MapDetectionLoss(
            num_classes=3,
            weight_cls=2.0,
            weight_pts=5.0,
            weight_dir=0.25,  # æŠ˜ä¸­æ–¹æ¡ˆï¼ˆMapTRç”¨0.005å‡ ä¹æ— ä½œç”¨ï¼Œ2.0ä¼šä¸»å¯¼è®­ç»ƒï¼‰
        )
        self._aux_matcher = HungarianMatcher(cost_class=2.0, cost_points=5.0)
        print(f"âœ… Loss function initialized")
        
        # 7. LoRA or Freeze LLM
        if use_lora:
            if not PEFT_AVAILABLE:
                raise ImportError("peft is required for LoRA. Install with: pip install peft")
            
            print(f"\n{'='*60}")
            print(f"Applying LoRA to LLM...")
            print(f"{'='*60}")
            
            # Default target modules for LLaMA-based models
            # é’ˆå¯¹åœ°å›¾æ£€æµ‹ä»»åŠ¡ä¼˜åŒ–çš„ LoRA é…ç½®ï¼š
            # - q_proj: Map Queries å¦‚ä½•æŸ¥è¯¢ Scene Tokensï¼ˆæ ¸å¿ƒï¼‰
            # - k_proj: Scene Tokens å¦‚ä½•è¢«ç´¢å¼•ï¼ˆé‡è¦ï¼‰
            # - v_proj: Scene Tokens æä¾›ä»€ä¹ˆä¿¡æ¯ï¼ˆæ ¸å¿ƒï¼‰
            # - o_proj: Attention è¾“å‡ºæŠ•å½±ï¼ˆé‡è¦ï¼‰
            # æ³¨ï¼šä¸åŒ…å« MLP å±‚ï¼Œå› ä¸ºæ£€æµ‹ä»»åŠ¡ä¸»è¦ä¾èµ– Attention æœºåˆ¶
            if lora_target_modules is None:
                lora_target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
            
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=lora_r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                target_modules=lora_target_modules,
                bias="none",
            )
            
            # Apply LoRA to LLM
            self.llm = get_peft_model(self.llm, lora_config)
            
            # Make sure map_queries are trainable
            for param in self.llm.base_model.model.map_queries.parameters():
                param.requires_grad = True
            
            # ã€å…³é”®ä¿®å¤ã€‘å°† LoRA å‚æ•°è½¬æ¢ä¸º FP32
            # åŸå› ï¼šLLM ä»¥ FP16 åŠ è½½ï¼ŒLoRA ç»§æ‰¿ FP16ã€‚
            # åœ¨ FP16 ä¸‹ï¼ŒGradScaler ç¼©æ”¾åçš„æ¢¯åº¦ææ˜“æº¢å‡ºï¼ˆFP16 max=65504ï¼‰
            # å¯¼è‡´æ¯ä¸ªæ¢¯åº¦æ­¥éƒ½å‡ºç° NaN/Infã€‚
            # è½¬ä¸º FP32 åï¼Œæ¢¯åº¦èŒƒå›´æ‰©å¤§åˆ° 3.4e38ï¼Œå½»åº•è§£å†³æº¢å‡ºé—®é¢˜ã€‚
            lora_param_count = 0
            for name, param in self.llm.named_parameters():
                if 'lora_' in name and param.requires_grad:
                    param.data = param.data.float()
                    lora_param_count += 1
            print(f"âœ… Converted {lora_param_count} LoRA parameters to FP32")
            
            print(f"âœ… LoRA applied to LLM!")
            print(f"   - LoRA rank (r): {lora_r}")
            print(f"   - LoRA alpha: {lora_alpha}")
            print(f"   - LoRA dropout: {lora_dropout}")
            print(f"   - Target modules: {lora_target_modules}")
            self.llm.print_trainable_parameters()
            
        elif freeze_llm:
            print(f"\n{'='*60}")
            print(f"Freezing LLM backbone parameters...")
            print(f"{'='*60}")
            for param in self.llm.model.parameters():
                param.requires_grad = False
            # Only train map_queries
            for param in self.llm.map_queries.parameters():
                param.requires_grad = True
            print(f"âœ… LLM frozen, only training:")
            print(f"   - Q-Former")
            print(f"   - Map Queries (1050 learnable queries)")
            print(f"   - Map-Scene Interaction Layer")
            print(f"   - Map Decoder")
        else:
            print(f"\n{'='*60}")
            print(f"Full LLM fine-tuning enabled (not recommended)")
            print(f"{'='*60}")
        
        print(f"\n{'='*60}")
        print(f"âœ… LLaVAMapDetector initialized successfully!")
        print(f"{'='*60}")
        self._print_trainable_params()
    
    def _load_qformer_pretrained(self, pretrained_path: str):
        """
        Load pretrained Q-Former weights from BLIP-2 or custom checkpoint.
        
        Args:
            pretrained_path: 
                - 'blip2': Load from Salesforce BLIP-2
                - Local path: Load from local checkpoint
        """
        import os
        
        if pretrained_path == 'blip2':
            print(f"ğŸ“¥ Loading BLIP-2 pretrained Q-Former...")
            
            # Use local BLIP-2 path if available
            local_blip2_path = "/home/cly/auto/llava_test/LLaVA/blip2-opt-2.7b"
            
            try:
                from transformers import Blip2Model
                
                # Check if local BLIP-2 exists
                if os.path.exists(local_blip2_path):
                    print(f"   Loading from local: {local_blip2_path}")
                    blip2 = Blip2Model.from_pretrained(
                        local_blip2_path,
                        torch_dtype=torch.float32,
                        local_files_only=True,
                    )
                else:
                    print(f"   âš ï¸ Local BLIP-2 not found, trying remote: Salesforce/blip2-opt-2.7b")
                    blip2 = Blip2Model.from_pretrained(
                        "Salesforce/blip2-opt-2.7b",
                        torch_dtype=torch.float32,
                    )
                
                # Extract Q-Former components
                qformer_state = {}
                for name, param in blip2.named_parameters():
                    if 'qformer' in name or 'query_tokens' in name:
                        # Remove prefix
                        new_name = name.replace('qformer.', '')
                        new_name = new_name.replace('language_model.', '')
                        qformer_state[new_name] = param.data
                
                # Load into our Q-Former (partial loading, ignore size mismatch)
                missing, unexpected = self.qformer.load_state_dict(qformer_state, strict=False)
                
                print(f"âœ… BLIP-2 Q-Former loaded!")
                print(f"   Loaded parameters: {len(qformer_state)}")
                if len(missing) > 0:
                    print(f"   Missing keys (will use random init): {len(missing)}")
                if len(unexpected) > 0:
                    print(f"   Unexpected keys (ignored): {len(unexpected)}")
                
                del blip2  # Free memory
                
            except Exception as e:
                print(f"âš ï¸  Failed to load BLIP-2 weights: {e}")
                print(f"   Falling back to random initialization")
        
        elif os.path.exists(pretrained_path):
            print(f"ğŸ“¥ Loading Q-Former from local checkpoint...")
            print(f"   Path: {pretrained_path}")
            
            try:
                # Check if it's a directory (HuggingFace model format)
                if os.path.isdir(pretrained_path):
                    print(f"   Detected HuggingFace model directory, using from_pretrained...")
                    from transformers import Blip2Model
                    
                    # Load BLIP-2 model from local directory
                    blip2 = Blip2Model.from_pretrained(
                        pretrained_path,
                        torch_dtype=torch.float32,
                    )
                    
                    # Extract Q-Former components
                    qformer_state = {}
                    for name, param in blip2.named_parameters():
                        if 'qformer' in name or 'query_tokens' in name:
                            new_name = name.replace('qformer.', '')
                            new_name = new_name.replace('language_model.', '')
                            qformer_state[new_name] = param.data
                    
                    # Load into our Q-Former (partial loading)
                    missing, unexpected = self.qformer.load_state_dict(qformer_state, strict=False)
                    
                    print(f"âœ… BLIP-2 Q-Former loaded from local directory!")
                    print(f"   Loaded parameters: {len(qformer_state)}")
                    if len(missing) > 0:
                        print(f"   Missing keys (will use random init): {len(missing)}")
                    if len(unexpected) > 0:
                        print(f"   Unexpected keys (ignored): {len(unexpected)}")
                    
                    del blip2  # Free memory
                else:
                    # It's a single file checkpoint
                    state_dict = torch.load(pretrained_path, map_location='cpu')
                    
                    # Handle different checkpoint formats
                    if 'qformer' in state_dict:
                        state_dict = state_dict['qformer']
                    elif 'model' in state_dict:
                        state_dict = state_dict['model']
                    
                    missing, unexpected = self.qformer.load_state_dict(state_dict, strict=False)
                    
                    print(f"âœ… Q-Former checkpoint loaded!")
                    if len(missing) > 0:
                        print(f"   Missing keys: {len(missing)}")
                    if len(unexpected) > 0:
                        print(f"   Unexpected keys: {len(unexpected)}")
                    
            except Exception as e:
                print(f"âš ï¸  Failed to load checkpoint: {e}")
                import traceback
                traceback.print_exc()
                print(f"   Falling back to random initialization")
        
        else:
            print(f"âš ï¸  Pretrained path not found: {pretrained_path}")
            print(f"   Using random initialization")
    
    def _print_trainable_params(self):
        """Print trainable parameter statistics."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"\nParameter Statistics:")
        print(f"  Total: {total_params:,}")
        print(f"  Trainable: {trainable_params:,}")
        print(f"  Frozen: {total_params - trainable_params:,}")
        print(f"  Trainable %: {100 * trainable_params / total_params:.2f}%")
    
    def forward(
        self,
        images: torch.Tensor,
        text_ids: torch.Tensor,
        return_loss: bool = False,
        gt_labels: Optional[torch.Tensor] = None,
        gt_points: Optional[torch.Tensor] = None,
        gt_masks: Optional[torch.Tensor] = None,
        cam_intrinsics: Optional[torch.Tensor] = None,
        cam_extrinsics: Optional[torch.Tensor] = None,
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass of complete model.
        
        Args:
            images: (B, 6, 3, 448, 800) - 6 camera views (H=448, W=800)
            text_ids: (B, L) - Tokenized text with IMAGE_TOKEN_INDEX=-200
            return_loss: Whether to compute loss (requires GT)
            cam_intrinsics: (B, 6, 3, 3) - Camera intrinsic matrices (optional, for 3D pos encoding)
            cam_extrinsics: (B, 6, 4, 4) - Camera extrinsic matrices (optional, for 3D pos encoding)
            gt_labels: (B, M) - Ground truth class labels
            gt_points: (B, M, 20, 2) - Ground truth points
            gt_masks: (B, M) - Valid GT mask
        
        Returns:
            dict with keys:
                - pred_logits: (B, 50, 3) classification logits
                - pred_points: (B, 50, 20, 2) point coordinates
                - instance_features: (B, 50, 4096) (optional)
                - point_features: (B, 50, 20, 4096) (optional)
                - loss_dict: dict of losses (if return_loss=True)
        """
        batch_size = images.shape[0]
        
        # ===== Step 1: Q-Former - Images to Scene Tokens =====
        # Pass camera parameters for 3D position encoding if available
        scene_tokens = self.qformer(
            images, 
            cam_intrinsics=cam_intrinsics,
            cam_extrinsics=cam_extrinsics
        )  # (B, 768, 4096)
        
        # ã€å®‰å…¨æ£€æŸ¥ã€‘ç¡®è®¤ Q-Former è¾“å‡ºæ­£å¸¸ï¼ˆä¿®å¤ autocast åä¸åº”å†å‡ºç° NaNï¼‰
        if torch.isnan(scene_tokens).any() or torch.isinf(scene_tokens).any():
            print(f"âŒ [Forward] Q-Former output still contains NaN/Inf after autocast fix! "
                  f"This indicates a deeper issue.", flush=True)
        
        # Convert to match LLM precision (BF16)
        scene_tokens = scene_tokens.to(self.llm_dtype)
        
        # ===== Step 2: Embed Text and Replace IMAGE_TOKEN =====
        # Handle IMAGE_TOKEN_INDEX (-200) which is a placeholder for scene tokens
        from llava.constants import IMAGE_TOKEN_INDEX
        
        # Replace IMAGE_TOKEN_INDEX with a valid token ID (0 = pad token) temporarily
        text_ids_safe = text_ids.clone()
        image_token_mask = (text_ids == IMAGE_TOKEN_INDEX)
        text_ids_safe[image_token_mask] = 0  # Use pad token temporarily
        
        # Get text embeddings
        # Note: éœ€è¦å¤„ç† LoRA åŒ…è£…åçš„è®¿é—®è·¯å¾„
        if self.use_lora:
            # LoRA åŒ…è£…åè·¯å¾„: base_model.model.model.embed_tokens
            embed_tokens = self.llm.base_model.model.model.embed_tokens
        else:
            # åŸå§‹è·¯å¾„: model.embed_tokens
            embed_tokens = self.llm.model.embed_tokens
        text_embeds_temp = embed_tokens(text_ids_safe)  # (B, L, 4096)
        
        # Replace the embeddings at IMAGE_TOKEN positions with scene_tokens
        # Create new list to hold embeddings with varying lengths
        text_embeds_list = []
        expected_length = None
        
        for b in range(batch_size):
            image_positions = torch.where(image_token_mask[b])[0]
            if len(image_positions) > 0:
                # Replace IMAGE_TOKEN embedding with scene_tokens
                pos = image_positions[0].item()
                # Concatenate: text[:pos] + scene_tokens + text[pos+1:]
                new_embeds = torch.cat([
                    text_embeds_temp[b, :pos],
                    scene_tokens[b],  # Insert 768 scene tokens
                    text_embeds_temp[b, pos+1:]
                ], dim=0)
            else:
                # æ²¡æœ‰ IMAGE_TOKENï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ embeddings
                # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œæ·»åŠ è­¦å‘Š
                import warnings
                warnings.warn(f"Batch {b} has no IMAGE_TOKEN! This may cause issues.")
                new_embeds = text_embeds_temp[b]
            
            # æ£€æŸ¥é•¿åº¦ä¸€è‡´æ€§
            if expected_length is None:
                expected_length = new_embeds.shape[0]
            else:
                if new_embeds.shape[0] != expected_length:
                    raise ValueError(
                        f"Batch {b} has different embedding length {new_embeds.shape[0]} "
                        f"vs expected {expected_length}. "
                        f"Ensure all samples have IMAGE_TOKEN at the same position."
                    )
            
            text_embeds_list.append(new_embeds)
        
        # Stack back into tensor (all should have same length now)
        # æ³¨ï¼š768 scene tokens æ›¿æ¢ 1 ä¸ª IMAGE_TOKEN = å‡€å¢ 767 ä¸ª tokens
        text_embeds = torch.stack(text_embeds_list, dim=0)  # (B, L+767, 4096)
        
        # ===== Step 3: LLM Forward with Map Queries =====
        # This will:
        # - Add 1050 learnable queries
        # - Concatenate [text_with_scene, queries]
        # - Forward through LLM
        # - Extract instance and point features from query positions
        # Note: text_embeds now includes scene_tokens, so we pass it directly
        # and set scene_tokens=None to avoid double-adding
        
        # å¤„ç† LoRA æ¨¡å¼ï¼šPEFT åŒ…è£…åéœ€è¦é€šè¿‡ base_model è®¿é—®è‡ªå®šä¹‰æ–¹æ³•
        if self.use_lora:
            # LoRA æ¨¡å¼ï¼šé€šè¿‡ base_model è°ƒç”¨ forward_with_map
            llm_output = self.llm.base_model.forward_with_map(
                text_embeds=text_embeds,  # Already includes scene tokens
                scene_tokens=None,  # Don't add scene tokens again
                return_map_features=True,
            )
        else:
            # é LoRA æ¨¡å¼ï¼šç›´æ¥è°ƒç”¨
            llm_output = self.llm.forward_with_map(
                text_embeds=text_embeds,  # Already includes scene tokens
                scene_tokens=None,  # Don't add scene tokens again
                return_map_features=True,
            )
        
        # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨ query_outputs ä¿æŒ LLM åŸå§‹è¾“å‡ºé¡ºåº
        # åŸå§‹é¡ºåº: [Inst0, P0_1..P0_20, Inst1, P1_1..P1_20, ..., Inst49, P49_1..P49_20]
        query_outputs = llm_output['query_outputs']  # (B, 1050, 4096) - ä¿æŒåŸå§‹é¡ºåº
        
        # ã€å®‰å…¨æ£€æŸ¥ã€‘ç¡®è®¤ LLM è¾“å‡ºæ­£å¸¸
        if torch.isnan(query_outputs).any() or torch.isinf(query_outputs).any():
            print(f"âŒ [Forward] LLM query_outputs contains NaN/Inf!", flush=True)
        
        # è·å–ç»´åº¦ä¿¡æ¯
        B = query_outputs.shape[0]
        N_inst = 50   # å®ä¾‹æ•°é‡
        N_pts = 20    # æ¯ä¸ªå®ä¾‹çš„ç‚¹æ•°é‡
        queries_per_inst = 1 + N_pts  # 21 (1 instance query + 20 point queries)
        H = query_outputs.shape[2]  # hidden_size (4096)
        
        # ===== Step 4: Map-Scene Interaction (æ–°å¢ï¼) =====
        # è®© Map Features ç›´æ¥å’Œ Scene Tokens åš Cross-Attention
        # 
        # ã€é‡è¦è®¾è®¡å†³ç­–ã€‘
        # 1. ä½¿ç”¨**åŸå§‹ scene tokens**ï¼ˆQ-Former è¾“å‡ºï¼‰ï¼Œè€Œé LLM å¤„ç†åçš„
        # 2. ã€ä¼˜åŒ–ã€‘ä¿æŒ LLM è¾“å‡ºçš„åŸå§‹é¡ºåºé€å…¥ Map-Scene Interaction
        #    - åŸå§‹é¡ºåº: [Inst0, P0_1..P0_20, Inst1, P1_1..P1_20, ...]
        #    - å¥½å¤„: åŒä¸€å®ä¾‹çš„ instance å’Œ points åœ¨åºåˆ—ä¸­ç›¸é‚»ï¼Œ
        #           Self-Attention æ—¶æ›´å®¹æ˜“å»ºç«‹å±€éƒ¨å…³è”
        #
        # Map-Scene Interaction: Cross-Attention
        # ã€å…³é”®ã€‘ä½¿ç”¨åŸå§‹ scene_tokensï¼ˆQ-Former ç›´æ¥è¾“å‡ºï¼‰ï¼Œä¸ä½¿ç”¨ LLM å¤„ç†åçš„
        scene_tokens_for_interaction = scene_tokens  # ä½¿ç”¨ Q-Former åŸå§‹è¾“å‡º
        
        # ç¡®ä¿ dtype ä¸€è‡´ï¼ˆè½¬ä¸º FP32 ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼‰
        map_features_combined = query_outputs.to(dtype=torch.float32)
        scene_tokens_for_interaction = scene_tokens_for_interaction.to(dtype=torch.float32)
        
        # Cross-Attention: Map Features ä» Scene Tokens æå–è§†è§‰ä¿¡æ¯
        enhanced_map_features = self.map_scene_interaction(
            map_features=map_features_combined,
            scene_tokens=scene_tokens_for_interaction,
        )  # (B, 1050, 4096)
        
        # ã€å®‰å…¨æ£€æŸ¥ã€‘ç¡®è®¤ Map-Scene Interaction è¾“å‡ºæ­£å¸¸
        if torch.isnan(enhanced_map_features).any() or torch.isinf(enhanced_map_features).any():
            print(f"âŒ [Forward] Map-Scene Interaction output contains NaN/Inf!", flush=True)
        
        # ã€ä¼˜åŒ–ã€‘ä»å¢å¼ºåçš„ç‰¹å¾ä¸­æŒ‰åŸå§‹é¡ºåºé‡æ–°æå– instance å’Œ point features
        # åŸå§‹é¡ºåº: [Inst0, P0_1..P0_20, Inst1, P1_1..P1_20, ...]
        instance_features_list = []
        point_features_list = []
        
        for i in range(N_inst):
            start_idx = i * queries_per_inst
            # Instance query ä½äºæ¯ç»„çš„ç¬¬ä¸€ä¸ªä½ç½®
            inst_feat = enhanced_map_features[:, start_idx:start_idx+1, :]  # (B, 1, H)
            instance_features_list.append(inst_feat)
            # Point queries ä½äº instance ä¹‹åçš„ 20 ä¸ªä½ç½®
            point_feat = enhanced_map_features[:, start_idx+1:start_idx+queries_per_inst, :]  # (B, 20, H)
            point_features_list.append(point_feat)
        
        # æ‹¼æ¥æˆæœ€ç»ˆå½¢çŠ¶
        instance_features = torch.cat(instance_features_list, dim=1)  # (B, 50, H)
        point_features = torch.stack(point_features_list, dim=1)      # (B, 50, 20, H)
        
        # ===== Step 5: Decode to Predictions =====
        # Move features to decoder's device and dtype
        decoder_device = next(self.decoder.parameters()).device
        decoder_dtype = next(self.decoder.parameters()).dtype
        instance_features = instance_features.to(device=decoder_device, dtype=decoder_dtype)
        point_features = point_features.to(device=decoder_device, dtype=decoder_dtype)
        
        # æ³¨ï¼šä¸å† clamp Decoder è¾“å…¥ï¼Œé¿å…é˜»æ–­æ¢¯åº¦æµ
        
        # Instance-Conditioned Point Prediction
        # Uses both instance_features and point_features
        decoder_output = self.decoder(instance_features, point_features)
        
        pred_logits = decoder_output['class_logits']  # (B, 50, 3)
        pred_points = decoder_output['points']        # (B, 50, 20, 2)
        pred_bbox = decoder_output['bbox']            # (B, 50, 4)
        
        # ã€å®‰å…¨æ£€æŸ¥ã€‘ç¡®è®¤ Decoder è¾“å‡ºæ­£å¸¸
        if torch.isnan(pred_logits).any() or torch.isinf(pred_logits).any():
            print(f"âŒ [Forward] Decoder pred_logits contains NaN/Inf!", flush=True)
        if torch.isnan(pred_points).any() or torch.isinf(pred_points).any():
            print(f"âŒ [Forward] Decoder pred_points contains NaN/Inf!", flush=True)
        
        # Build output dict
        output = {
            'pred_logits': pred_logits,
            'pred_points': pred_points,
            'pred_bbox': pred_bbox,
            'instance_features': instance_features,
            'point_features': point_features,
        }
        
        # ===== Step 6: Compute Loss (if requested) =====
        if return_loss:
            if gt_labels is None or gt_points is None or gt_masks is None:
                raise ValueError("GT data required for loss computation")
            
            # ã€å…³é”®ä¿®å¤ã€‘æŸå¤±è®¡ç®—å¿…é¡»åœ¨ FP32 ä¸‹æ‰§è¡Œï¼
            # åŸå› ï¼šautocast ä¼šæŠŠæŸäº›æ“ä½œé™ä¸º FP16ï¼ˆå¦‚ F.binary_cross_entropy_with_logitsï¼‰ï¼Œ
            # å¯¼è‡´å¤§ loss å€¼åœ¨åå‘ä¼ æ’­æ—¶äº§ç”Ÿ FP16 æ¢¯åº¦æº¢å‡ºã€‚
            # ä½¿ç”¨ autocast(enabled=False) ç¡®ä¿æ‰€æœ‰æŸå¤±è®¡ç®—éƒ½åœ¨ FP32 ä¸‹ã€‚
            with torch.cuda.amp.autocast(enabled=False):
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½æ˜¯ FP32
                pred_logits_f32 = pred_logits.float()
                pred_points_f32 = pred_points.float()
                
                # Prepare GT lists (Loss expects lists)
                gt_labels_list = []
                gt_points_list = []
                gt_masks_list = []
                
                for b in range(batch_size):
                    mask = gt_masks[b]  # (M,)
                    num_valid = int(mask.sum().item())  # ç¡®ä¿æ˜¯æ•´æ•°ï¼Œé¿å…æµ®ç‚¹åˆ‡ç‰‡é”™è¯¯
                    
                    if num_valid > 0:
                        gt_labels_list.append(gt_labels[b, :num_valid])
                        gt_points_list.append(gt_points[b, :num_valid].float())
                        gt_masks_list.append(torch.ones(num_valid, 20, dtype=torch.bool, device=mask.device))
                    else:
                        # Empty GT
                        gt_labels_list.append(torch.empty(0, dtype=torch.long, device=mask.device))
                        gt_points_list.append(torch.empty(0, 20, 2, dtype=torch.float32, device=mask.device))
                        gt_masks_list.append(torch.empty(0, 20, dtype=torch.bool, device=mask.device))
                
                # Compute main loss (final points)
                total_loss, loss_dict = self.criterion(
                    pred_logits=pred_logits_f32,
                    pred_lines=pred_points_f32,
                    gt_labels=gt_labels_list,
                    gt_lines=gt_points_list,
                    gt_masks=gt_masks_list,
                )
                
                # ========== è¾…åŠ©æŸå¤±ï¼šç›‘ç£åˆå§‹ç‚¹å’Œä¸­é—´å±‚ï¼ˆå®Œæ•´ç›‘ç£ï¼‰==========
                # ä¸ MapTR ä¸€è‡´ï¼šå¯¹æ¯ä¸ªä¸­é—´å±‚è®¡ç®—å®Œæ•´çš„ cls + pts + dir æŸå¤±
                if 'init_points' in decoder_output and 'intermediate_points' in decoder_output:
                    intermediate_points = decoder_output['intermediate_points']
                    
                    # intermediate_points åŒ…å«: [init, layer1, layer2, layer3, layer4, layer5, layer6(=final)]
                    # æˆ‘ä»¬ç›‘ç£é™¤æœ€ç»ˆå±‚å¤–çš„æ‰€æœ‰ä¸­é—´å±‚ (å…± 6 ä¸ª)
                    # ã€ä¿®æ”¹ã€‘è¾…åŠ©æŸå¤±æƒé‡ï¼šç»Ÿä¸€æƒé‡ï¼Œé¿å…æ¢¯åº¦å†²çªå¯¼è‡´è®­ç»ƒä¸ç¨³å®š
                    # é€’å¢æƒé‡ [0.1-0.6] ä¼šå¯¼è‡´æµ…å±‚ç›‘ç£ä¸è¶³ã€æ·±å±‚è¿‡å¼ºï¼Œè¾…åŠ©æŸå¤±çˆ†ç‚¸ï¼ˆ6.7å€ä¸»æŸå¤±ï¼‰
                    # MapTR 3å±‚ç”¨ç»Ÿä¸€1.0ï¼Œæˆ‘ä»¬6å±‚ç”¨ç»Ÿä¸€0.5ï¼Œæ€»æƒé‡3.0å€ï¼ˆå¯¹é½MapTRï¼‰
                    num_aux = len(intermediate_points) - 1  # ä¸åŒ…æ‹¬æœ€ç»ˆå±‚
                    aux_weights = [0.5 for _ in range(num_aux)]  # [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
                    
                    aux_loss_total = 0.0
                    for i, aux_pts in enumerate(intermediate_points[:-1]):
                        # è®¡ç®—å®Œæ•´çš„è¾…åŠ©æŸå¤±ï¼ˆcls + pts + dirï¼‰ï¼Œä¸ MapTR ä¸€è‡´
                        aux_loss_dict = self._compute_aux_full_loss(
                            pred_logits=pred_logits_f32,  # åˆ†ç±»ä½¿ç”¨æœ€ç»ˆå±‚çš„ logits
                            pred_points=aux_pts.float(),
                            gt_labels_list=gt_labels_list,
                            gt_points_list=gt_points_list,
                            gt_masks_list=gt_masks_list,
                        )
                        
                        # åŠ æƒæ±‚å’Œï¼šä¸ä¸»æŸå¤±ä½¿ç”¨ç›¸åŒçš„æƒé‡æ¯”ä¾‹
                        aux_layer_loss = (
                            self.criterion.weight_cls * aux_loss_dict['cls'] +
                            self.criterion.weight_pts * aux_loss_dict['pts'] +
                            self.criterion.weight_dir * aux_loss_dict['dir']
                        )
                        aux_loss_total = aux_loss_total + aux_weights[i] * aux_layer_loss
                        
                        # è®°å½•å„é¡¹æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
                        loss_dict[f'loss_aux_{i}_cls'] = aux_loss_dict['cls'].detach()
                        loss_dict[f'loss_aux_{i}_pts'] = aux_loss_dict['pts'].detach()
                        loss_dict[f'loss_aux_{i}_dir'] = aux_loss_dict['dir'].detach()
                    
                    total_loss = total_loss + aux_loss_total
                    loss_dict['loss_aux_total'] = aux_loss_total.detach() if isinstance(aux_loss_total, torch.Tensor) else torch.tensor(aux_loss_total)
            
            output['loss_dict'] = loss_dict
            output['loss'] = total_loss
        
        return output
    
    def _compute_aux_full_loss(
        self,
        pred_logits: torch.Tensor,
        pred_points: torch.Tensor,
        gt_labels_list: list,
        gt_points_list: list,
        gt_masks_list: list,
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å®Œæ•´çš„è¾…åŠ©æŸå¤±ï¼ˆcls + pts + dirï¼‰ï¼Œä¸ MapTR è®¾è®¡ä¸€è‡´ã€‚
        
        å¯¹æ¯ä¸ªä¸­é—´å±‚éƒ½è®¡ç®—å®Œæ•´çš„ä¸‰é¡¹æŸå¤±ï¼Œæä¾›æ›´å¼ºçš„ç›‘ç£ä¿¡å·ã€‚
        
        Args:
            pred_logits: [B, N, 3] åˆ†ç±»é¢„æµ‹ï¼ˆä½¿ç”¨æœ€ç»ˆå±‚çš„ logitsï¼‰
            pred_points: [B, N, P, 2] ä¸­é—´å±‚çš„ç‚¹é¢„æµ‹
            gt_labels_list: List[Tensor] çœŸå®æ ‡ç­¾
            gt_points_list: List[Tensor] çœŸå®ç‚¹åæ ‡
            gt_masks_list: List[Tensor] ç‚¹æœ‰æ•ˆæ©ç 
            
        Returns:
            Dict with 'cls', 'pts', 'dir' losses
        """
        B = pred_points.shape[0]
        N = pred_points.shape[1]
        device = pred_points.device
        dtype = pred_points.dtype
        
        # ä½¿ç”¨ matcher è·å–åŒ¹é…å…³ç³»
        indices = self._aux_matcher(
            pred_logits, pred_points, 
            gt_labels_list, gt_points_list, gt_masks_list
        )
        
        # ========== 1. åˆ†ç±»æŸå¤± ==========
        target_classes = torch.full((B, N), 3, dtype=torch.long, device=device)  # 3 = background
        num_total_pos = 0
        
        for b, (pred_idx, gt_idx, _) in enumerate(indices):
            if len(pred_idx) > 0:
                target_classes[b, pred_idx] = gt_labels_list[b][gt_idx].to(device)
                num_total_pos += len(pred_idx)
        
        avg_factor = max(num_total_pos, 1.0)
        pred_logits_flat = pred_logits.reshape(-1, 3)
        target_classes_flat = target_classes.reshape(-1)
        loss_cls = self.criterion.focal_loss(pred_logits_flat, target_classes_flat, avg_factor=avg_factor)
        
        # ========== 2. ç‚¹è·ç¦»æŸå¤± ==========
        all_pts_loss = []
        for b, (pred_idx, gt_idx, best_gt) in enumerate(indices):
            if len(pred_idx) == 0:
                continue
            matched_pred = pred_points[b, pred_idx]
            matched_gt = best_gt.to(device=device, dtype=dtype)
            matched_mask = gt_masks_list[b][gt_idx].to(device=device)
            
            diff = torch.abs(matched_pred - matched_gt)
            diff_masked = diff * matched_mask.unsqueeze(-1)
            all_pts_loss.append(diff_masked.sum())
        
        if num_total_pos == 0:
            loss_pts = pred_points.sum() * 0.0
        else:
            loss_pts = sum(all_pts_loss) / avg_factor
        
        # ========== 3. æ–¹å‘æŸå¤± ==========
        all_dir_loss = []
        for b, (pred_idx, gt_idx, best_gt) in enumerate(indices):
            if len(pred_idx) == 0:
                continue
            matched_pred = pred_points[b, pred_idx]
            matched_gt = best_gt.to(device=device, dtype=dtype)
            matched_mask = gt_masks_list[b][gt_idx].to(device=device)
            
            # åå½’ä¸€åŒ–åˆ°ç‰©ç†åæ ‡
            pred_denorm = matched_pred.clone()
            pred_denorm[..., 0] = matched_pred[..., 0] * 15.0
            pred_denorm[..., 1] = matched_pred[..., 1] * 30.0
            gt_denorm = matched_gt.clone()
            gt_denorm[..., 0] = matched_gt[..., 0] * 15.0
            gt_denorm[..., 1] = matched_gt[..., 1] * 30.0
            
            # è®¡ç®—æ–¹å‘å‘é‡
            pred_dirs = pred_denorm[:, 1:] - pred_denorm[:, :-1]
            gt_dirs = gt_denorm[:, 1:] - gt_denorm[:, :-1]
            
            # ã€æ ¹æœ¬ä¿®å¤ã€‘ä½¿ç”¨ sqrt(x^2 + eps) ä»£æ›¿ .norm()
            # .norm() åœ¨é›¶ç‚¹æ¢¯åº¦ä¸º NaN (0/0)ï¼Œtorch.where æ— æ³•å±è”½ï¼ˆNaN*0=NaNï¼‰
            eps_sq = 1e-6
            pred_len = torch.sqrt((pred_dirs ** 2).sum(dim=-1, keepdim=True) + eps_sq)
            gt_len = torch.sqrt((gt_dirs ** 2).sum(dim=-1, keepdim=True) + eps_sq)
            
            # å®‰å…¨å½’ä¸€åŒ–
            pred_dirs_norm = pred_dirs / pred_len
            gt_dirs_norm = gt_dirs / gt_len
            
            # ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
            cosine_sim = (pred_dirs_norm * gt_dirs_norm).sum(dim=-1).clamp(-1, 1)
            dir_loss = 1.0 - cosine_sim
            
            # è¾¹æ©ç ï¼ˆç”¨ raw squared length åˆ¤æ–­ï¼Œé¿å… .norm()ï¼‰
            edge_mask = matched_mask[:, :-1] & matched_mask[:, 1:]
            raw_pred_len_sq = (pred_dirs ** 2).sum(dim=-1)
            raw_gt_len_sq = (gt_dirs ** 2).sum(dim=-1)
            valid_edge = (raw_pred_len_sq > 1e-4) & (raw_gt_len_sq > 1e-4)
            final_mask = edge_mask & valid_edge
            
            dir_loss_masked = dir_loss * final_mask.to(dir_loss.device)
            all_dir_loss.append(dir_loss_masked.sum())
        
        if num_total_pos == 0:
            loss_dir = pred_points.sum() * 0.0
        else:
            # æŒ‰å®ä¾‹æ•°å½’ä¸€åŒ–ï¼ˆä¸ MapTR ä¸€è‡´ï¼‰
            loss_dir = sum(all_dir_loss) / avg_factor
        
        return {
            'cls': loss_cls,
            'pts': loss_pts,
            'dir': loss_dir,
        }
    
    @torch.no_grad()
    def predict(
        self,
        images: torch.Tensor,
        text_ids: torch.Tensor,
        score_threshold: float = 0.3,
    ) -> Dict[str, torch.Tensor]:
        """
        Inference mode prediction.
        
        Args:
            images: (B, 6, 3, 448, 800) - 6 camera views (H=448, W=800)
            text_ids: (B, L)
            score_threshold: Confidence threshold
        
        Returns:
            dict with predictions
        """
        self.eval()
        
        output = self.forward(images, text_ids, return_loss=False)
        
        # Post-process: filter by score
        pred_logits = output['pred_logits']  # (B, 50, 3)
        pred_points = output['pred_points']  # (B, 50, 20, 2)
        
        # Get scores and labels
        pred_probs = torch.softmax(pred_logits, dim=-1)  # (B, 50, 3)
        pred_scores, pred_labels = pred_probs.max(dim=-1)  # (B, 50)
        
        # Filter by threshold
        batch_predictions = []
        for b in range(pred_logits.shape[0]):
            valid_mask = pred_scores[b] >= score_threshold
            
            batch_predictions.append({
                'labels': pred_labels[b][valid_mask],
                'scores': pred_scores[b][valid_mask],
                'points': pred_points[b][valid_mask],
            })
        
        return batch_predictions


def build_map_detector(
    qformer_config_path: str = None,
    llm_path: str = "lmsys/vicuna-7b-v1.5",
    freeze_llm: bool = True,
    qformer_pretrained: str = None,
    use_lora: bool = True,            # é»˜è®¤å¯ç”¨ LoRA å¾®è°ƒ
    lora_r: int = 32,                  # å¢åŠ  rank ä»¥æä¾›è¶³å¤Ÿå­¦ä¹ èƒ½åŠ›
    lora_alpha: int = 64,              # ä¿æŒ alpha/r = 2
    lora_dropout: float = 0.1,
    lora_target_modules: Optional[list] = None,
) -> LLaVAMapDetector:
    """
    Build complete map detector model.
    
    Args:
        qformer_config_path: Path to Q-Former config (None = use default)
        llm_path: Path to LLM (default: Vicuna-7B)
        freeze_llm: Whether to freeze LLM (default: True, ignored if use_lora=True)
        qformer_pretrained: Q-Former pretrained weights
            - None: Random initialization (not recommended)
            - 'blip2': Load from BLIP-2 (recommended)
            - '/path/to/checkpoint.pth': Load from local file
        use_lora: Whether to use LoRA fine-tuning (default: True, æ¨èç”¨äºåœ°å›¾æ£€æµ‹)
        lora_r: LoRA rank (default: 32, å¢åŠ ä»¥é€‚åº”ç©ºé—´ç†è§£ä»»åŠ¡)
        lora_alpha: LoRA alpha (default: 64, ä¿æŒ alpha/r=2)
        lora_dropout: LoRA dropout (default: 0.1)
        lora_target_modules: Target modules for LoRA 
            (default: ["q_proj", "k_proj", "v_proj", "o_proj"] - åªå¾®è°ƒ Attention å±‚)
    
    Returns:
        LLaVAMapDetector model
    
    Example:
        >>> # Freeze LLM (default, fast training)
        >>> model = build_map_detector(freeze_llm=True)
        
        >>> # LoRA fine-tuning (recommended for better performance)
        >>> model = build_map_detector(
        ...     use_lora=True,
        ...     lora_r=16,
        ...     lora_alpha=32,
        ... )
        
        >>> # Full fine-tuning (not recommended, requires lots of memory)
        >>> model = build_map_detector(freeze_llm=False)
    """
    # Default Q-Former config
    if qformer_config_path is None:
        qformer_config = {
            'img_backbone': 'resnet50',
            'embed_dims': 256,
            'num_queries': 768,
            'num_decoder_layers': 6,
            'llm_hidden_size': 4096,
            # Enhanced 3D Position Encoding (ABCæ–¹æ¡ˆ)
            'depth_num': 32,        # 32ä¸ªæ·±åº¦å‡è®¾ï¼ˆæ›´å¯†é›†çš„æ·±åº¦é‡‡æ ·ï¼‰
            'depth_start': 1.0,     # æœ€å°æ·±åº¦ 1ç±³
            'depth_max': 60.0,      # æœ€å¤§æ·±åº¦ 60ç±³
            'use_lid': True,        # æ–¹æ¡ˆB: LIDæ·±åº¦åˆ†å¸ƒ (è¿‘å¯†è¿œç–)
            # pc_range æ ¼å¼ï¼š[x_min, y_min, z_min, x_max, y_max, z_max]
            # ä¸ MapConfig ä¿æŒä¸€è‡´ï¼MapTR ä½¿ç”¨æ­¤èŒƒå›´
            'pc_range': [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0],
        }
    else:
        import json
        with open(qformer_config_path, 'r') as f:
            qformer_config = json.load(f)
    
    model = LLaVAMapDetector(
        qformer_config=qformer_config,
        llm_path=llm_path,
        freeze_llm=freeze_llm,
        qformer_pretrained_path=qformer_pretrained,
        use_lora=use_lora,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        lora_target_modules=lora_target_modules,
    )
    
    return model


if __name__ == "__main__":
    print("Testing LLaVAMapDetector...")
    
    # Build model
    model = build_map_detector(freeze_llm=True)
    
    # Test input (H=448 divisible by 32, W=800)
    batch_size = 2
    images = torch.randn(batch_size, 6, 3, 448, 800)
    text_ids = torch.randint(0, 32000, (batch_size, 100))
    
    # GT data
    gt_labels = torch.randint(0, 3, (batch_size, 10))
    gt_points = torch.randn(batch_size, 10, 20, 2)
    gt_masks = torch.ones(batch_size, 10, dtype=torch.bool)
    
    print(f"\nForward pass (with loss)...")
    output = model(
        images=images,
        text_ids=text_ids,
        return_loss=True,
        gt_labels=gt_labels,
        gt_points=gt_points,
        gt_masks=gt_masks,
    )
    
    print(f"\nOutput keys: {output.keys()}")
    print(f"  pred_logits: {output['pred_logits'].shape}")
    print(f"  pred_points: {output['pred_points'].shape}")
    print(f"  loss: {output['loss'].item():.4f}")
    
    print(f"\nâœ… Test passed!")

