# å®Œæ•´æ•°æ®æµï¼šä»nuScenesåˆ°Q-Formerè¾“å‡º

## ç›®å½•
- [é˜¶æ®µé›¶ï¼šnuScenesåŸå§‹æ•°æ®](#é˜¶æ®µé›¶nuscenesåŸå§‹æ•°æ®)
- [é˜¶æ®µä¸€ï¼šGTé¢„å¤„ç†ï¼ˆç¦»çº¿ï¼‰](#é˜¶æ®µä¸€gté¢„å¤„ç†ç¦»çº¿)
- [é˜¶æ®µäºŒï¼šDatasetåŠ è½½](#é˜¶æ®µäºŒdatasetåŠ è½½)
- [é˜¶æ®µä¸‰ï¼šBatchæ„å»º](#é˜¶æ®µä¸‰batchæ„å»º)
- [é˜¶æ®µå››ï¼šQ-Formerå¤„ç†](#é˜¶æ®µå››q-formerå¤„ç†)
- [å®Œæ•´æ•°æ®æµæ€»è§ˆ](#å®Œæ•´æ•°æ®æµæ€»è§ˆ)

---

## é˜¶æ®µé›¶ï¼šnuScenesåŸå§‹æ•°æ®

### æ•°æ®ç»“æ„
```
nuScenes/
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ CAM_FRONT/
â”‚   â”‚   â””â”€â”€ n015-2018-07-18-11-07-57+0800__CAM_FRONT__1531883530412470.jpg
â”‚   â”œâ”€â”€ CAM_FRONT_LEFT/
â”‚   â”œâ”€â”€ CAM_FRONT_RIGHT/
â”‚   â”œâ”€â”€ CAM_BACK/
â”‚   â”œâ”€â”€ CAM_BACK_LEFT/
â”‚   â””â”€â”€ CAM_BACK_RIGHT/
â”œâ”€â”€ v1.0-trainval/
â”‚   â”œâ”€â”€ sample.json           # å…³é”®å¸§ç´¢å¼•
â”‚   â”œâ”€â”€ sample_data.json      # ä¼ æ„Ÿå™¨æ•°æ®
â”‚   â”œâ”€â”€ ego_pose.json         # è½¦è¾†ä½å§¿
â”‚   â”œâ”€â”€ calibrated_sensor.json # ç›¸æœºæ ‡å®š
â”‚   â””â”€â”€ ...
â””â”€â”€ maps/expansion/
    â””â”€â”€ map_data.json         # åœ°å›¾æ ‡æ³¨
```

### å•ä¸ªSampleçš„åŸå§‹æ•°æ®

**å›¾åƒæ•°æ®**ï¼š
```python
6å¼ JPGå›¾åƒæ–‡ä»¶ï¼š
- è·¯å¾„ï¼šsamples/CAM_FRONT/xxx.jpg
- å°ºå¯¸ï¼š1600 Ã— 900 (width Ã— height)
- æ ¼å¼ï¼šRGB, uint8
- å€¼åŸŸï¼š[0, 255]
```

**ç›¸æœºå‚æ•°ï¼ˆä»JSONè¯»å–ï¼‰**ï¼š
```python
camera_intrinsic: List[3Ã—3] (6ä¸ªç›¸æœºå„ä¸€ä¸ª)
ç¤ºä¾‹ï¼š
[[1266.417, 0.0,      816.267],
 [0.0,      1266.417, 491.507],
 [0.0,      0.0,      1.0    ]]

camera_calibration: dict (6ä¸ª)
{
    "translation": [1.70, 0.016, 1.51],  # ç›¸æœºâ†’è½¦è¾†çš„å¹³ç§»
    "rotation": [0.5, -0.5, 0.5, -0.5]   # å››å…ƒæ•°æ ¼å¼
}
```

**è½¦è¾†ä½å§¿ï¼ˆä»JSONè¯»å–ï¼‰**ï¼š
```python
ego_pose: dict
{
    "translation": [411.3, 1180.9, 0.0],  # ä¸–ç•Œåæ ‡
    "rotation": [0.572, -0.002, 0.012, -0.820]  # å››å…ƒæ•°
}
```

**åœ°å›¾æ ‡æ³¨ï¼ˆä»map APIè·å–ï¼‰**ï¼š
```python
map_elements: List[dict]
[
    {
        "class": "lane_divider",
        "points": [[x1,y1], [x2,y2], ...],  # ä¸–ç•Œåæ ‡
        "is_closed": False
    },
    {
        "class": "ped_crossing",
        "points": [[x1,y1], [x2,y2], ...],  # ä¸–ç•Œåæ ‡
        "is_closed": True
    },
    ...
]
```

---

## é˜¶æ®µä¸€ï¼šGTé¢„å¤„ç†ï¼ˆç¦»çº¿ï¼‰

### æ­¥éª¤1.1ï¼šæå–åœ°å›¾å…ƒç´ ï¼ˆä¸–ç•Œåæ ‡ï¼‰

**ä»£ç ä½ç½®**ï¼š`llava/data/map_gt/nuscenes_map_api.py`

**è¾“å…¥**ï¼š
```python
sample_token: str
ego_pose: {
    "translation": [411.3, 1180.9, 0.0],
    "rotation": [0.572, -0.002, 0.012, -0.820]
}
```

**å¤„ç†**ï¼š
```python
# 1. è·å–è½¦è¾†ä½ç½®
ego_xy = ego_pose['translation'][:2]  # [411.3, 1180.9]

# 2. æœç´¢60ç±³åŠå¾„å†…çš„åœ°å›¾å…ƒç´ 
records = map_api.get_records_in_radius(
    x=ego_xy[0], y=ego_xy[1],
    radius=60.0,
    layer_names=['road_divider', 'lane_divider', 'ped_crossing']
)

# 3. æå–å‡ ä½•ä¿¡æ¯
for record in records:
    points_world = extract_geometry(record)  # [N, 2] ä¸–ç•Œåæ ‡
```

**è¾“å‡º**ï¼š
```python
map_elements: List[dict]
[
    {
        'class_id': 0,  # road_divider
        'points_world': array([[411.5, 1181.2],
                              [412.0, 1182.5],
                              ...], dtype=float32),  # [N, 2]
        'is_closed': False
    },
    {
        'class_id': 2,  # ped_crossing
        'points_world': array([[410.0, 1200.0],
                              [415.0, 1200.0],
                              ...], dtype=float32),  # [M, 2]
        'is_closed': True
    },
    ...
]
```

---

### æ­¥éª¤1.2ï¼šåæ ‡å˜æ¢ï¼ˆä¸–ç•Œâ†’è½¦è¾†egoï¼‰

**ä»£ç ä½ç½®**ï¼š`llava/data/map_gt/geometry.py` - `transform_to_ego()`

**è¾“å…¥**ï¼š
```python
points_world: array [N, 2]  # ä¸–ç•Œåæ ‡
    [[411.5, 1181.2],
     [412.0, 1182.5],
     ...]

ego_translation: array [3]  # [411.3, 1180.9, 0.0]
ego_rotation: array [3, 3]  # æ—‹è½¬çŸ©é˜µ
    [[cos(Î¸), -sin(Î¸), 0],
     [sin(Î¸),  cos(Î¸), 0],
     [0,       0,      1]]
```

**å¤„ç†**ï¼š
```python
# è½¬æ¢ä¸ºé½æ¬¡åæ ‡
points_3d = np.hstack([points_world, np.zeros((N, 1))])  # [N, 3]

# å˜æ¢ï¼šp_ego = R^T @ (p_world - t)
points_centered = points_3d - ego_translation
points_ego = points_centered @ ego_rotation.T

# åªä¿ç•™x, y
points_ego_2d = points_ego[:, :2]
```

**è¾“å‡º**ï¼š
```python
points_ego: array [N, 2], dtype=float32
    [[0.2, 0.3],    # è½¦è¾†å‰æ–¹0.2må³ï¼Œ0.3må‰
     [0.7, 1.8],
     ...]
# åæ ‡ç³»ï¼šxè½´å·¦(-)/å³(+)ï¼Œyè½´å(-)/å‰(+)
# å•ä½ï¼šç±³
```

---

### æ­¥éª¤1.3ï¼šROIè£å‰ª

**ä»£ç ä½ç½®**ï¼š`llava/data/map_gt/geometry.py` - `clip_polyline_by_roi()`

**è¾“å…¥**ï¼š
```python
points_ego: array [N, 2]  # å¯èƒ½è¶…å‡ºæ„ŸçŸ¥èŒƒå›´
PC_RANGE = [-15, -30, -2, 15, 30, 2]  # [x_min, y_min, z_min, x_max, y_max, z_max]
```

**å¤„ç†**ï¼š
```python
from shapely.geometry import LineString, box

# å®šä¹‰ROI
roi = box(x_min=-15, y_min=-30, x_max=15, y_max=30)

# è£å‰ª
if is_closed:  # Polygon
    poly = Polygon(points_ego)
    clipped = poly.intersection(roi)
else:  # Polyline
    line = LineString(points_ego)
    clipped = line.intersection(roi)

# æå–åæ ‡
clipped_points = np.array(clipped.coords)
```

**è¾“å‡º**ï¼š
```python
clipped_segments: List[array]
[
    array([[âˆ’14.5, 10.2],
           [âˆ’10.3, 12.5],
           ...], dtype=float32),  # [M1, 2]
    array([[5.0, âˆ’20.0],
           [8.0, âˆ’18.0],
           ...], dtype=float32),  # [M2, 2]
]
# æ³¨æ„ï¼šä¸€ä¸ªå…ƒç´ å¯èƒ½è¢«è£å‰ªæˆå¤šæ®µ
# åæ ‡èŒƒå›´ï¼šxâˆˆ[-15,15], yâˆˆ[-30,30]
```

---

### æ­¥éª¤1.4ï¼šé‡‡æ ·20ä¸ªç‚¹

**ä»£ç ä½ç½®**ï¼š`llava/data/map_gt/geometry.py` - `sample_polyline_20()`

**è¾“å…¥**ï¼š
```python
polyline: array [M, 2]  # Mä¸ªç‚¹ï¼Œæ•°é‡ä¸å®š
```

**å¤„ç†**ï¼š
```python
from shapely.geometry import LineString

line = LineString(polyline)
total_length = line.length

# æŒ‰å¼§é•¿å‡åŒ€é‡‡æ ·20ä¸ªç‚¹
distances = np.linspace(0, total_length, 20)
points_20 = []
for dist in distances:
    point = line.interpolate(dist)
    points_20.append([point.x, point.y])

points_20 = np.array(points_20, dtype=np.float32)
```

**è¾“å‡º**ï¼š
```python
points_20: array [20, 2], dtype=float32
    [[âˆ’14.5, 10.2],
     [âˆ’13.8, 10.5],
     [âˆ’13.1, 10.8],
     ...
     [8.0, âˆ’18.0]]

# å¯¹äºpolygonï¼ˆäººè¡Œæ¨ªé“ï¼‰ï¼š
points_20[0] â‰ˆ points_20[19]  # é¦–å°¾ç›¸åŒï¼ˆé—­åˆï¼‰
# é¡ºæ—¶é’ˆæ’åˆ—
```

---

### æ­¥éª¤1.5ï¼šåæ ‡å½’ä¸€åŒ–

**ä»£ç ä½ç½®**ï¼š`llava/data/map_dataset.py` - `_normalize_coords()`

**è¾“å…¥**ï¼š
```python
points_ego: array [20, 2], dtype=float32
    # å€¼åŸŸï¼šxâˆˆ[-15,15], yâˆˆ[-30,30] ç±³

PC_RANGE = [-15, -30, -2, 15, 30, 2]
```

**å¤„ç†**ï¼š
```python
x_min, x_max = -15, 15  # èŒƒå›´30ç±³
y_min, y_max = -30, 30  # èŒƒå›´60ç±³

# å½’ä¸€åŒ–åˆ°[-1, 1]
x_norm = (x - x_min) / (x_max - x_min) * 2 - 1
y_norm = (y - y_min) / (y_max - y_min) * 2 - 1
```

**è¾“å‡º**ï¼š
```python
points_normalized: array [20, 2], dtype=float32
    [[0.033, 0.340],   # x=0.033åœ¨[-1,1]ä¸­ï¼Œå¯¹åº”egoç³»ä¸­çº¦0.5ç±³
     [0.080, 0.350],
     ...
     [0.067, -0.200]]

# å€¼åŸŸï¼š[-1, 1]
# ä¸­å¿ƒç‚¹(0, 0)å¯¹åº”è½¦è¾†ä½ç½®
# (-1, -1)å¯¹åº”å·¦åè§’ï¼Œ(1, 1)å¯¹åº”å³å‰è§’
```

---

### æ­¥éª¤1.6ï¼šè®¡ç®—AABBåŒ…å›´ç›’

**ä»£ç ä½ç½®**ï¼š`llava/data/map_gt/geometry.py` - `compute_aabb()`

**è¾“å…¥**ï¼š
```python
points_normalized: array [20, 2]
```

**å¤„ç†**ï¼š
```python
x_coords = points[:, 0]
y_coords = points[:, 1]

x_min, x_max = x_coords.min(), x_coords.max()
y_min, y_max = y_coords.min(), y_coords.max()

x_center = (x_min + x_max) / 2
y_center = (y_min + y_max) / 2
width = x_max - x_min
height = y_max - y_min

bbox = [x_center, y_center, width, height]
```

**è¾“å‡º**ï¼š
```python
bbox: array [4], dtype=float32
    [0.050, 0.270, 0.040, 0.200]
    # [x_center, y_center, width, height]
    # å½’ä¸€åŒ–åçš„å€¼åŸŸï¼š[-1, 1]
```

---

### æ­¥éª¤1.7ï¼šä¿å­˜GTç¼“å­˜

**ä»£ç ä½ç½®**ï¼š`llava/data/map_gt/cache.py` - `process_one_sample()`

**è¾“å‡ºæ–‡ä»¶**ï¼š`gt_cache/annotations/{sample_token}.pkl`

**å†…å®¹**ï¼š
```python
{
    'sample_token': 'ca9a282c9e77460f8360f564131a8af5',
    'gt_classes': array([0, 0, 1, 2, 0], dtype=int64),
    'gt_points': array([
        [[0.033, 0.340], [0.080, 0.350], ..., [0.067, -0.200]],  # å®ä¾‹0
        [[âˆ’0.500, 0.800], [...], ...],  # å®ä¾‹1
        ...
    ], shape=[5, 20, 2], dtype=float32),
    'gt_is_closed': array([False, False, False, True, False], dtype=bool),
    'gt_bbox': array([
        [0.050, 0.270, 0.040, 0.200],  # å®ä¾‹0çš„bbox
        [âˆ’0.450, 0.750, 0.100, 0.300],
        ...
    ], shape=[5, 4], dtype=float32)
}
```

**é˜¶æ®µä¸€æ€»ç»“**ï¼š
- è¾“å…¥ï¼šnuScenesåŸå§‹æ•°æ®ï¼ˆJSON + åœ°å›¾ï¼‰
- è¾“å‡ºï¼šæ¯ä¸ªsampleä¸€ä¸ª.pklæ–‡ä»¶
- å†…å®¹ï¼šç±»åˆ«ã€20ç‚¹ã€æ˜¯å¦é—­åˆã€åŒ…å›´ç›’
- åæ ‡ç³»ï¼šè½¦è¾†egoï¼Œå½’ä¸€åŒ–åˆ°[-1, 1]

---

## é˜¶æ®µäºŒï¼šDatasetåŠ è½½

### æ­¥éª¤2.1ï¼šDatasetåˆå§‹åŒ–

**ä»£ç ä½ç½®**ï¼š`llava/data/map_dataset.py` - `MapDetectionDataset.__init__()`

**æ“ä½œ**ï¼š
```python
dataset = MapDetectionDataset(
    dataroot='/path/to/nuscenes',
    version='v1.0-mini',
    split='train',
    gt_cache_path='/path/to/gt_cache',
    prompt='è¯·å¸®æˆ‘è¯†åˆ«å›¾ä¸­çš„è½¦é“çº¿ã€é“è·¯è¾¹ç•Œã€äººè¡Œæ¨ªé“ä¸‰ç±»ç‰©ä½“'
)
```

**å†…éƒ¨çŠ¶æ€**ï¼š
```python
self.nusc = NuScenes(version='v1.0-mini', dataroot=dataroot)
self.sample_tokens = ['token1', 'token2', ...]  # 80ä¸ªæ ·æœ¬ï¼ˆminiï¼‰
self.image_processor = CLIPImageProcessor.from_pretrained(...)
self.tokenizer = AutoTokenizer.from_pretrained('vicuna-7b-v1.5')

# Promptå¤„ç†
self.prompt_with_images = "<image>\nè¯·å¸®æˆ‘è¯†åˆ«..."
self.prompt_ids = tokenizer_image_token(self.prompt_with_images)
# ç»“æœï¼š[1, 319, 526, ..., -200, ..., 2]  # -200æ˜¯IMAGE_TOKEN_INDEX
```

---

### æ­¥éª¤2.2ï¼š__getitem__ - åŠ è½½å•ä¸ªæ ·æœ¬

**ä»£ç ä½ç½®**ï¼š`llava/data/map_dataset.py` - `__getitem__()`

#### å­æ­¥éª¤2.2.1ï¼šåŠ è½½6å¼ å›¾åƒ

**è¾“å…¥**ï¼š
```python
sample_token: str
```

**å¤„ç†**ï¼š
```python
sample = self.nusc.get('sample', sample_token)

images = []
for cam_name in ['CAM_FRONT', 'CAM_FRONT_RIGHT', ...]:
    # 1. è·å–å›¾åƒè·¯å¾„
    cam_token = sample['data'][cam_name]
    cam_data = self.nusc.get('sample_data', cam_token)
    img_path = os.path.join(dataroot, cam_data['filename'])
    
    # 2. è¯»å–å›¾åƒ
    img = Image.open(img_path).convert('RGB')
    # PIL Image, å°ºå¯¸=(1600, 900), mode='RGB'
    
    # 3. CLIPé¢„å¤„ç†
    processed = self.image_processor(images=img, return_tensors='pt')
    img_tensor = processed['pixel_values'][0]
    # shape: (3, 336, 336)
    # dtype: torch.float32
    # å€¼åŸŸ: çº¦[-2.5, 2.5]ï¼ˆCLIPå½’ä¸€åŒ–ï¼‰
    
    images.append(img_tensor)

# 4. å †å 
images = torch.stack(images, dim=0)
```

**è¾“å‡º**ï¼š
```python
images: torch.FloatTensor
    shape: (6, 3, 336, 336)
    dtype: torch.float32
    å€¼åŸŸ: çº¦[-2.5, 2.5]
    å†…å­˜: 6Ã—3Ã—336Ã—336Ã—4 bytes â‰ˆ 8.1 MB
```

---

#### å­æ­¥éª¤2.2.2ï¼šåŠ è½½GT

**è¾“å…¥**ï¼š
```python
sample_token: str
gt_cache_path: str
```

**å¤„ç†**ï¼š
```python
# 1. ä»ç¼“å­˜åŠ è½½
gt_file = os.path.join(gt_cache_path, f'{sample_token}.pkl')
with open(gt_file, 'rb') as f:
    gt_dict = pickle.load(f)

# 2. è½¬æ¢ä¸ºMapGroundTruthå¯¹è±¡
gt = MapGroundTruth(
    class_labels=torch.from_numpy(gt_dict['gt_classes']),
    points=torch.from_numpy(gt_dict['gt_points']),
    bbox=torch.from_numpy(gt_dict['gt_bbox'])
)
gt.is_closed = torch.from_numpy(gt_dict['gt_is_closed'])
```

**è¾“å‡º**ï¼š
```python
gt: MapGroundTruthå¯¹è±¡
    gt.class_labels: torch.LongTensor, shape=(N_gt,)
        ä¾‹å¦‚ï¼štensor([0, 0, 1, 2, 0])  # 5ä¸ªå®ä¾‹
    
    gt.points: torch.FloatTensor, shape=(N_gt, 20, 2)
        ä¾‹å¦‚ï¼štensor([
            [[0.033, 0.340], [0.080, 0.350], ..., [0.067, -0.200]],
            [[-0.500, 0.800], [...], ...],
            ...
        ])
        å€¼åŸŸï¼š[-1, 1]
    
    gt.is_closed: torch.BoolTensor, shape=(N_gt,)
        ä¾‹å¦‚ï¼štensor([False, False, False, True, False])
    
    gt.bbox: torch.FloatTensor, shape=(N_gt, 4)
        ä¾‹å¦‚ï¼štensor([
            [0.050, 0.270, 0.040, 0.200],
            [-0.450, 0.750, 0.100, 0.300],
            ...
        ])
```

---

#### å­æ­¥éª¤2.2.3ï¼šåŠ è½½ç›¸æœºå‚æ•°

**è¾“å…¥**ï¼š
```python
sample_token: str
```

**å¤„ç†**ï¼š
```python
sample = self.nusc.get('sample', sample_token)

# 1. è·å–ego_pose
lidar_token = sample['data']['LIDAR_TOP']
lidar_data = self.nusc.get('sample_data', lidar_token)
ego_pose = self.nusc.get('ego_pose', lidar_data['ego_pose_token'])

# 2. è·å–ç›¸æœºå‚æ•°
cam_intrinsics = []
cam_extrinsics = []
for cam_name in CAMERA_NAMES:
    cam_token = sample['data'][cam_name]
    cam_data = self.nusc.get('sample_data', cam_token)
    cam_calib = self.nusc.get('calibrated_sensor', 
                               cam_data['calibrated_sensor_token'])
    
    # å†…å‚
    intrinsic = cam_calib['camera_intrinsic']  # List[List[float]]
    cam_intrinsics.append(intrinsic)
    
    # å¤–å‚
    cam_extrinsics.append({
        'translation': cam_calib['translation'],  # [x, y, z]
        'rotation': cam_calib['rotation']  # [w, x, y, z] å››å…ƒæ•°
    })

img_metas = {
    'ego_pose': {
        'translation': ego_pose['translation'],
        'rotation': ego_pose['rotation']
    },
    'cam_intrinsics': cam_intrinsics,
    'cam_extrinsics': cam_extrinsics,
    ...
}
```

**è¾“å‡º**ï¼š
```python
img_metas: dict
{
    'sample_token': 'ca9a282c...',
    'ego_pose': {
        'translation': [411.3, 1180.9, 0.0],
        'rotation': [0.572, -0.002, 0.012, -0.820]
    },
    'cam_intrinsics': [
        [[1266.417, 0.0, 816.267],
         [0.0, 1266.417, 491.507],
         [0.0, 0.0, 1.0]],
        [...],  # 6ä¸ªç›¸æœº
    ],
    'cam_extrinsics': [
        {
            'translation': [1.70, 0.016, 1.51],
            'rotation': [0.5, -0.5, 0.5, -0.5]
        },
        {...},  # 6ä¸ªç›¸æœº
    ],
    'pc_range': [-15, -30, -2, 15, 30, 2]
}
```

---

#### å­æ­¥éª¤2.2.4ï¼šè¿”å›å•ä¸ªæ ·æœ¬

**è¾“å‡º**ï¼š
```python
return {
    'images': torch.FloatTensor (6, 3, 336, 336),
    'text_ids': torch.LongTensor (L,),  # ä¾‹å¦‚L=76
    'gt': MapGroundTruthå¯¹è±¡,
    'sample_token': str,
    'img_metas': dict
}
```

---

## é˜¶æ®µä¸‰ï¼šBatchæ„å»º

### æ­¥éª¤3.1ï¼šcollate_fnå¤„ç†

**ä»£ç ä½ç½®**ï¼š`llava/data/map_dataset.py` - `collate_fn()`

**è¾“å…¥**ï¼š
```python
batch: List[dict]ï¼Œé•¿åº¦=B (batch_size)
[
    {'images': (6,3,336,336), 'text_ids': (76,), 'gt': ..., ...},
    {'images': (6,3,336,336), 'text_ids': (76,), 'gt': ..., ...},
    ...
]
```

#### å­æ­¥éª¤3.1.1ï¼šå †å images

**å¤„ç†**ï¼š
```python
images = torch.stack([item['images'] for item in batch], dim=0)
```

**è¾“å‡º**ï¼š
```python
images: torch.FloatTensor
    shape: (B, 6, 3, 336, 336)
    dtype: torch.float32
    å€¼åŸŸ: [-2.5, 2.5]
    
ä¾‹å¦‚B=4:
    shape: (4, 6, 3, 336, 336)
    å†…å­˜: 4Ã—6Ã—3Ã—336Ã—336Ã—4 bytes â‰ˆ 32.4 MB
```

---

#### å­æ­¥éª¤3.1.2ï¼šå †å text_ids

**å¤„ç†**ï¼š
```python
text_ids = torch.stack([item['text_ids'] for item in batch], dim=0)
```

**è¾“å‡º**ï¼š
```python
text_ids: torch.LongTensor
    shape: (B, L)  # ä¾‹å¦‚(4, 76)
    dtype: torch.int64
    
ç¤ºä¾‹å†…å®¹ï¼š
tensor([[1, 319, 526, ..., -200, ..., 29962, 2],
        [1, 319, 526, ..., -200, ..., 29962, 2],
        [1, 319, 526, ..., -200, ..., 29962, 2],
        [1, 319, 526, ..., -200, ..., 29962, 2]])

å…³é”®ç‚¹ï¼š
- ç¬¬0åˆ—éƒ½æ˜¯1ï¼ˆBOS tokenï¼‰
- ä¸­é—´æŸä½ç½®éƒ½æ˜¯-200ï¼ˆIMAGE_TOKEN_INDEXï¼‰
- æœ€åä¸€åˆ—éƒ½æ˜¯2ï¼ˆEOS tokenï¼‰
- æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„promptï¼Œæ‰€ä»¥å†…å®¹ç›¸åŒ
```

---

#### å­æ­¥éª¤3.1.3ï¼šå¤„ç†ç›¸æœºå‚æ•°

**å¤„ç†**ï¼š
```python
from pyquaternion import Quaternion

cam_intrinsics = []
cam_extrinsics = []
ego_poses = []

for item in batch:
    # å†…å‚ï¼šç›´æ¥è½¬tensor
    intrinsics = torch.tensor(
        item['img_metas']['cam_intrinsics'], 
        dtype=torch.float32
    )  # (6, 3, 3)
    cam_intrinsics.append(intrinsics)
    
    # å¤–å‚ï¼šå››å…ƒæ•°â†’æ—‹è½¬çŸ©é˜µ
    extrinsics_list = []
    for ext in item['img_metas']['cam_extrinsics']:
        mat = torch.eye(4, dtype=torch.float32)
        
        # Rotation
        quat = Quaternion(ext['rotation'])
        mat[:3, :3] = torch.from_numpy(quat.rotation_matrix)
        
        # Translation
        mat[:3, 3] = torch.tensor(ext['translation'])
        
        extrinsics_list.append(mat)
    
    extrinsics = torch.stack(extrinsics_list)  # (6, 4, 4)
    cam_extrinsics.append(extrinsics)
    
    # Ego poseï¼šåŒæ ·å¤„ç†
    ego_dict = item['img_metas']['ego_pose']
    ego_mat = torch.eye(4)
    ego_quat = Quaternion(ego_dict['rotation'])
    ego_mat[:3, :3] = torch.from_numpy(ego_quat.rotation_matrix)
    ego_mat[:3, 3] = torch.tensor(ego_dict['translation'])
    ego_poses.append(ego_mat)

cam_intrinsics = torch.stack(cam_intrinsics)  # (B, 6, 3, 3)
cam_extrinsics = torch.stack(cam_extrinsics)  # (B, 6, 4, 4)
ego_poses = torch.stack(ego_poses)  # (B, 4, 4)
```

**è¾“å‡º**ï¼š
```python
cam_intrinsics: torch.FloatTensor, shape=(B, 6, 3, 3)
ç¤ºä¾‹ï¼ˆB=1çš„ç¬¬0ä¸ªç›¸æœºï¼‰ï¼š
tensor([[[505.668,   0.000, 326.507],
         [  0.000, 505.668, 196.403],
         [  0.000,   0.000,   1.000]]])

cam_extrinsics: torch.FloatTensor, shape=(B, 6, 4, 4)
ç¤ºä¾‹ï¼ˆB=1çš„ç¬¬0ä¸ªç›¸æœºï¼‰ï¼š
tensor([[[ 0.0200, -0.9998,  0.0300,  1.7008],
         [ 0.0100,  0.0300,  0.9995,  0.0159],
         [-0.9998, -0.0200,  0.0100,  1.5110],
         [ 0.0000,  0.0000,  0.0000,  1.0000]]])

ego_pose: torch.FloatTensor, shape=(B, 4, 4)
ç¤ºä¾‹ï¼ˆB=1ï¼‰ï¼š
tensor([[[ 0.5720, -0.8201,  0.0120, 411.3039],
         [ 0.8201,  0.5720, -0.0017, 1180.890],
         [ 0.0000,  0.0118,  0.9999,   0.0000],
         [ 0.0000,  0.0000,  0.0000,   1.0000]]])
```

---

#### å­æ­¥éª¤3.1.4ï¼šPadding GT

**å¤„ç†**ï¼š
```python
# æ‰¾åˆ°æœ€å¤§GTæ•°é‡
max_num_gts = max(len(item['gt'].class_labels) for item in batch)

# åˆå§‹åŒ–padding tensor
gt_classes = torch.zeros(B, max_num_gts, dtype=torch.long)
gt_points = torch.zeros(B, max_num_gts, 20, 2, dtype=torch.float32)
gt_is_closed = torch.zeros(B, max_num_gts, dtype=torch.bool)
gt_bbox = torch.zeros(B, max_num_gts, 4, dtype=torch.float32)
gt_mask = torch.zeros(B, max_num_gts, dtype=torch.bool)

# å¡«å…¥çœŸå®GT
for i, item in enumerate(batch):
    gt = item['gt']
    num_gts = len(gt.class_labels)
    
    if num_gts > 0:
        gt_classes[i, :num_gts] = gt.class_labels
        gt_points[i, :num_gts] = gt.points
        gt_is_closed[i, :num_gts] = gt.is_closed
        gt_bbox[i, :num_gts] = gt.bbox
        gt_mask[i, :num_gts] = True  # æ ‡è®°æœ‰æ•ˆGT
```

**è¾“å‡º**ï¼š
```python
gt_classes: torch.LongTensor, shape=(B, max_N)
ç¤ºä¾‹ï¼ˆB=2, max_N=8ï¼‰ï¼š
tensor([[0, 0, 1, 2, 0, 0, 0, 0],  # æ ·æœ¬0æœ‰5ä¸ªGTï¼Œå3ä¸ªæ˜¯padding
        [1, 2, 0, 1, 2, 0, 0, 0]]) # æ ·æœ¬1æœ‰6ä¸ªGTï¼Œå2ä¸ªæ˜¯padding

gt_points: torch.FloatTensor, shape=(B, max_N, 20, 2)
ç¤ºä¾‹ï¼š
tensor([[
    [[0.033, 0.340], [0.080, 0.350], ..., [0.067, -0.200]],  # GT 0
    [[-0.500, 0.800], [...], ...],  # GT 1
    ...
    [[0.0, 0.0], [0.0, 0.0], ..., [0.0, 0.0]]  # padding
]])

gt_mask: torch.BoolTensor, shape=(B, max_N)
ç¤ºä¾‹ï¼š
tensor([[True, True, True, True, True, False, False, False],
        [True, True, True, True, True, True, False, False]])
```

---

### æ­¥éª¤3.2ï¼šæœ€ç»ˆBatchè¾“å‡º

**è¾“å‡º**ï¼š
```python
batch = {
    # ========== å›¾åƒæ•°æ® ==========
    'images': torch.FloatTensor, shape=(B, 6, 3, 336, 336)
        å€¼åŸŸ: [-2.5, 2.5]
        å†…å­˜: BÃ—6Ã—3Ã—336Ã—336Ã—4 bytes
    
    # ========== æ–‡æœ¬æ•°æ® ==========
    'text_ids': torch.LongTensor, shape=(B, L)
        åŒ…å«IMAGE_TOKEN_INDEX (-200)
        ç¤ºä¾‹ï¼š[1, 319, ..., -200, ..., 2]
    
    # ========== ç›¸æœºå‚æ•° ==========
    'cam_intrinsics': torch.FloatTensor, shape=(B, 6, 3, 3)
        6ä¸ªç›¸æœºçš„å†…å‚çŸ©é˜µ
    
    'cam_extrinsics': torch.FloatTensor, shape=(B, 6, 4, 4)
        6ä¸ªç›¸æœºåˆ°è½¦è¾†çš„å˜æ¢çŸ©é˜µï¼ˆå«rotationï¼‰
    
    'ego_pose': torch.FloatTensor, shape=(B, 4, 4)
        è½¦è¾†åˆ°ä¸–ç•Œçš„å˜æ¢çŸ©é˜µ
    
    # ========== GTæ ‡æ³¨ ==========
    'gt_classes': torch.LongTensor, shape=(B, max_N)
        å€¼åŸŸ: [0, 1, 2]
    
    'gt_points': torch.FloatTensor, shape=(B, max_N, 20, 2)
        å€¼åŸŸ: [-1, 1]
    
    'gt_is_closed': torch.BoolTensor, shape=(B, max_N)
    
    'gt_bbox': torch.FloatTensor, shape=(B, max_N, 4)
        å€¼åŸŸ: [-1, 1]
    
    'gt_mask': torch.BoolTensor, shape=(B, max_N)
        æ ‡è®°æœ‰æ•ˆGTï¼ˆTrueï¼‰å’Œpaddingï¼ˆFalseï¼‰
    
    # ========== å…ƒæ•°æ® ==========
    'sample_tokens': List[str], é•¿åº¦B
    'img_metas': List[dict], é•¿åº¦B
}
```

---

## é˜¶æ®µå››ï¼šQ-Formerå¤„ç†

### æ­¥éª¤4.1ï¼šæå–å›¾åƒç‰¹å¾

**ä»£ç ä½ç½®**ï¼š`llava/model/qformer.py` - `extract_img_feat()`

**è¾“å…¥**ï¼š
```python
imgs: torch.FloatTensor, shape=(B, 6, 3, 336, 336)
```

**å¤„ç†**ï¼š
```python
B, N, C, H, W = imgs.shape  # B=batch, N=6 cameras
imgs = imgs.reshape(B * N, C, H, W)  # (B*6, 3, 336, 336)

# Backbone (ResNet50)
img_feats = self.img_backbone(imgs)

# å‡è®¾ä½¿ç”¨ResNet50çš„layer4è¾“å‡º
# stride=16, 336/16=21
```

**è¾“å‡º**ï¼š
```python
img_feats: torch.FloatTensor
    shape: (B*6, 256, 21, 21)
    dtype: torch.float32
    
ç¤ºä¾‹B=4:
    shape: (24, 256, 21, 21)
    å†…å­˜: 24Ã—256Ã—21Ã—21Ã—4 bytes â‰ˆ 5.4 MB
```

---

### æ­¥éª¤4.2ï¼šæ·»åŠ ä½ç½®ç¼–ç 

**ä»£ç ä½ç½®**ï¼š`llava/model/qformer.py` - `add_position_encoding()`

**è¾“å…¥**ï¼š
```python
img_feats: (B*N, C, h, w) = (B*6, 256, 21, 21)
```

**å¤„ç†**ï¼š
```python
BN, C, h, w = img_feats.shape

# 1. 2Dä½ç½®ç¼–ç 
pos_embed = self.position_encoding(h, w)  # (h, w, C)
pos_embed = pos_embed.permute(2, 0, 1).unsqueeze(0)  # (1, C, h, w)
pos_embed = pos_embed.expand(BN, -1, -1, -1)  # (B*6, 256, 21, 21)

# 2. Camera ID embedding
cam_ids = torch.arange(6).repeat(B)  # [0,1,2,3,4,5, 0,1,2,3,4,5, ...]
cam_embed = self.camera_embed(cam_ids)  # (B*6, 256)
cam_embed = cam_embed[:, :, None, None].expand(-1, -1, h, w)  # (B*6, 256, 21, 21)

# 3. ç›¸åŠ 
img_feats = img_feats + pos_embed + cam_embed

# 4. å±•å¹³
img_feats = img_feats.flatten(2).permute(0, 2, 1)  # (B*6, h*w, 256)
img_feats = img_feats.reshape(B, 6*h*w, C)  # (B, 2646, 256)
```

**è¾“å‡º**ï¼š
```python
memory: torch.FloatTensor
    shape: (B, N*h*w, C) = (B, 2646, 256)
    # 2646 = 6 cameras Ã— 21 Ã— 21
    
ç¤ºä¾‹B=4:
    shape: (4, 2646, 256)
    å†…å­˜: 4Ã—2646Ã—256Ã—4 bytes â‰ˆ 10.8 MB
```

---

### æ­¥éª¤4.3ï¼šå‡†å¤‡Scene Queries

**ä»£ç ä½ç½®**ï¼š`llava/model/qformer.py` - `forward()`

**å¤„ç†**ï¼š
```python
# Learnable queries
queries = self.query_embed.weight  # (512, 256)
queries = queries.unsqueeze(0).expand(B, -1, -1)  # (B, 512, 256)
```

**è¾“å‡º**ï¼š
```python
queries: torch.FloatTensor
    shape: (B, 512, 256)
    dtype: torch.float32
    
    # è¿™512ä¸ªå‘é‡æ˜¯å¯å­¦ä¹ çš„å‚æ•°
    # åˆå§‹ï¼šéšæœºåˆå§‹åŒ–
    # è®­ç»ƒåï¼šæ¯ä¸ªqueryå­¦ä¼šæå–ç‰¹å®šç±»å‹çš„åœºæ™¯ä¿¡æ¯
```

---

### æ­¥éª¤4.4ï¼šTransformer Decoder

**ä»£ç ä½ç½®**ï¼š`llava/model/qformer.py` - `forward()`

**è¾“å…¥**ï¼š
```python
tgt: queries (B, 512, 256)
memory: (B, 2646, 256)
```

**å¤„ç†**ï¼š
```python
# 6å±‚Transformer Decoder
# æ¯å±‚åŒ…å«ï¼šSelf-Attention + Cross-Attention + FFN

for layer in range(6):
    # Self-Attention: queriesä¹‹é—´äº¤äº’
    queries = SelfAttention(Q=queries, K=queries, V=queries)
    
    # Cross-Attention: queriesä»å›¾åƒæå–ä¿¡æ¯
    queries = CrossAttention(Q=queries, K=memory, V=memory)
    
    # FFN
    queries = FFN(queries)

scene_features = queries
```

**è¾“å‡º**ï¼š
```python
scene_features: torch.FloatTensor
    shape: (B, 512, 256)
    dtype: torch.float32
    
    # è¿™512ä¸ªç‰¹å¾å‘é‡å·²ç»èåˆäº†6å¼ å›¾çš„ä¿¡æ¯
    # æ¯ä¸ªå‘é‡æå–äº†ç‰¹å®šçš„åœºæ™¯ä¿¡æ¯
```

---

### æ­¥éª¤4.5ï¼šæŠ•å½±åˆ°LLMç»´åº¦

**ä»£ç ä½ç½®**ï¼š`llava/model/qformer.py` - `forward()`

**è¾“å…¥**ï¼š
```python
scene_features: (B, 512, 256)
```

**å¤„ç†**ï¼š
```python
# MLP projector
scene_tokens = self.projector(scene_features)

# projectorç»“æ„ï¼š
# Linear(256 â†’ 512) + GELU + Linear(512 â†’ 4096)
```

**è¾“å‡º**ï¼š
```python
scene_tokens: torch.FloatTensor
    shape: (B, 512, 4096)
    dtype: torch.float32
    
ç¤ºä¾‹B=4:
    shape: (4, 512, 4096)
    å†…å­˜: 4Ã—512Ã—4096Ã—4 bytes â‰ˆ 33.6 MB
    
å€¼åŸŸ: æ— å›ºå®šèŒƒå›´ï¼ˆå–å†³äºè®­ç»ƒï¼‰
```

---

### Q-Formeræœ€ç»ˆè¾“å‡º

**æ€»ç»“**ï¼š
```python
# Q-Formerçš„forwardè¿”å›
scene_tokens = qformer(
    imgs=batch['images'],  # (B, 6, 3, 336, 336)
    cam_intrinsics=batch['cam_intrinsics'],  # å¯é€‰
    cam_extrinsics=batch['cam_extrinsics'],  # å¯é€‰
)

# è¾“å‡º
scene_tokens: torch.FloatTensor
    shape: (B, 512, 4096)
    å«ä¹‰: 
        - 512ä¸ªtokenä»£è¡¨æ•´ä¸ªåœºæ™¯
        - æ¯ä¸ªtokenæ˜¯4096ç»´å‘é‡
        - å·²èåˆ6å¼ ç›¸æœºå›¾åƒçš„ä¿¡æ¯
        - ç»´åº¦ä¸LLMå¯¹é½ï¼Œå‡†å¤‡æ›¿æ¢IMAGE_TOKEN_INDEX
```

---

## å®Œæ•´æ•°æ®æµæ€»è§ˆ

### æ•°æ®ç»´åº¦å˜åŒ–

```
ã€åŸå§‹æ•°æ®ã€‘
6å¼ JPGå›¾åƒ: 6 Ã— (1600, 900, 3) uint8
åœ°å›¾æ ‡æ³¨: List[ä¸å®šé•¿ç‚¹åºåˆ—] ä¸–ç•Œåæ ‡

    â†“ GTé¢„å¤„ç†ï¼ˆç¦»çº¿ï¼‰

ã€GTç¼“å­˜ã€‘.pklæ–‡ä»¶
gt_classes: (N_gt,) int64, [0,1,2]
gt_points: (N_gt, 20, 2) float32, [-1, 1]
gt_bbox: (N_gt, 4) float32, [-1, 1]

    â†“ Dataset.__getitem__

ã€å•ä¸ªæ ·æœ¬ã€‘
images: (6, 3, 336, 336) float32, [-2.5, 2.5]
text_ids: (L,) int64, åŒ…å«-200
gt: MapGroundTruthå¯¹è±¡

    â†“ collate_fn

ã€Batchã€‘
images: (B, 6, 3, 336, 336)
text_ids: (B, L)
cam_intrinsics: (B, 6, 3, 3)
cam_extrinsics: (B, 6, 4, 4)
ego_pose: (B, 4, 4)
gt_classes: (B, max_N)
gt_points: (B, max_N, 20, 2)
gt_mask: (B, max_N)

    â†“ Q-Former.extract_img_feat

ã€Backboneè¾“å‡ºã€‘
img_feats: (B*6, 256, 21, 21)

    â†“ Q-Former.add_position_encoding

ã€Memoryã€‘
memory: (B, 2646, 256)  # 2646 = 6Ã—21Ã—21

    â†“ Q-Former.decoder

ã€Scene Featuresã€‘
scene_features: (B, 512, 256)

    â†“ Q-Former.projector

ã€æœ€ç»ˆè¾“å‡ºã€‘
scene_tokens: (B, 512, 4096)
```

---

### å®Œæ•´Pipelineæ€»ç»“

| é˜¶æ®µ | è¾“å…¥ | è¾“å‡º | æ“ä½œ |
|------|------|------|------|
| **GTé¢„å¤„ç†** | nuScenesåŸå§‹æ•°æ® | .pklç¼“å­˜ | ä¸–ç•Œâ†’egoâ†’è£å‰ªâ†’é‡‡æ ·â†’å½’ä¸€åŒ– |
| **DatasetåŠ è½½** | sample_token | å•ä¸ªæ ·æœ¬dict | åŠ è½½å›¾åƒ+GT+ç›¸æœºå‚æ•° |
| **Batchæ„å»º** | List[æ ·æœ¬] | Batch dict | Stack+Padding |
| **Q-Former** | Batch | scene_tokens | Backboneâ†’Decoderâ†’Projector |

**æœ€ç»ˆå¾—åˆ°**ï¼š
```python
scene_tokens: (B, 512, 4096)
# å‡†å¤‡æ›¿æ¢text_idsä¸­çš„IMAGE_TOKEN_INDEX (-200)
# ç„¶åé€å…¥LLM
```

---

## å…³é”®æ•°å€¼ç¤ºä¾‹ï¼ˆB=4ï¼‰

```python
# Batch size = 4

images:         (4, 6, 3, 336, 336)    32.4 MB
text_ids:       (4, 76)                 2.4 KB
cam_intrinsics: (4, 6, 3, 3)           1.1 KB
cam_extrinsics: (4, 6, 4, 4)           1.5 KB
ego_pose:       (4, 4, 4)               256 B
gt_classes:     (4, 20)                 320 B   # å‡è®¾max_N=20
gt_points:      (4, 20, 20, 2)         25.6 KB
gt_mask:        (4, 20)                 80 B

Q-Formerå†…éƒ¨ï¼š
img_feats:      (24, 256, 21, 21)      5.4 MB
memory:         (4, 2646, 256)        10.8 MB
queries:        (4, 512, 256)          2.1 MB
scene_tokens:   (4, 512, 4096)        33.6 MB  â† æœ€ç»ˆè¾“å‡º

æ€»å†…å­˜å³°å€¼ï¼šçº¦ 85 MB (forward pass, å•æ¬¡forward)
```

---

**è¿™å°±æ˜¯ä»nuScenesåŸå§‹æ•°æ®åˆ°Q-Formerè¾“å‡ºçš„å®Œæ•´æµç¨‹ï¼æ¯ä¸€æ­¥çš„æ•°æ®æ ¼å¼å’Œç»´åº¦éƒ½å·²è¯¦ç»†åˆ—å‡ºã€‚** ğŸ“Šâœ¨

