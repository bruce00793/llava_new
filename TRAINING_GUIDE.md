# LLaVA Map Detection Training Guide

## è®­ç»ƒé˜¶æ®µè¯´æ˜

### Stage 2: è”åˆè®­ç»ƒï¼ˆå½“å‰é˜¶æ®µï¼‰

**è®­ç»ƒç­–ç•¥ï¼š**
- âœ… **Q-Former**: å¾®è°ƒï¼ˆä»BLIP-2é¢„è®­ç»ƒæƒé‡å¼€å§‹ï¼Œå­¦ä¹ ç‡ 1e-5ï¼‰
- âœ… **Map Queries**: ä»å¤´è®­ç»ƒï¼ˆå­¦ä¹ ç‡ 1e-4ï¼‰
- âœ… **Map Decoder**: ä»å¤´è®­ç»ƒï¼ˆå­¦ä¹ ç‡ 1e-4ï¼‰
- âŒ **LLM Backbone**: å†»ç»“ï¼ˆä¸æ›´æ–°å‚æ•°ï¼‰

**ä¼˜åŠ¿ï¼š**
- ä¿ç•™LLMçš„é¢„è®­ç»ƒèƒ½åŠ›
- å¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚ï¼ˆ~23GB vs ~93GBï¼‰
- è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆåªæ›´æ–°1.6%çš„å‚æ•°ï¼‰
- é˜²æ­¢è¿‡æ‹Ÿåˆ

---

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

#### 1.1 ä¸‹è½½nuScenesæ•°æ®é›†

```bash
# ä¸‹è½½æ•°æ®åˆ° /path/to/nuscenes
# ç›®å½•ç»“æ„ï¼š
# /path/to/nuscenes/
#   â”œâ”€â”€ maps/
#   â”œâ”€â”€ samples/
#   â”œâ”€â”€ sweeps/
#   â””â”€â”€ v1.0-mini/ (æˆ– v1.0-trainval)
```

#### 1.2 ç”ŸæˆGT Cacheï¼ˆé¦–æ¬¡è®­ç»ƒå‰å¿…é¡»æ‰§è¡Œï¼‰

```bash
cd /home/cly/auto/llava_test/LLaVA

# ç”Ÿæˆtrainå’Œvalçš„GT cache
python tools/generate_gt_cache.py \
    --dataroot /path/to/nuscenes \
    --version v1.0-mini \
    --split train \
    --output gt_cache_train.pkl

python tools/generate_gt_cache.py \
    --dataroot /path/to/nuscenes \
    --version v1.0-mini \
    --split val \
    --output gt_cache_val.pkl
```

**GT Cacheçš„ä½œç”¨ï¼š**
- é¢„å¤„ç†æ‰€æœ‰æ ·æœ¬çš„GTï¼ˆåæ ‡è½¬æ¢ã€é‡‡æ ·åˆ°20ç‚¹ã€è®¡ç®—AABBç­‰ï¼‰
- é¿å…è®­ç»ƒæ—¶é‡å¤è®¡ç®—ï¼Œå¤§å¹…åŠ é€Ÿæ•°æ®åŠ è½½
- train set: ~700ä¸ªæ ·æœ¬ï¼ˆminiï¼‰/ ~28Kä¸ªæ ·æœ¬ï¼ˆtrainvalï¼‰
- val set: ~200ä¸ªæ ·æœ¬ï¼ˆminiï¼‰/ ~6Kä¸ªæ ·æœ¬ï¼ˆtrainvalï¼‰

---

### 2. ä¿®æ”¹é…ç½®

ç¼–è¾‘ `scripts/train_stage2.sh`ï¼š

```bash
# ä¿®æ”¹è¿™äº›è·¯å¾„
DATAROOT="/path/to/nuscenes"  # â† æ”¹ä¸ºä½ çš„nuScenesè·¯å¾„
VERSION="v1.0-mini"            # â† æˆ– v1.0-trainval

# å¯é€‰ï¼šä¿®æ”¹LLMè·¯å¾„ï¼ˆå¦‚æœå·²ä¸‹è½½åˆ°æœ¬åœ°ï¼‰
LLM_PATH="lmsys/vicuna-7b-v1.5"  # â† æˆ–æœ¬åœ°è·¯å¾„

# å¯é€‰ï¼šè°ƒæ•´æ˜¾å­˜é…ç½®
BATCH_SIZE=4  # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œæ”¹ä¸º2
```

---

### 3. å¼€å§‹è®­ç»ƒ

#### å•å¡è®­ç»ƒï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰

```bash
cd /home/cly/auto/llava_test/LLaVA
bash scripts/train_stage2.sh
```

#### å¤šå¡è®­ç»ƒï¼ˆæ¨èç”¨äºå®Œæ•´è®­ç»ƒï¼‰

```bash
cd /home/cly/auto/llava_test/LLaVA

# ä¿®æ”¹ scripts/train_stage2_distributed.sh ä¸­çš„ NUM_GPUS
# ç„¶åè¿è¡Œï¼š
bash scripts/train_stage2_distributed.sh
```

---

## æ˜¾å­˜éœ€æ±‚

### å•å¡é…ç½®

| é…ç½® | æ˜¾å­˜å ç”¨ | æ¨èGPU |
|-----|---------|---------|
| Batch=4, FP16 | ~23GB | A100 40GB |
| Batch=2, FP16 | ~15GB | V100 32GB |
| Batch=1, FP16 | ~10GB | RTX 3090 24GB |

### å¤šå¡é…ç½®ï¼ˆæ¨èï¼‰

| GPUs | Per-GPU Batch | Total Batch | æ˜¾å­˜/å¡ |
|------|---------------|-------------|---------|
| 4Ã—A100 | 4 | 16 | ~23GB |
| 4Ã—V100 | 2 | 8 | ~15GB |
| 8Ã—A100 | 4 | 32 | ~23GB |

---

## è®­ç»ƒç›‘æ§

### æ—¥å¿—è¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ‰“å°ï¼š

```
Epoch [1/24] Step [10/200] Loss: 3.5421 (Avg: 3.6234) LR: q=1.00e-05 m=1.00e-04 d=1.00e-04
  Detailed losses:
    loss_cls: 1.2345 (Avg: 1.3456)
    loss_pts: 1.8765 (Avg: 1.9012)
    loss_dir: 0.4311 (Avg: 0.3766)
    loss_total: 3.5421 (Avg: 3.6234)
```

**æŒ‡æ ‡è¯´æ˜ï¼š**
- `loss_cls`: åˆ†ç±»æŸå¤±ï¼ˆFocal Lossï¼‰
- `loss_pts`: ç‚¹å›å½’æŸå¤±ï¼ˆL1 Lossï¼‰
- `loss_dir`: æ–¹å‘æŸå¤±ï¼ˆCosine Lossï¼‰
- `loss_total`: æ€»æŸå¤±ï¼ˆåŠ æƒå’Œï¼‰

### æ£€æŸ¥ç‚¹ä¿å­˜

```
outputs/map_detection_stage2_YYYYMMDD_HHMMSS/
â”œâ”€â”€ config.json                  # è®­ç»ƒé…ç½®
â”œâ”€â”€ best_model.pth              # éªŒè¯é›†æœ€ä¼˜æ¨¡å‹
â”œâ”€â”€ checkpoint_epoch_1.pth      # æ¯epochçš„æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint_epoch_2.pth
â”œâ”€â”€ ...
â””â”€â”€ final_model.pth             # æœ€ç»ˆæ¨¡å‹
```

---

## éªŒè¯å’Œæ¨ç†

### éªŒè¯æ¨¡å‹æ€§èƒ½

```bash
python test_map_model.py \
    --checkpoint outputs/map_detection_stage2_XXX/best_model.pth \
    --dataroot /path/to/nuscenes \
    --version v1.0-mini \
    --split val
```

### å•æ ·æœ¬æ¨ç†

```python
import torch
from llava.model.map_llava_model import build_map_detector

# åŠ è½½æ¨¡å‹
model = build_map_detector(
    llm_path='lmsys/vicuna-7b-v1.5',
    freeze_llm=True,
    qformer_pretrained='blip2'
)

# åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
checkpoint = torch.load('outputs/map_detection_stage2_XXX/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model = model.cuda().eval()

# æ¨ç†
with torch.no_grad():
    output = model(images, text_ids, return_loss=False)
    pred_logits = output['pred_logits']  # [B, 50, 4]
    pred_points = output['pred_points']  # [B, 50, 20, 2]
```

---

## è¶…å‚æ•°è°ƒä¼˜å»ºè®®

### å­¦ä¹ ç‡

```python
# é»˜è®¤é…ç½®ï¼ˆæ¨èï¼‰
LR_QFORMER = 1e-5   # BLIP-2é¢„è®­ç»ƒï¼Œå°å­¦ä¹ ç‡å¾®è°ƒ
LR_QUERIES = 1e-4   # ä»å¤´è®­ç»ƒï¼Œæ ‡å‡†å­¦ä¹ ç‡
LR_DECODER = 1e-4   # ä»å¤´è®­ç»ƒï¼Œæ ‡å‡†å­¦ä¹ ç‡

# å¦‚æœLossä¸ä¸‹é™ï¼Œå°è¯•ï¼š
LR_QFORMER = 5e-6   # é™ä½Q-Formerå­¦ä¹ ç‡
LR_QUERIES = 5e-5   # é™ä½Querieså­¦ä¹ ç‡

# å¦‚æœLosséœ‡è¡ï¼Œå°è¯•ï¼š
WARMUP_STEPS = 1000  # å¢åŠ warmupæ­¥æ•°
GRAD_CLIP = 0.05     # é™ä½æ¢¯åº¦è£å‰ªé˜ˆå€¼
```

### Batch Size

```python
# æœ‰æ•ˆbatch sizeåº”åœ¨8-32ä¹‹é—´
Effective_Batch = Batch_per_GPU Ã— Num_GPUs Ã— Gradient_Accumulation

# ç¤ºä¾‹ï¼š
# 4 GPUs Ã— batch=2 Ã— grad_accum=2 = 16 (æ¨è)
# 1 GPU Ã— batch=4 Ã— grad_accum=4 = 16 (ç­‰æ•ˆ)
```

### è®­ç»ƒè½®æ•°

```python
# v1.0-mini (å°æ•°æ®é›†)
EPOCHS = 24   # é»˜è®¤

# v1.0-trainval (å®Œæ•´æ•°æ®é›†)
EPOCHS = 12   # æ•°æ®é‡å¤§ï¼Œæ”¶æ•›å¿«
```

---

## å¸¸è§é—®é¢˜

### Q1: OOM (Out of Memory)

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. é™ä½batch size
BATCH_SIZE=2  # æˆ– 1

# 2. å¯ç”¨æ··åˆç²¾åº¦ï¼ˆé»˜è®¤å·²å¯ç”¨ï¼‰
--fp16

# 3. å‡å°‘workers
NUM_WORKERS=2

# 4. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰
```

### Q2: GT Cacheç”Ÿæˆå¤±è´¥

**å¯èƒ½åŸå› ï¼š**
- nuScenesè·¯å¾„ä¸æ­£ç¡®
- ç‰ˆæœ¬ä¸åŒ¹é…ï¼ˆmini vs trainvalï¼‰
- ç¼ºå°‘mapæ•°æ®

**æ£€æŸ¥æ–¹æ³•ï¼š**
```bash
# éªŒè¯nuScenesåŠ è½½
python -c "
from nuscenes.nuscenes import NuScenes
nusc = NuScenes(version='v1.0-mini', dataroot='/path/to/nuscenes')
print(f'Loaded {len(nusc.sample)} samples')
"
```

### Q3: BLIP-2ä¸‹è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ‰‹åŠ¨ä¸‹è½½BLIP-2æ¨¡å‹
# æˆ–ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–è·³è¿‡BLIP-2é¢„è®­ç»ƒï¼ˆä¸æ¨èï¼‰
--qformer-pretrained none
```

### Q4: Lossä¸ä¸‹é™

**æ£€æŸ¥æ¸…å•ï¼š**
1. âœ… GT cacheç”Ÿæˆæ­£ç¡®
2. âœ… æ•°æ®é¢„å¤„ç†æ­£ç¡®ï¼ˆåæ ‡å½’ä¸€åŒ–ï¼‰
3. âœ… å­¦ä¹ ç‡è®¾ç½®åˆç†
4. âœ… æ¢¯åº¦æ­£å¸¸ä¼ æ’­ï¼ˆæ£€æŸ¥`requires_grad`ï¼‰

**è°ƒè¯•å‘½ä»¤ï¼š**
```bash
# æ‰“å°æ¨¡å‹å‚æ•°ç»Ÿè®¡
python -c "
from llava.model.map_llava_model import build_map_detector
model = build_map_detector(freeze_llm=True, qformer_pretrained='blip2')
for name, param in model.named_parameters():
    if param.requires_grad:
        print(f'{name}: {param.shape}')
"
```

---

## ä¸‹ä¸€æ­¥ï¼šStage 3ï¼ˆå¯é€‰ï¼‰

**å¦‚æœStage 2çš„æ€§èƒ½å·²ç»æ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥è·³è¿‡Stage 3ã€‚**

Stage 3ä¼šè§£å†»LLMè¿›è¡Œå¾®è°ƒï¼Œéœ€è¦ï¼š
- æ›´å¤šæ˜¾å­˜ï¼ˆ~93GBï¼‰
- æ›´å°å­¦ä¹ ç‡ï¼ˆ1e-6ï¼‰
- æ›´çŸ­è®­ç»ƒæ—¶é—´ï¼ˆ3 epochsï¼‰

---

## è”ç³»ä¸æ”¯æŒ

- ä»£ç è·¯å¾„: `/home/cly/auto/llava_test/LLaVA`
- å†å²å¯¹è¯: æŸ¥çœ‹å®Œæ•´pipelineè®¾è®¡
- é—®é¢˜åé¦ˆ: è®°å½•åˆ°issueæˆ–å¯¹è¯ä¸­

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**

